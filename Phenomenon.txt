loss function : max-loss the loss can be quite large via adding noise into the training dataset
loss function : min-loss the loss can not be converage via adding noise into the training dataset

Fuck pytorch version :
Nan when useing pytorch2
Normal when use pytorch1.7





max_clip_loss_generator:  preTrain model : ViT-B/32
Natural zero-shot cifar-10 test_set :
Top-1 accuracy: 88.52
Top-5 accuracy: 99.39
Generate noise of downstream CIFAR-10 dataset:  δ = 8   l2 norm 
Top-1 accuracy: 24.19
Top-5 accuracy: 56.79
Random delta:  δ = 8   l2 norm
Top-1 accuracy: 84.63
Top-5 accuracy: 99.12

Inference:
the noise from generator can be generalize to other dataset


max_clip_loss_generator:  preTrain model : ViT-B/32 dataset: cifar-100
Natural zero-shot cifar-100 test_set :
Top-1 accuracy: 61.39
Top-5 accuracy: 86.87
Generate noise of downstream CIFAR-100 dataset: δ = 8   l2 norm
Top-1 accuracy: 9.13
Top-5 accuracy: 20.62
Generate noise of downstream CIFAR-100 dataset: δ = 4   l2 norm
Top-1 accuracy: 20.70
Top-5 accuracy: 41.81
Random delta: δ = 8   l2 norm
Top-1 accuracy: 59.23
Top-5 accuracy: 85.25
Random delta: δ = 4   l2 norm
Top-1 accuracy: 60.99
Top-5 accuracy: 85.89


max_clip_loss_generator:  preTrain model : ViT-B/32
Natural zero-shot image-net-1000 test_set :
Top-1 accuracy: 56.78
Top-5 accuracy: 84.23
Generate noise of downstream image-net-1000 dataset: δ = 8   l2 norm
Top-1 accuracy: 28.64
Top-5 accuracy: 53.04
Random delta: δ = 8   l2 norm
Top-1 accuracy: 55.82
Top-5 accuracy: 83.49