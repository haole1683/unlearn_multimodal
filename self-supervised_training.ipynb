{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85da1ff-c829-4dda-be6c-98d511a8bc73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu ReLU(inplace=True)\n",
      "maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "layer1 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer2 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer3 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer4 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "avgpool AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "fc Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# net.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import resnet50\n",
    "\n",
    "\n",
    "# stage one ,unsupervised learning\n",
    "class SimCLRStage1(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(SimCLRStage1, self).__init__()\n",
    "\n",
    "        self.f = []\n",
    "        for name, module in resnet50().named_children():\n",
    "            if name == 'conv1':\n",
    "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
    "                self.f.append(module)\n",
    "        # encoder\n",
    "        self.f = nn.Sequential(*self.f)\n",
    "        # projection head\n",
    "        self.g = nn.Sequential(nn.Linear(2048, 512, bias=False),\n",
    "                               nn.BatchNorm1d(512),\n",
    "                               nn.ReLU(inplace=True),\n",
    "                               nn.Linear(512, feature_dim, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.g(feature)\n",
    "        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)\n",
    "\n",
    "\n",
    "# stage two ,supervised learning\n",
    "class SimCLRStage2(torch.nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(SimCLRStage2, self).__init__()\n",
    "        # encoder\n",
    "        self.f = SimCLRStage1().f\n",
    "        # classifier\n",
    "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
    "\n",
    "        for param in self.f.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.fc(feature)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss,self).__init__()\n",
    "\n",
    "    def forward(self,out_1,out_2,batch_size,temperature=0.5):\n",
    "        # 分母 ：X.X.T，再去掉对角线值，分析结果一行，可以看成它与除了这行外的其他行都进行了点积运算（包括out_1和out_2）,\n",
    "        # 而每一行为一个batch的一个取值，即一个输入图像的特征表示，\n",
    "        # 因此，X.X.T，再去掉对角线值表示，每个输入图像的特征与其所有输出特征（包括out_1和out_2）的点积，用点积来衡量相似性\n",
    "        # 加上exp操作，该操作实际计算了分母\n",
    "        # [2*B, D]\n",
    "        out = torch.cat([out_1, out_2], dim=0)\n",
    "        # [2*B, 2*B]\n",
    "        sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n",
    "        mask = (torch.ones_like(sim_matrix) - torch.eye(2 * batch_size, device=sim_matrix.device)).bool()\n",
    "        # [2*B, 2*B-1]\n",
    "        sim_matrix = sim_matrix.masked_select(mask).view(2 * batch_size, -1)\n",
    "\n",
    "        # 分子： *为对应位置相乘，也是点积\n",
    "        # compute loss\n",
    "        pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "        # [2*B]\n",
    "        pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n",
    "        return (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean()\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    for name, module in resnet50().named_children():\n",
    "        print(name,module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a6b0b4-0050-4c50-aae6-10ec6d11684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "use_gpu=True\n",
    "gpu_name=1\n",
    "\n",
    "pre_model=os.path.join('pth','model.pth')\n",
    "\n",
    "save_path=\"pth\"\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe3e262-de69-477f-b35b-c545a677a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaddataset.py\n",
    "from torchvision.datasets import CIFAR10\n",
    "from PIL import Image\n",
    "\n",
    "from utils.data_utils import ContrastivePairDataset\n",
    "\n",
    "class PreDataset(CIFAR10):\n",
    "    def __getitem__(self, item):\n",
    "        img,target=self.data[item],self.targets[item]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            imgL = self.transform(img)\n",
    "            imgR = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return imgL, imgR, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d0b8a8-d1d0-4313-a7d6-5edbaace2da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(tensor([[[-0.0835, -0.1998, -0.3355,  ..., -0.3549, -0.2967, -0.3161],\n",
      "         [-0.1804, -0.2580, -0.3355,  ..., -0.6457, -0.5487, -0.5293],\n",
      "         [-0.3743, -0.3936, -0.4130,  ..., -0.6844, -0.6263, -0.6069],\n",
      "         ...,\n",
      "         [-0.4906, -0.4518, -0.3355,  ...,  0.1879,  0.0716, -0.0447],\n",
      "         [-0.3549, -0.3355, -0.2386,  ...,  0.0910, -0.0641, -0.1998],\n",
      "         [-0.3161, -0.2967, -0.1998,  ..., -0.0641, -0.1610, -0.3161]],\n",
      "\n",
      "        [[-0.0386, -0.1566, -0.2942,  ..., -0.3139, -0.2549, -0.2746],\n",
      "         [-0.1369, -0.2156, -0.2942,  ..., -0.6089, -0.5106, -0.4909],\n",
      "         [-0.3336, -0.3532, -0.3729,  ..., -0.6482, -0.5892, -0.5696],\n",
      "         ...,\n",
      "         [-0.4516, -0.4122, -0.2942,  ...,  0.2368,  0.1188,  0.0008],\n",
      "         [-0.3139, -0.2942, -0.1959,  ...,  0.1384, -0.0189, -0.1566],\n",
      "         [-0.2746, -0.2549, -0.1566,  ..., -0.0189, -0.1172, -0.2746]],\n",
      "\n",
      "        [[ 0.1394,  0.0223, -0.1143,  ..., -0.1338, -0.0753, -0.0948],\n",
      "         [ 0.0418, -0.0362, -0.1143,  ..., -0.4264, -0.3289, -0.3094],\n",
      "         [-0.1533, -0.1728, -0.1923,  ..., -0.4655, -0.4069, -0.3874],\n",
      "         ...,\n",
      "         [-0.2704, -0.2313, -0.1143,  ...,  0.4125,  0.2954,  0.1784],\n",
      "         [-0.1338, -0.1143, -0.0167,  ...,  0.3149,  0.1589,  0.0223],\n",
      "         [-0.0948, -0.0753,  0.0223,  ...,  0.1589,  0.0613, -0.0948]]]), tensor([[[-1.4792, -1.2660, -0.9946,  ..., -0.8977, -0.7426, -0.6457],\n",
      "         [-1.4017, -1.2466, -0.9364,  ..., -0.8589, -0.8201, -0.7620],\n",
      "         [-1.2854, -1.1303, -0.8783,  ..., -0.7232, -0.7426, -0.7232],\n",
      "         ...,\n",
      "         [ 0.2461, -0.2192, -0.5100,  ...,  0.0328, -0.0835, -0.2386],\n",
      "         [ 0.8470,  0.1491, -0.3743,  ..., -0.1610, -0.2773, -0.3549],\n",
      "         [ 1.3704,  0.4787, -0.1998,  ..., -0.2773, -0.3355, -0.4130]],\n",
      "\n",
      "        [[-1.4546, -1.2382, -0.9629,  ..., -0.8646, -0.7072, -0.6089],\n",
      "         [-1.3759, -1.2186, -0.9039,  ..., -0.8252, -0.7859, -0.7269],\n",
      "         [-1.2579, -1.1006, -0.8449,  ..., -0.6876, -0.7072, -0.6876],\n",
      "         ...,\n",
      "         [ 0.2958, -0.1762, -0.4712,  ...,  0.0794, -0.0386, -0.1959],\n",
      "         [ 0.9054,  0.1974, -0.3336,  ..., -0.1172, -0.2352, -0.3139],\n",
      "         [ 1.4364,  0.5318, -0.1566,  ..., -0.2352, -0.2942, -0.3729]],\n",
      "\n",
      "        [[-1.2654, -1.0508, -0.7776,  ..., -0.6801, -0.5240, -0.4264],\n",
      "         [-1.1873, -1.0313, -0.7191,  ..., -0.6411, -0.6020, -0.5435],\n",
      "         [-1.0703, -0.9142, -0.6606,  ..., -0.5045, -0.5240, -0.5045],\n",
      "         ...,\n",
      "         [ 0.4710,  0.0028, -0.2899,  ...,  0.2564,  0.1394, -0.0167],\n",
      "         [ 1.0758,  0.3735, -0.1533,  ...,  0.0613, -0.0558, -0.1338],\n",
      "         [ 1.6026,  0.7052,  0.0223,  ..., -0.0558, -0.1143, -0.1923]]]), 6)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ContrastivePairDataset('cifar10',contrastive_transform = train_transform)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da37bfe-b480-4831-a90b-edd8a8eba03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/remote-home/songtianwei/research/unlearn_multimodal/output/unlearn_self_supervised\"\n",
    "batch_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de1157a1-0166-4094-a5ec-72bd8a5e18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainstage1.py\n",
    "import torch,argparse,os\n",
    "\n",
    "\n",
    "# train stage one\n",
    "def train():\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = torch.device(\"cuda:\" + str(\"1\"))\n",
    "        # 每次训练计算图改动较小使用，在开始前选取较优的基础算法（比如选择一种当前高效的卷积算法）\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "    print(\"current deveice:\", DEVICE)\n",
    "\n",
    "    train_data=torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True, num_workers=16 , drop_last=True)\n",
    "\n",
    "    model = SimCLRStage1().to(DEVICE)\n",
    "    lossLR= Loss().to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for epoch in range(1,1000+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch,(imgL,imgR,labels) in enumerate(train_data):\n",
    "            imgL,imgR,labels=imgL.to(DEVICE),imgR.to(DEVICE),labels.to(DEVICE)\n",
    "\n",
    "            _, pre_L=model(imgL)\n",
    "            _, pre_R=model(imgR)\n",
    "\n",
    "            loss=lossLR(pre_L,pre_R,batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"epoch\", epoch, \"batch\", batch, \"loss:\", loss.detach().item())\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        print(\"epoch loss:\",total_loss/len(train_dataset)*batch_size)\n",
    "\n",
    "        with open(os.path.join(save_path, \"stage1_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss/len(train_dataset)*batch_size) + \" \")\n",
    "\n",
    "        if epoch % 5==0:\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, 'model_stage1_epoch' + str(epoch) + '.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d8ea4c0-ac54-47fc-a69e-2f4443b84544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current deveice: cuda:1\n",
      "epoch 1 batch 0 loss: 6.697634696960449\n",
      "epoch 1 batch 1 loss: 6.707064151763916\n",
      "epoch 1 batch 2 loss: 6.642988204956055\n",
      "epoch 1 batch 3 loss: 6.641754627227783\n",
      "epoch 1 batch 4 loss: 6.629385948181152\n",
      "epoch 1 batch 5 loss: 6.579751968383789\n",
      "epoch 1 batch 6 loss: 6.542335510253906\n",
      "epoch 1 batch 7 loss: 6.57478666305542\n",
      "epoch 1 batch 8 loss: 6.498300552368164\n",
      "epoch 1 batch 9 loss: 6.494478702545166\n",
      "epoch 1 batch 10 loss: 6.495560169219971\n",
      "epoch 1 batch 11 loss: 6.411915302276611\n",
      "epoch 1 batch 12 loss: 6.364268779754639\n",
      "epoch 1 batch 13 loss: 6.321639060974121\n",
      "epoch 1 batch 14 loss: 6.412575721740723\n",
      "epoch 1 batch 15 loss: 6.319292068481445\n",
      "epoch 1 batch 16 loss: 6.309178352355957\n",
      "epoch 1 batch 17 loss: 6.33734130859375\n",
      "epoch 1 batch 18 loss: 6.305140018463135\n",
      "epoch 1 batch 19 loss: 6.304636001586914\n",
      "epoch 1 batch 20 loss: 6.229784965515137\n",
      "epoch 1 batch 21 loss: 6.239404201507568\n",
      "epoch 1 batch 22 loss: 6.276711463928223\n",
      "epoch 1 batch 23 loss: 6.199467658996582\n",
      "epoch 1 batch 24 loss: 6.117846488952637\n",
      "epoch 1 batch 25 loss: 6.188119888305664\n",
      "epoch 1 batch 26 loss: 6.17876672744751\n",
      "epoch 1 batch 27 loss: 6.226981163024902\n",
      "epoch 1 batch 28 loss: 6.217813491821289\n",
      "epoch 1 batch 29 loss: 6.195955753326416\n",
      "epoch 1 batch 30 loss: 6.197377681732178\n",
      "epoch 1 batch 31 loss: 6.124238014221191\n",
      "epoch 1 batch 32 loss: 6.078530788421631\n",
      "epoch 1 batch 33 loss: 6.1807475090026855\n",
      "epoch 1 batch 34 loss: 6.172629356384277\n",
      "epoch 1 batch 35 loss: 6.086138725280762\n",
      "epoch 1 batch 36 loss: 6.132856845855713\n",
      "epoch 1 batch 37 loss: 6.0988898277282715\n",
      "epoch 1 batch 38 loss: 6.057321548461914\n",
      "epoch 1 batch 39 loss: 6.024401664733887\n",
      "epoch 1 batch 40 loss: 6.086170673370361\n",
      "epoch 1 batch 41 loss: 6.03142786026001\n",
      "epoch 1 batch 42 loss: 6.074469089508057\n",
      "epoch 1 batch 43 loss: 6.036139011383057\n",
      "epoch 1 batch 44 loss: 6.113183498382568\n",
      "epoch 1 batch 45 loss: 6.11479377746582\n",
      "epoch 1 batch 46 loss: 6.055612564086914\n",
      "epoch 1 batch 47 loss: 6.058168888092041\n",
      "epoch 1 batch 48 loss: 5.869047164916992\n",
      "epoch 1 batch 49 loss: 6.054511070251465\n",
      "epoch 1 batch 50 loss: 6.003096103668213\n",
      "epoch 1 batch 51 loss: 6.030871868133545\n",
      "epoch 1 batch 52 loss: 6.042368412017822\n",
      "epoch 1 batch 53 loss: 6.026522159576416\n",
      "epoch 1 batch 54 loss: 6.037576675415039\n",
      "epoch 1 batch 55 loss: 5.9769721031188965\n",
      "epoch 1 batch 56 loss: 6.008388042449951\n",
      "epoch 1 batch 57 loss: 6.038459777832031\n",
      "epoch 1 batch 58 loss: 6.010971546173096\n",
      "epoch 1 batch 59 loss: 5.895769119262695\n",
      "epoch 1 batch 60 loss: 5.989020824432373\n",
      "epoch 1 batch 61 loss: 6.00693416595459\n",
      "epoch 1 batch 62 loss: 5.9265055656433105\n",
      "epoch 1 batch 63 loss: 5.9132981300354\n",
      "epoch 1 batch 64 loss: 5.911365985870361\n",
      "epoch 1 batch 65 loss: 5.895228862762451\n",
      "epoch 1 batch 66 loss: 5.928338527679443\n",
      "epoch 1 batch 67 loss: 5.939822196960449\n",
      "epoch 1 batch 68 loss: 5.936463356018066\n",
      "epoch 1 batch 69 loss: 5.899995803833008\n",
      "epoch 1 batch 70 loss: 5.91741418838501\n",
      "epoch 1 batch 71 loss: 5.861895561218262\n",
      "epoch 1 batch 72 loss: 5.870094299316406\n",
      "epoch 1 batch 73 loss: 5.934573173522949\n",
      "epoch 1 batch 74 loss: 5.953244686126709\n",
      "epoch 1 batch 75 loss: 5.895439147949219\n",
      "epoch 1 batch 76 loss: 5.908457279205322\n",
      "epoch 1 batch 77 loss: 5.907840251922607\n",
      "epoch 1 batch 78 loss: 5.905745506286621\n",
      "epoch 1 batch 79 loss: 5.873579025268555\n",
      "epoch 1 batch 80 loss: 5.848836421966553\n",
      "epoch 1 batch 81 loss: 5.9062981605529785\n",
      "epoch 1 batch 82 loss: 5.927157878875732\n",
      "epoch 1 batch 83 loss: 5.882032871246338\n",
      "epoch 1 batch 84 loss: 5.904060363769531\n",
      "epoch 1 batch 85 loss: 5.865090847015381\n",
      "epoch 1 batch 86 loss: 5.889557838439941\n",
      "epoch 1 batch 87 loss: 5.901474475860596\n",
      "epoch 1 batch 88 loss: 5.854493141174316\n",
      "epoch 1 batch 89 loss: 5.859643459320068\n",
      "epoch 1 batch 90 loss: 5.857378959655762\n",
      "epoch 1 batch 91 loss: 5.891852378845215\n",
      "epoch 1 batch 92 loss: 5.946387767791748\n",
      "epoch 1 batch 93 loss: 5.81342077255249\n",
      "epoch 1 batch 94 loss: 5.875157356262207\n",
      "epoch 1 batch 95 loss: 5.8863701820373535\n",
      "epoch 1 batch 96 loss: 5.835597991943359\n",
      "epoch 1 batch 97 loss: 5.80744743347168\n",
      "epoch 1 batch 98 loss: 5.872021198272705\n",
      "epoch 1 batch 99 loss: 5.850296974182129\n",
      "epoch 1 batch 100 loss: 5.907449722290039\n",
      "epoch 1 batch 101 loss: 5.907454013824463\n",
      "epoch 1 batch 102 loss: 5.861313343048096\n",
      "epoch 1 batch 103 loss: 5.827096939086914\n",
      "epoch 1 batch 104 loss: 5.83200740814209\n",
      "epoch 1 batch 105 loss: 5.807718276977539\n",
      "epoch 1 batch 106 loss: 5.84959077835083\n",
      "epoch 1 batch 107 loss: 5.868772029876709\n",
      "epoch 1 batch 108 loss: 5.885461807250977\n",
      "epoch 1 batch 109 loss: 5.830005168914795\n",
      "epoch 1 batch 110 loss: 5.761675834655762\n",
      "epoch 1 batch 111 loss: 5.869945526123047\n",
      "epoch 1 batch 112 loss: 5.805809020996094\n",
      "epoch 1 batch 113 loss: 5.806990623474121\n",
      "epoch 1 batch 114 loss: 5.842866897583008\n",
      "epoch 1 batch 115 loss: 5.812150001525879\n",
      "epoch 1 batch 116 loss: 5.757258892059326\n",
      "epoch 1 batch 117 loss: 5.778835296630859\n",
      "epoch 1 batch 118 loss: 5.811287879943848\n",
      "epoch 1 batch 119 loss: 5.79902458190918\n",
      "epoch 1 batch 120 loss: 5.855366230010986\n",
      "epoch 1 batch 121 loss: 5.833682060241699\n",
      "epoch 1 batch 122 loss: 5.798371315002441\n",
      "epoch 1 batch 123 loss: 5.799757957458496\n",
      "epoch 1 batch 124 loss: 5.776940822601318\n",
      "epoch loss: 6.039321571350097\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 41\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m,total_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;241m*\u001b[39mbatch_size)\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[43mconfig\u001b[49m\u001b[38;5;241m.\u001b[39msave_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstage1_loss.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     42\u001b[0m     f\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mstr\u001b[39m(total_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;241m*\u001b[39mbatch_size) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m5\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d479dbf-7797-4d3b-a272-ea0c835123f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3bc61-3d70-42ae-8c2a-10617b5cca69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
