{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48bf4baf-5b1f-4675-a54e-2465c271922c",
   "metadata": {},
   "source": [
    "# reference\n",
    "\n",
    "https://blog.csdn.net/qq_43027065/article/details/118657728"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c85da1ff-c829-4dda-be6c-98d511a8bc73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1 Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "bn1 BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "relu ReLU(inplace=True)\n",
      "maxpool MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "layer1 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer2 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer3 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (3): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (4): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (5): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "layer4 Sequential(\n",
      "  (0): Bottleneck(\n",
      "    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "    (downsample): Sequential(\n",
      "      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (1): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      "  (2): Bottleneck(\n",
      "    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (relu): ReLU(inplace=True)\n",
      "  )\n",
      ")\n",
      "avgpool AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "fc Linear(in_features=2048, out_features=1000, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# net.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.models.resnet import resnet50\n",
    "\n",
    "\n",
    "# stage one ,unsupervised learning\n",
    "class SimCLRStage1(nn.Module):\n",
    "    def __init__(self, feature_dim=128):\n",
    "        super(SimCLRStage1, self).__init__()\n",
    "\n",
    "        self.f = []\n",
    "        for name, module in resnet50().named_children():\n",
    "            if name == 'conv1':\n",
    "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
    "                self.f.append(module)\n",
    "        # encoder\n",
    "        self.f = nn.Sequential(*self.f)\n",
    "        # projection head\n",
    "        self.g = nn.Sequential(nn.Linear(2048, 512, bias=False),\n",
    "                               nn.BatchNorm1d(512),\n",
    "                               nn.ReLU(inplace=True),\n",
    "                               nn.Linear(512, feature_dim, bias=True))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.g(feature)\n",
    "        return F.normalize(feature, dim=-1), F.normalize(out, dim=-1)\n",
    "\n",
    "\n",
    "# stage two ,supervised learning\n",
    "class SimCLRStage2(torch.nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super(SimCLRStage2, self).__init__()\n",
    "        # encoder\n",
    "        self.f = SimCLRStage1().f\n",
    "        # classifier\n",
    "        self.fc = nn.Linear(2048, num_class, bias=True)\n",
    "\n",
    "        for param in self.f.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.f(x)\n",
    "        feature = torch.flatten(x, start_dim=1)\n",
    "        out = self.fc(feature)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Loss,self).__init__()\n",
    "\n",
    "    def forward(self,out_1,out_2,batch_size,temperature=0.5):\n",
    "        # 分母 ：X.X.T，再去掉对角线值，分析结果一行，可以看成它与除了这行外的其他行都进行了点积运算（包括out_1和out_2）,\n",
    "        # 而每一行为一个batch的一个取值，即一个输入图像的特征表示，\n",
    "        # 因此，X.X.T，再去掉对角线值表示，每个输入图像的特征与其所有输出特征（包括out_1和out_2）的点积，用点积来衡量相似性\n",
    "        # 加上exp操作，该操作实际计算了分母\n",
    "        # [2*B, D]\n",
    "        out = torch.cat([out_1, out_2], dim=0)\n",
    "        # [2*B, 2*B]\n",
    "        sim_matrix = torch.exp(torch.mm(out, out.t().contiguous()) / temperature)\n",
    "        mask = (torch.ones_like(sim_matrix) - torch.eye(2 * batch_size, device=sim_matrix.device)).bool()\n",
    "        # [2*B, 2*B-1]\n",
    "        sim_matrix = sim_matrix.masked_select(mask).view(2 * batch_size, -1)\n",
    "\n",
    "        # 分子： *为对应位置相乘，也是点积\n",
    "        # compute loss\n",
    "        pos_sim = torch.exp(torch.sum(out_1 * out_2, dim=-1) / temperature)\n",
    "        # [2*B]\n",
    "        pos_sim = torch.cat([pos_sim, pos_sim], dim=0)\n",
    "        return (- torch.log(pos_sim / sim_matrix.sum(dim=-1))).mean()\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    for name, module in resnet50().named_children():\n",
    "        print(name,module)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2a6b0b4-0050-4c50-aae6-10ec6d11684f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config.py\n",
    "import os\n",
    "from torchvision import transforms\n",
    "\n",
    "use_gpu=True\n",
    "gpu_name=1\n",
    "\n",
    "pre_model=os.path.join('pth','model.pth')\n",
    "\n",
    "save_path=\"pth\"\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(32),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fe3e262-de69-477f-b35b-c545a677a8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loaddataset.py\n",
    "from torchvision.datasets import CIFAR10\n",
    "from PIL import Image\n",
    "\n",
    "from utils.data_utils import ContrastivePairDataset\n",
    "\n",
    "class PreDataset(CIFAR10):\n",
    "    def __getitem__(self, item):\n",
    "        img,target=self.data[item],self.targets[item]\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            imgL = self.transform(img)\n",
    "            imgR = self.transform(img)\n",
    "\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "\n",
    "        return imgL, imgR, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d0b8a8-d1d0-4313-a7d6-5edbaace2da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "(tensor([[[-0.0835, -0.1998, -0.3355,  ..., -0.3549, -0.2967, -0.3161],\n",
      "         [-0.1804, -0.2580, -0.3355,  ..., -0.6457, -0.5487, -0.5293],\n",
      "         [-0.3743, -0.3936, -0.4130,  ..., -0.6844, -0.6263, -0.6069],\n",
      "         ...,\n",
      "         [-0.4906, -0.4518, -0.3355,  ...,  0.1879,  0.0716, -0.0447],\n",
      "         [-0.3549, -0.3355, -0.2386,  ...,  0.0910, -0.0641, -0.1998],\n",
      "         [-0.3161, -0.2967, -0.1998,  ..., -0.0641, -0.1610, -0.3161]],\n",
      "\n",
      "        [[-0.0386, -0.1566, -0.2942,  ..., -0.3139, -0.2549, -0.2746],\n",
      "         [-0.1369, -0.2156, -0.2942,  ..., -0.6089, -0.5106, -0.4909],\n",
      "         [-0.3336, -0.3532, -0.3729,  ..., -0.6482, -0.5892, -0.5696],\n",
      "         ...,\n",
      "         [-0.4516, -0.4122, -0.2942,  ...,  0.2368,  0.1188,  0.0008],\n",
      "         [-0.3139, -0.2942, -0.1959,  ...,  0.1384, -0.0189, -0.1566],\n",
      "         [-0.2746, -0.2549, -0.1566,  ..., -0.0189, -0.1172, -0.2746]],\n",
      "\n",
      "        [[ 0.1394,  0.0223, -0.1143,  ..., -0.1338, -0.0753, -0.0948],\n",
      "         [ 0.0418, -0.0362, -0.1143,  ..., -0.4264, -0.3289, -0.3094],\n",
      "         [-0.1533, -0.1728, -0.1923,  ..., -0.4655, -0.4069, -0.3874],\n",
      "         ...,\n",
      "         [-0.2704, -0.2313, -0.1143,  ...,  0.4125,  0.2954,  0.1784],\n",
      "         [-0.1338, -0.1143, -0.0167,  ...,  0.3149,  0.1589,  0.0223],\n",
      "         [-0.0948, -0.0753,  0.0223,  ...,  0.1589,  0.0613, -0.0948]]]), tensor([[[-1.4792, -1.2660, -0.9946,  ..., -0.8977, -0.7426, -0.6457],\n",
      "         [-1.4017, -1.2466, -0.9364,  ..., -0.8589, -0.8201, -0.7620],\n",
      "         [-1.2854, -1.1303, -0.8783,  ..., -0.7232, -0.7426, -0.7232],\n",
      "         ...,\n",
      "         [ 0.2461, -0.2192, -0.5100,  ...,  0.0328, -0.0835, -0.2386],\n",
      "         [ 0.8470,  0.1491, -0.3743,  ..., -0.1610, -0.2773, -0.3549],\n",
      "         [ 1.3704,  0.4787, -0.1998,  ..., -0.2773, -0.3355, -0.4130]],\n",
      "\n",
      "        [[-1.4546, -1.2382, -0.9629,  ..., -0.8646, -0.7072, -0.6089],\n",
      "         [-1.3759, -1.2186, -0.9039,  ..., -0.8252, -0.7859, -0.7269],\n",
      "         [-1.2579, -1.1006, -0.8449,  ..., -0.6876, -0.7072, -0.6876],\n",
      "         ...,\n",
      "         [ 0.2958, -0.1762, -0.4712,  ...,  0.0794, -0.0386, -0.1959],\n",
      "         [ 0.9054,  0.1974, -0.3336,  ..., -0.1172, -0.2352, -0.3139],\n",
      "         [ 1.4364,  0.5318, -0.1566,  ..., -0.2352, -0.2942, -0.3729]],\n",
      "\n",
      "        [[-1.2654, -1.0508, -0.7776,  ..., -0.6801, -0.5240, -0.4264],\n",
      "         [-1.1873, -1.0313, -0.7191,  ..., -0.6411, -0.6020, -0.5435],\n",
      "         [-1.0703, -0.9142, -0.6606,  ..., -0.5045, -0.5240, -0.5045],\n",
      "         ...,\n",
      "         [ 0.4710,  0.0028, -0.2899,  ...,  0.2564,  0.1394, -0.0167],\n",
      "         [ 1.0758,  0.3735, -0.1533,  ...,  0.0613, -0.0558, -0.1338],\n",
      "         [ 1.6026,  0.7052,  0.0223,  ..., -0.0558, -0.1143, -0.1923]]]), 6)\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ContrastivePairDataset('cifar10',contrastive_transform = train_transform)\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6da37bfe-b480-4831-a90b-edd8a8eba03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/remote-home/songtianwei/research/unlearn_multimodal/output/unlearn_self_supervised\"\n",
    "batch_size = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de1157a1-0166-4094-a5ec-72bd8a5e18ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainstage1.py\n",
    "import torch,argparse,os\n",
    "\n",
    "\n",
    "# train stage one\n",
    "def train():\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = torch.device(\"cuda:\" + str(\"1\"))\n",
    "        # 每次训练计算图改动较小使用，在开始前选取较优的基础算法（比如选择一种当前高效的卷积算法）\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "    print(\"current deveice:\", DEVICE)\n",
    "\n",
    "    train_data=torch.utils.data.DataLoader(train_dataset,batch_size=batch_size, shuffle=True, num_workers=16 , drop_last=True)\n",
    "\n",
    "    model = SimCLRStage1().to(DEVICE)\n",
    "    lossLR= Loss().to(DEVICE)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for epoch in range(1,1000+1):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch,(imgL,imgR,labels) in enumerate(train_data):\n",
    "            imgL,imgR,labels=imgL.to(DEVICE),imgR.to(DEVICE),labels.to(DEVICE)\n",
    "\n",
    "            _, pre_L=model(imgL)\n",
    "            _, pre_R=model(imgR)\n",
    "\n",
    "            loss=lossLR(pre_L,pre_R,batch_size)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            print(\"epoch\", epoch, \"batch\", batch, \"loss:\", loss.detach().item())\n",
    "            total_loss += loss.detach().item()\n",
    "\n",
    "        print(\"epoch loss:\",total_loss/len(train_dataset)*batch_size)\n",
    "\n",
    "        with open(os.path.join(save_path, \"stage1_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss/len(train_dataset)*batch_size) + \" \")\n",
    "\n",
    "        if epoch % 5==0:\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, 'model_stage1_epoch' + str(epoch) + '.pth'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d8ea4c0-ac54-47fc-a69e-2f4443b84544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current deveice: cuda:1\n",
      "epoch 1 batch 0 loss: 6.660226345062256\n",
      "epoch 1 batch 1 loss: 6.676202297210693\n",
      "epoch 1 batch 2 loss: 6.687729358673096\n",
      "epoch 1 batch 3 loss: 6.653592586517334\n",
      "epoch 1 batch 4 loss: 6.638753414154053\n",
      "epoch 1 batch 5 loss: 6.596418380737305\n",
      "epoch 1 batch 6 loss: 6.563289642333984\n",
      "epoch 1 batch 7 loss: 6.5818610191345215\n",
      "epoch 1 batch 8 loss: 6.543348789215088\n",
      "epoch 1 batch 9 loss: 6.51478910446167\n",
      "epoch 1 batch 10 loss: 6.489308834075928\n",
      "epoch 1 batch 11 loss: 6.487301349639893\n",
      "epoch 1 batch 12 loss: 6.49058198928833\n",
      "epoch 1 batch 13 loss: 6.386373043060303\n",
      "epoch 1 batch 14 loss: 6.417940616607666\n",
      "epoch 1 batch 15 loss: 6.422379016876221\n",
      "epoch 1 batch 16 loss: 6.345763206481934\n",
      "epoch 1 batch 17 loss: 6.446457386016846\n",
      "epoch 1 batch 18 loss: 6.408658504486084\n",
      "epoch 1 batch 19 loss: 6.421524524688721\n",
      "epoch 1 batch 20 loss: 6.430500507354736\n",
      "epoch 1 batch 21 loss: 6.350765228271484\n",
      "epoch 1 batch 22 loss: 6.31705904006958\n",
      "epoch 1 batch 23 loss: 6.346568584442139\n",
      "epoch 1 batch 24 loss: 6.347920894622803\n",
      "epoch 1 batch 25 loss: 6.285579204559326\n",
      "epoch 1 batch 26 loss: 6.236773490905762\n",
      "epoch 1 batch 27 loss: 6.241527080535889\n",
      "epoch 1 batch 28 loss: 6.246706485748291\n",
      "epoch 1 batch 29 loss: 6.186022758483887\n",
      "epoch 1 batch 30 loss: 6.242471694946289\n",
      "epoch 1 batch 31 loss: 6.179118633270264\n",
      "epoch 1 batch 32 loss: 6.246423244476318\n",
      "epoch 1 batch 33 loss: 6.284383296966553\n",
      "epoch 1 batch 34 loss: 6.152840614318848\n",
      "epoch 1 batch 35 loss: 6.305596828460693\n",
      "epoch 1 batch 36 loss: 6.186860084533691\n",
      "epoch 1 batch 37 loss: 6.138636112213135\n",
      "epoch 1 batch 38 loss: 6.2312188148498535\n",
      "epoch 1 batch 39 loss: 6.167042255401611\n",
      "epoch 1 batch 40 loss: 6.284775257110596\n",
      "epoch 1 batch 41 loss: 6.169188022613525\n",
      "epoch 1 batch 42 loss: 6.141373157501221\n",
      "epoch 1 batch 43 loss: 6.106683254241943\n",
      "epoch 1 batch 44 loss: 6.197947978973389\n",
      "epoch 1 batch 45 loss: 6.087036609649658\n",
      "epoch 1 batch 46 loss: 6.106592655181885\n",
      "epoch 1 batch 47 loss: 6.0612993240356445\n",
      "epoch 1 batch 48 loss: 6.056386470794678\n",
      "epoch 1 batch 49 loss: 6.024830341339111\n",
      "epoch 1 batch 50 loss: 6.110132217407227\n",
      "epoch 1 batch 51 loss: 6.046443939208984\n",
      "epoch 1 batch 52 loss: 6.09246301651001\n",
      "epoch 1 batch 53 loss: 6.067052364349365\n",
      "epoch 1 batch 54 loss: 6.112607479095459\n",
      "epoch 1 batch 55 loss: 6.027017593383789\n",
      "epoch 1 batch 56 loss: 6.031180381774902\n",
      "epoch 1 batch 57 loss: 6.117720127105713\n",
      "epoch 1 batch 58 loss: 6.0315375328063965\n",
      "epoch 1 batch 59 loss: 6.076398849487305\n",
      "epoch 1 batch 60 loss: 6.060566425323486\n",
      "epoch 1 batch 61 loss: 6.051473140716553\n",
      "epoch 1 batch 62 loss: 5.99915885925293\n",
      "epoch 1 batch 63 loss: 6.007788181304932\n",
      "epoch 1 batch 64 loss: 6.027074337005615\n",
      "epoch 1 batch 65 loss: 6.001837253570557\n",
      "epoch 1 batch 66 loss: 6.008669376373291\n",
      "epoch 1 batch 67 loss: 5.934996128082275\n",
      "epoch 1 batch 68 loss: 5.991495132446289\n",
      "epoch 1 batch 69 loss: 6.010515213012695\n",
      "epoch 1 batch 70 loss: 5.982572078704834\n",
      "epoch 1 batch 71 loss: 5.95844841003418\n",
      "epoch 1 batch 72 loss: 6.02282190322876\n",
      "epoch 1 batch 73 loss: 5.960044860839844\n",
      "epoch 1 batch 74 loss: 6.087882041931152\n",
      "epoch 1 batch 75 loss: 5.954681396484375\n",
      "epoch 1 batch 76 loss: 5.963976860046387\n",
      "epoch 1 batch 77 loss: 6.023276329040527\n",
      "epoch 1 batch 78 loss: 5.976393699645996\n",
      "epoch 1 batch 79 loss: 6.026263236999512\n",
      "epoch 1 batch 80 loss: 5.950912952423096\n",
      "epoch 1 batch 81 loss: 5.984663486480713\n",
      "epoch 1 batch 82 loss: 6.014082431793213\n",
      "epoch 1 batch 83 loss: 5.981639862060547\n",
      "epoch 1 batch 84 loss: 5.967623233795166\n",
      "epoch 1 batch 85 loss: 5.961167335510254\n",
      "epoch 1 batch 86 loss: 5.964991092681885\n",
      "epoch 1 batch 87 loss: 6.0071940422058105\n",
      "epoch 1 batch 88 loss: 6.0184550285339355\n",
      "epoch 1 batch 89 loss: 5.9869208335876465\n",
      "epoch 1 batch 90 loss: 5.986922264099121\n",
      "epoch 1 batch 91 loss: 5.953721046447754\n",
      "epoch 1 batch 92 loss: 5.922839164733887\n",
      "epoch 1 batch 93 loss: 6.005131721496582\n",
      "epoch 1 batch 94 loss: 5.924898624420166\n",
      "epoch 1 batch 95 loss: 5.974400520324707\n",
      "epoch 1 batch 96 loss: 5.9729485511779785\n",
      "epoch 1 batch 97 loss: 5.953166484832764\n",
      "epoch 1 batch 98 loss: 5.915211200714111\n",
      "epoch 1 batch 99 loss: 5.9486846923828125\n",
      "epoch 1 batch 100 loss: 5.940968990325928\n",
      "epoch 1 batch 101 loss: 5.880099773406982\n",
      "epoch 1 batch 102 loss: 5.936460971832275\n",
      "epoch 1 batch 103 loss: 5.898472785949707\n",
      "epoch 1 batch 104 loss: 5.881511211395264\n",
      "epoch 1 batch 105 loss: 5.9456939697265625\n",
      "epoch 1 batch 106 loss: 5.954557418823242\n",
      "epoch 1 batch 107 loss: 5.895883083343506\n",
      "epoch 1 batch 108 loss: 5.92814302444458\n",
      "epoch 1 batch 109 loss: 5.902270317077637\n",
      "epoch 1 batch 110 loss: 5.925965309143066\n",
      "epoch 1 batch 111 loss: 5.881463527679443\n",
      "epoch 1 batch 112 loss: 5.904730319976807\n",
      "epoch 1 batch 113 loss: 5.910662651062012\n",
      "epoch 1 batch 114 loss: 5.931981086730957\n",
      "epoch 1 batch 115 loss: 5.860010623931885\n",
      "epoch 1 batch 116 loss: 5.901413917541504\n",
      "epoch 1 batch 117 loss: 5.937822341918945\n",
      "epoch 1 batch 118 loss: 5.854361534118652\n",
      "epoch 1 batch 119 loss: 5.9025444984436035\n",
      "epoch 1 batch 120 loss: 5.879111289978027\n",
      "epoch 1 batch 121 loss: 5.896823406219482\n",
      "epoch 1 batch 122 loss: 5.921858787536621\n",
      "epoch 1 batch 123 loss: 5.905336856842041\n",
      "epoch 1 batch 124 loss: 5.905383110046387\n",
      "epoch loss: 6.117073768615723\n",
      "epoch 2 batch 0 loss: 5.916901111602783\n",
      "epoch 2 batch 1 loss: 5.863187313079834\n",
      "epoch 2 batch 2 loss: 5.883744239807129\n",
      "epoch 2 batch 3 loss: 5.880147457122803\n",
      "epoch 2 batch 4 loss: 5.865631103515625\n",
      "epoch 2 batch 5 loss: 5.906233787536621\n",
      "epoch 2 batch 6 loss: 5.822304725646973\n",
      "epoch 2 batch 7 loss: 5.8516058921813965\n",
      "epoch 2 batch 8 loss: 5.805325031280518\n",
      "epoch 2 batch 9 loss: 5.912567138671875\n",
      "epoch 2 batch 10 loss: 5.890748023986816\n",
      "epoch 2 batch 11 loss: 5.85874080657959\n",
      "epoch 2 batch 12 loss: 5.881363391876221\n",
      "epoch 2 batch 13 loss: 5.82420539855957\n",
      "epoch 2 batch 14 loss: 5.843312740325928\n",
      "epoch 2 batch 15 loss: 5.815283298492432\n",
      "epoch 2 batch 16 loss: 5.8173747062683105\n",
      "epoch 2 batch 17 loss: 5.852640151977539\n",
      "epoch 2 batch 18 loss: 5.815118789672852\n",
      "epoch 2 batch 19 loss: 5.795998573303223\n",
      "epoch 2 batch 20 loss: 5.799123287200928\n",
      "epoch 2 batch 21 loss: 5.75938081741333\n",
      "epoch 2 batch 22 loss: 5.828476905822754\n",
      "epoch 2 batch 23 loss: 5.809378623962402\n",
      "epoch 2 batch 24 loss: 5.774988651275635\n",
      "epoch 2 batch 25 loss: 5.758334636688232\n",
      "epoch 2 batch 26 loss: 5.828062057495117\n",
      "epoch 2 batch 27 loss: 5.795633316040039\n",
      "epoch 2 batch 28 loss: 5.8285064697265625\n",
      "epoch 2 batch 29 loss: 5.743367671966553\n",
      "epoch 2 batch 30 loss: 5.8195481300354\n",
      "epoch 2 batch 31 loss: 5.7467570304870605\n",
      "epoch 2 batch 32 loss: 5.793281555175781\n",
      "epoch 2 batch 33 loss: 5.792413234710693\n",
      "epoch 2 batch 34 loss: 5.735092639923096\n",
      "epoch 2 batch 35 loss: 5.760280609130859\n",
      "epoch 2 batch 36 loss: 5.749736309051514\n",
      "epoch 2 batch 37 loss: 5.794748306274414\n",
      "epoch 2 batch 38 loss: 5.77593469619751\n",
      "epoch 2 batch 39 loss: 5.810510158538818\n",
      "epoch 2 batch 40 loss: 5.813808441162109\n",
      "epoch 2 batch 41 loss: 5.786380290985107\n",
      "epoch 2 batch 42 loss: 5.767425537109375\n",
      "epoch 2 batch 43 loss: 5.769272327423096\n",
      "epoch 2 batch 44 loss: 5.809051513671875\n",
      "epoch 2 batch 45 loss: 5.718486785888672\n",
      "epoch 2 batch 46 loss: 5.813448429107666\n",
      "epoch 2 batch 47 loss: 5.799418926239014\n",
      "epoch 2 batch 48 loss: 5.7662353515625\n",
      "epoch 2 batch 49 loss: 5.774357795715332\n",
      "epoch 2 batch 50 loss: 5.718889236450195\n",
      "epoch 2 batch 51 loss: 5.744213581085205\n",
      "epoch 2 batch 52 loss: 5.74757719039917\n",
      "epoch 2 batch 53 loss: 5.733345985412598\n",
      "epoch 2 batch 54 loss: 5.715055465698242\n",
      "epoch 2 batch 55 loss: 5.740442752838135\n",
      "epoch 2 batch 56 loss: 5.765036582946777\n",
      "epoch 2 batch 57 loss: 5.771658897399902\n",
      "epoch 2 batch 58 loss: 5.780989646911621\n",
      "epoch 2 batch 59 loss: 5.77455472946167\n",
      "epoch 2 batch 60 loss: 5.711099624633789\n",
      "epoch 2 batch 61 loss: 5.768874168395996\n",
      "epoch 2 batch 62 loss: 5.730701923370361\n",
      "epoch 2 batch 63 loss: 5.781728267669678\n",
      "epoch 2 batch 64 loss: 5.701686859130859\n",
      "epoch 2 batch 65 loss: 5.752657413482666\n",
      "epoch 2 batch 66 loss: 5.664615154266357\n",
      "epoch 2 batch 67 loss: 5.722072601318359\n",
      "epoch 2 batch 68 loss: 5.708707809448242\n",
      "epoch 2 batch 69 loss: 5.719576358795166\n",
      "epoch 2 batch 70 loss: 5.691868305206299\n",
      "epoch 2 batch 71 loss: 5.753594875335693\n",
      "epoch 2 batch 72 loss: 5.716667175292969\n",
      "epoch 2 batch 73 loss: 5.723927021026611\n",
      "epoch 2 batch 74 loss: 5.682968616485596\n",
      "epoch 2 batch 75 loss: 5.772737503051758\n",
      "epoch 2 batch 76 loss: 5.67387580871582\n",
      "epoch 2 batch 77 loss: 5.70744514465332\n",
      "epoch 2 batch 78 loss: 5.79211950302124\n",
      "epoch 2 batch 79 loss: 5.726197242736816\n",
      "epoch 2 batch 80 loss: 5.730895042419434\n",
      "epoch 2 batch 81 loss: 5.733973026275635\n",
      "epoch 2 batch 82 loss: 5.743233680725098\n",
      "epoch 2 batch 83 loss: 5.671343803405762\n",
      "epoch 2 batch 84 loss: 5.613089084625244\n",
      "epoch 2 batch 85 loss: 5.717618465423584\n",
      "epoch 2 batch 86 loss: 5.719598293304443\n",
      "epoch 2 batch 87 loss: 5.6814775466918945\n",
      "epoch 2 batch 88 loss: 5.710795879364014\n",
      "epoch 2 batch 89 loss: 5.695940017700195\n",
      "epoch 2 batch 90 loss: 5.698211669921875\n",
      "epoch 2 batch 91 loss: 5.697787284851074\n",
      "epoch 2 batch 92 loss: 5.70701265335083\n",
      "epoch 2 batch 93 loss: 5.698892116546631\n",
      "epoch 2 batch 94 loss: 5.713466167449951\n",
      "epoch 2 batch 95 loss: 5.680617332458496\n",
      "epoch 2 batch 96 loss: 5.648016452789307\n",
      "epoch 2 batch 97 loss: 5.711467266082764\n",
      "epoch 2 batch 98 loss: 5.696154594421387\n",
      "epoch 2 batch 99 loss: 5.764702796936035\n",
      "epoch 2 batch 100 loss: 5.696509838104248\n",
      "epoch 2 batch 101 loss: 5.654277324676514\n",
      "epoch 2 batch 102 loss: 5.691718578338623\n",
      "epoch 2 batch 103 loss: 5.675599098205566\n",
      "epoch 2 batch 104 loss: 5.695387840270996\n",
      "epoch 2 batch 105 loss: 5.74259090423584\n",
      "epoch 2 batch 106 loss: 5.684728145599365\n",
      "epoch 2 batch 107 loss: 5.69761323928833\n",
      "epoch 2 batch 108 loss: 5.720438480377197\n",
      "epoch 2 batch 109 loss: 5.705109596252441\n",
      "epoch 2 batch 110 loss: 5.692792892456055\n",
      "epoch 2 batch 111 loss: 5.670944690704346\n",
      "epoch 2 batch 112 loss: 5.641414642333984\n",
      "epoch 2 batch 113 loss: 5.663906097412109\n",
      "epoch 2 batch 114 loss: 5.680064678192139\n",
      "epoch 2 batch 115 loss: 5.7062201499938965\n",
      "epoch 2 batch 116 loss: 5.724585056304932\n",
      "epoch 2 batch 117 loss: 5.694321155548096\n",
      "epoch 2 batch 118 loss: 5.661593914031982\n",
      "epoch 2 batch 119 loss: 5.690994739532471\n",
      "epoch 2 batch 120 loss: 5.70466423034668\n",
      "epoch 2 batch 121 loss: 5.60325288772583\n",
      "epoch 2 batch 122 loss: 5.676662921905518\n",
      "epoch 2 batch 123 loss: 5.655649185180664\n",
      "epoch 2 batch 124 loss: 5.70768928527832\n",
      "epoch loss: 5.750334259033203\n",
      "epoch 3 batch 0 loss: 5.637752532958984\n",
      "epoch 3 batch 1 loss: 5.687489032745361\n",
      "epoch 3 batch 2 loss: 5.715683460235596\n",
      "epoch 3 batch 3 loss: 5.652998924255371\n",
      "epoch 3 batch 4 loss: 5.6363043785095215\n",
      "epoch 3 batch 5 loss: 5.612083435058594\n",
      "epoch 3 batch 6 loss: 5.694589138031006\n",
      "epoch 3 batch 7 loss: 5.652558326721191\n",
      "epoch 3 batch 8 loss: 5.643754005432129\n",
      "epoch 3 batch 9 loss: 5.642266750335693\n",
      "epoch 3 batch 10 loss: 5.677584171295166\n",
      "epoch 3 batch 11 loss: 5.684417724609375\n",
      "epoch 3 batch 12 loss: 5.7138352394104\n",
      "epoch 3 batch 13 loss: 5.685003757476807\n",
      "epoch 3 batch 14 loss: 5.64415168762207\n",
      "epoch 3 batch 15 loss: 5.717718601226807\n",
      "epoch 3 batch 16 loss: 5.633045673370361\n",
      "epoch 3 batch 17 loss: 5.614921569824219\n",
      "epoch 3 batch 18 loss: 5.671298503875732\n",
      "epoch 3 batch 19 loss: 5.641729831695557\n",
      "epoch 3 batch 20 loss: 5.698894023895264\n",
      "epoch 3 batch 21 loss: 5.645239353179932\n",
      "epoch 3 batch 22 loss: 5.7025065422058105\n",
      "epoch 3 batch 23 loss: 5.676418304443359\n",
      "epoch 3 batch 24 loss: 5.687620162963867\n",
      "epoch 3 batch 25 loss: 5.655699253082275\n",
      "epoch 3 batch 26 loss: 5.673479080200195\n",
      "epoch 3 batch 27 loss: 5.585044860839844\n",
      "epoch 3 batch 28 loss: 5.69645881652832\n",
      "epoch 3 batch 29 loss: 5.668636322021484\n",
      "epoch 3 batch 30 loss: 5.651232719421387\n",
      "epoch 3 batch 31 loss: 5.651055812835693\n",
      "epoch 3 batch 32 loss: 5.623848915100098\n",
      "epoch 3 batch 33 loss: 5.710726737976074\n",
      "epoch 3 batch 34 loss: 5.661995887756348\n",
      "epoch 3 batch 35 loss: 5.693396091461182\n",
      "epoch 3 batch 36 loss: 5.736783981323242\n",
      "epoch 3 batch 37 loss: 5.703814506530762\n",
      "epoch 3 batch 38 loss: 5.671974182128906\n",
      "epoch 3 batch 39 loss: 5.67063570022583\n",
      "epoch 3 batch 40 loss: 5.720849514007568\n",
      "epoch 3 batch 41 loss: 5.691484451293945\n",
      "epoch 3 batch 42 loss: 5.665683269500732\n",
      "epoch 3 batch 43 loss: 5.707480430603027\n",
      "epoch 3 batch 44 loss: 5.684118747711182\n",
      "epoch 3 batch 45 loss: 5.660859107971191\n",
      "epoch 3 batch 46 loss: 5.678598880767822\n",
      "epoch 3 batch 47 loss: 5.66033411026001\n",
      "epoch 3 batch 48 loss: 5.6390061378479\n",
      "epoch 3 batch 49 loss: 5.740753173828125\n",
      "epoch 3 batch 50 loss: 5.6394243240356445\n",
      "epoch 3 batch 51 loss: 5.6898393630981445\n",
      "epoch 3 batch 52 loss: 5.712383270263672\n",
      "epoch 3 batch 53 loss: 5.657364368438721\n",
      "epoch 3 batch 54 loss: 5.686548709869385\n",
      "epoch 3 batch 55 loss: 5.583664417266846\n",
      "epoch 3 batch 56 loss: 5.657202243804932\n",
      "epoch 3 batch 57 loss: 5.6833319664001465\n",
      "epoch 3 batch 58 loss: 5.697858810424805\n",
      "epoch 3 batch 59 loss: 5.67402982711792\n",
      "epoch 3 batch 60 loss: 5.697786808013916\n",
      "epoch 3 batch 61 loss: 5.657244682312012\n",
      "epoch 3 batch 62 loss: 5.652713298797607\n",
      "epoch 3 batch 63 loss: 5.6871337890625\n",
      "epoch 3 batch 64 loss: 5.680536270141602\n",
      "epoch 3 batch 65 loss: 5.675363063812256\n",
      "epoch 3 batch 66 loss: 5.633883476257324\n",
      "epoch 3 batch 67 loss: 5.612072467803955\n",
      "epoch 3 batch 68 loss: 5.701343059539795\n",
      "epoch 3 batch 69 loss: 5.655784606933594\n",
      "epoch 3 batch 70 loss: 5.721567153930664\n",
      "epoch 3 batch 71 loss: 5.652137756347656\n",
      "epoch 3 batch 72 loss: 5.694126605987549\n",
      "epoch 3 batch 73 loss: 5.629692077636719\n",
      "epoch 3 batch 74 loss: 5.658031940460205\n",
      "epoch 3 batch 75 loss: 5.646849155426025\n",
      "epoch 3 batch 76 loss: 5.635522365570068\n",
      "epoch 3 batch 77 loss: 5.62893533706665\n",
      "epoch 3 batch 78 loss: 5.634987831115723\n",
      "epoch 3 batch 79 loss: 5.646923542022705\n",
      "epoch 3 batch 80 loss: 5.623509407043457\n",
      "epoch 3 batch 81 loss: 5.607378005981445\n",
      "epoch 3 batch 82 loss: 5.639883995056152\n",
      "epoch 3 batch 83 loss: 5.668672561645508\n",
      "epoch 3 batch 84 loss: 5.6161956787109375\n",
      "epoch 3 batch 85 loss: 5.648041248321533\n",
      "epoch 3 batch 86 loss: 5.624185085296631\n",
      "epoch 3 batch 87 loss: 5.642838001251221\n",
      "epoch 3 batch 88 loss: 5.600300312042236\n",
      "epoch 3 batch 89 loss: 5.6719231605529785\n",
      "epoch 3 batch 90 loss: 5.66038179397583\n",
      "epoch 3 batch 91 loss: 5.622716426849365\n",
      "epoch 3 batch 92 loss: 5.576780319213867\n",
      "epoch 3 batch 93 loss: 5.61818790435791\n",
      "epoch 3 batch 94 loss: 5.624104976654053\n",
      "epoch 3 batch 95 loss: 5.624362468719482\n",
      "epoch 3 batch 96 loss: 5.643107891082764\n",
      "epoch 3 batch 97 loss: 5.576399326324463\n",
      "epoch 3 batch 98 loss: 5.638354301452637\n",
      "epoch 3 batch 99 loss: 5.657236099243164\n",
      "epoch 3 batch 100 loss: 5.58805513381958\n",
      "epoch 3 batch 101 loss: 5.689747333526611\n",
      "epoch 3 batch 102 loss: 5.619635105133057\n",
      "epoch 3 batch 103 loss: 5.602177619934082\n",
      "epoch 3 batch 104 loss: 5.667363166809082\n",
      "epoch 3 batch 105 loss: 5.638150691986084\n",
      "epoch 3 batch 106 loss: 5.646709442138672\n",
      "epoch 3 batch 107 loss: 5.63999605178833\n",
      "epoch 3 batch 108 loss: 5.5517578125\n",
      "epoch 3 batch 109 loss: 5.612326145172119\n",
      "epoch 3 batch 110 loss: 5.538564205169678\n",
      "epoch 3 batch 111 loss: 5.546061992645264\n",
      "epoch 3 batch 112 loss: 5.632060527801514\n",
      "epoch 3 batch 113 loss: 5.557497978210449\n",
      "epoch 3 batch 114 loss: 5.568170547485352\n",
      "epoch 3 batch 115 loss: 5.571484088897705\n",
      "epoch 3 batch 116 loss: 5.554012298583984\n",
      "epoch 3 batch 117 loss: 5.6093902587890625\n",
      "epoch 3 batch 118 loss: 5.653021335601807\n",
      "epoch 3 batch 119 loss: 5.622369289398193\n",
      "epoch 3 batch 120 loss: 5.6069254875183105\n",
      "epoch 3 batch 121 loss: 5.62662935256958\n",
      "epoch 3 batch 122 loss: 5.597616672515869\n",
      "epoch 3 batch 123 loss: 5.608730316162109\n",
      "epoch 3 batch 124 loss: 5.606219291687012\n",
      "epoch loss: 5.649578636169434\n",
      "epoch 4 batch 0 loss: 5.616058826446533\n",
      "epoch 4 batch 1 loss: 5.602756977081299\n",
      "epoch 4 batch 2 loss: 5.597564697265625\n",
      "epoch 4 batch 3 loss: 5.601385593414307\n",
      "epoch 4 batch 4 loss: 5.586205959320068\n",
      "epoch 4 batch 5 loss: 5.602632522583008\n",
      "epoch 4 batch 6 loss: 5.630408763885498\n",
      "epoch 4 batch 7 loss: 5.599332332611084\n",
      "epoch 4 batch 8 loss: 5.611055850982666\n",
      "epoch 4 batch 9 loss: 5.6146674156188965\n",
      "epoch 4 batch 10 loss: 5.627260208129883\n",
      "epoch 4 batch 11 loss: 5.598780155181885\n",
      "epoch 4 batch 12 loss: 5.609279632568359\n",
      "epoch 4 batch 13 loss: 5.567677021026611\n",
      "epoch 4 batch 14 loss: 5.580577373504639\n",
      "epoch 4 batch 15 loss: 5.572220325469971\n",
      "epoch 4 batch 16 loss: 5.586889743804932\n",
      "epoch 4 batch 17 loss: 5.5801873207092285\n",
      "epoch 4 batch 18 loss: 5.594908237457275\n",
      "epoch 4 batch 19 loss: 5.553216934204102\n",
      "epoch 4 batch 20 loss: 5.62306022644043\n",
      "epoch 4 batch 21 loss: 5.598977088928223\n",
      "epoch 4 batch 22 loss: 5.542263031005859\n",
      "epoch 4 batch 23 loss: 5.546728610992432\n",
      "epoch 4 batch 24 loss: 5.610881328582764\n",
      "epoch 4 batch 25 loss: 5.552403450012207\n",
      "epoch 4 batch 26 loss: 5.588593006134033\n",
      "epoch 4 batch 27 loss: 5.613993167877197\n",
      "epoch 4 batch 28 loss: 5.616657733917236\n",
      "epoch 4 batch 29 loss: 5.629475116729736\n",
      "epoch 4 batch 30 loss: 5.544377326965332\n",
      "epoch 4 batch 31 loss: 5.543676853179932\n",
      "epoch 4 batch 32 loss: 5.551782608032227\n",
      "epoch 4 batch 33 loss: 5.576727390289307\n",
      "epoch 4 batch 34 loss: 5.609377384185791\n",
      "epoch 4 batch 35 loss: 5.589117527008057\n",
      "epoch 4 batch 36 loss: 5.592226982116699\n",
      "epoch 4 batch 37 loss: 5.557164669036865\n",
      "epoch 4 batch 38 loss: 5.535328388214111\n",
      "epoch 4 batch 39 loss: 5.572891712188721\n",
      "epoch 4 batch 40 loss: 5.578744411468506\n",
      "epoch 4 batch 41 loss: 5.559183120727539\n",
      "epoch 4 batch 42 loss: 5.604298114776611\n",
      "epoch 4 batch 43 loss: 5.542016506195068\n",
      "epoch 4 batch 44 loss: 5.576491355895996\n",
      "epoch 4 batch 45 loss: 5.598469257354736\n",
      "epoch 4 batch 46 loss: 5.541460037231445\n",
      "epoch 4 batch 47 loss: 5.558448314666748\n",
      "epoch 4 batch 48 loss: 5.607237339019775\n",
      "epoch 4 batch 49 loss: 5.543590545654297\n",
      "epoch 4 batch 50 loss: 5.548959732055664\n",
      "epoch 4 batch 51 loss: 5.570319652557373\n",
      "epoch 4 batch 52 loss: 5.55778169631958\n",
      "epoch 4 batch 53 loss: 5.545216083526611\n",
      "epoch 4 batch 54 loss: 5.585772514343262\n",
      "epoch 4 batch 55 loss: 5.569031715393066\n",
      "epoch 4 batch 56 loss: 5.570530891418457\n",
      "epoch 4 batch 57 loss: 5.584311008453369\n",
      "epoch 4 batch 58 loss: 5.535984516143799\n",
      "epoch 4 batch 59 loss: 5.562727928161621\n",
      "epoch 4 batch 60 loss: 5.567204475402832\n",
      "epoch 4 batch 61 loss: 5.555376052856445\n",
      "epoch 4 batch 62 loss: 5.592839241027832\n",
      "epoch 4 batch 63 loss: 5.580298900604248\n",
      "epoch 4 batch 64 loss: 5.489613056182861\n",
      "epoch 4 batch 65 loss: 5.558511734008789\n",
      "epoch 4 batch 66 loss: 5.604872703552246\n",
      "epoch 4 batch 67 loss: 5.605154514312744\n",
      "epoch 4 batch 68 loss: 5.56171989440918\n",
      "epoch 4 batch 69 loss: 5.560988426208496\n",
      "epoch 4 batch 70 loss: 5.587014198303223\n",
      "epoch 4 batch 71 loss: 5.536273002624512\n",
      "epoch 4 batch 72 loss: 5.628655910491943\n",
      "epoch 4 batch 73 loss: 5.591160297393799\n",
      "epoch 4 batch 74 loss: 5.618566989898682\n",
      "epoch 4 batch 75 loss: 5.577392578125\n",
      "epoch 4 batch 76 loss: 5.563097953796387\n",
      "epoch 4 batch 77 loss: 5.630651950836182\n",
      "epoch 4 batch 78 loss: 5.577852249145508\n",
      "epoch 4 batch 79 loss: 5.57002067565918\n",
      "epoch 4 batch 80 loss: 5.568965435028076\n",
      "epoch 4 batch 81 loss: 5.570718765258789\n",
      "epoch 4 batch 82 loss: 5.567834377288818\n",
      "epoch 4 batch 83 loss: 5.528624534606934\n",
      "epoch 4 batch 84 loss: 5.540167331695557\n",
      "epoch 4 batch 85 loss: 5.588375091552734\n",
      "epoch 4 batch 86 loss: 5.602382659912109\n",
      "epoch 4 batch 87 loss: 5.6149444580078125\n",
      "epoch 4 batch 88 loss: 5.5486063957214355\n",
      "epoch 4 batch 89 loss: 5.581455707550049\n",
      "epoch 4 batch 90 loss: 5.579555511474609\n",
      "epoch 4 batch 91 loss: 5.562366008758545\n",
      "epoch 4 batch 92 loss: 5.574733257293701\n",
      "epoch 4 batch 93 loss: 5.553671360015869\n",
      "epoch 4 batch 94 loss: 5.5431413650512695\n",
      "epoch 4 batch 95 loss: 5.596446990966797\n",
      "epoch 4 batch 96 loss: 5.572229862213135\n",
      "epoch 4 batch 97 loss: 5.544069290161133\n",
      "epoch 4 batch 98 loss: 5.558564186096191\n",
      "epoch 4 batch 99 loss: 5.550148963928223\n",
      "epoch 4 batch 100 loss: 5.557310581207275\n",
      "epoch 4 batch 101 loss: 5.545496940612793\n",
      "epoch 4 batch 102 loss: 5.586075305938721\n",
      "epoch 4 batch 103 loss: 5.539891242980957\n",
      "epoch 4 batch 104 loss: 5.564217567443848\n",
      "epoch 4 batch 105 loss: 5.552900314331055\n",
      "epoch 4 batch 106 loss: 5.57900857925415\n",
      "epoch 4 batch 107 loss: 5.537567138671875\n",
      "epoch 4 batch 108 loss: 5.528613567352295\n",
      "epoch 4 batch 109 loss: 5.481171131134033\n",
      "epoch 4 batch 110 loss: 5.532732009887695\n",
      "epoch 4 batch 111 loss: 5.521447658538818\n",
      "epoch 4 batch 112 loss: 5.547836780548096\n",
      "epoch 4 batch 113 loss: 5.513462543487549\n",
      "epoch 4 batch 114 loss: 5.547975540161133\n",
      "epoch 4 batch 115 loss: 5.600383758544922\n",
      "epoch 4 batch 116 loss: 5.503200531005859\n",
      "epoch 4 batch 117 loss: 5.539032936096191\n",
      "epoch 4 batch 118 loss: 5.539684295654297\n",
      "epoch 4 batch 119 loss: 5.55999231338501\n",
      "epoch 4 batch 120 loss: 5.59935998916626\n",
      "epoch 4 batch 121 loss: 5.5155816078186035\n",
      "epoch 4 batch 122 loss: 5.529882907867432\n",
      "epoch 4 batch 123 loss: 5.515214920043945\n",
      "epoch 4 batch 124 loss: 5.523242473602295\n",
      "epoch loss: 5.570679145812988\n",
      "epoch 5 batch 0 loss: 5.542263507843018\n",
      "epoch 5 batch 1 loss: 5.509051322937012\n",
      "epoch 5 batch 2 loss: 5.612704753875732\n",
      "epoch 5 batch 3 loss: 5.558164119720459\n",
      "epoch 5 batch 4 loss: 5.552459716796875\n",
      "epoch 5 batch 5 loss: 5.480833530426025\n",
      "epoch 5 batch 6 loss: 5.546606540679932\n",
      "epoch 5 batch 7 loss: 5.533694267272949\n",
      "epoch 5 batch 8 loss: 5.532420635223389\n",
      "epoch 5 batch 9 loss: 5.552156925201416\n",
      "epoch 5 batch 10 loss: 5.525224685668945\n",
      "epoch 5 batch 11 loss: 5.55864953994751\n",
      "epoch 5 batch 12 loss: 5.511183261871338\n",
      "epoch 5 batch 13 loss: 5.552870273590088\n",
      "epoch 5 batch 14 loss: 5.554085731506348\n",
      "epoch 5 batch 15 loss: 5.537330150604248\n",
      "epoch 5 batch 16 loss: 5.48503303527832\n",
      "epoch 5 batch 17 loss: 5.508051872253418\n",
      "epoch 5 batch 18 loss: 5.507513523101807\n",
      "epoch 5 batch 19 loss: 5.500002861022949\n",
      "epoch 5 batch 20 loss: 5.506542682647705\n",
      "epoch 5 batch 21 loss: 5.483094215393066\n",
      "epoch 5 batch 22 loss: 5.511251926422119\n",
      "epoch 5 batch 23 loss: 5.530995845794678\n",
      "epoch 5 batch 24 loss: 5.555283546447754\n",
      "epoch 5 batch 25 loss: 5.577128887176514\n",
      "epoch 5 batch 26 loss: 5.523459434509277\n",
      "epoch 5 batch 27 loss: 5.560769557952881\n",
      "epoch 5 batch 28 loss: 5.545322895050049\n",
      "epoch 5 batch 29 loss: 5.466499328613281\n",
      "epoch 5 batch 30 loss: 5.492238521575928\n",
      "epoch 5 batch 31 loss: 5.525809288024902\n",
      "epoch 5 batch 32 loss: 5.519474506378174\n",
      "epoch 5 batch 33 loss: 5.522551536560059\n",
      "epoch 5 batch 34 loss: 5.467415809631348\n",
      "epoch 5 batch 35 loss: 5.556484222412109\n",
      "epoch 5 batch 36 loss: 5.537027359008789\n",
      "epoch 5 batch 37 loss: 5.532314300537109\n",
      "epoch 5 batch 38 loss: 5.525337219238281\n",
      "epoch 5 batch 39 loss: 5.48423957824707\n",
      "epoch 5 batch 40 loss: 5.538395881652832\n",
      "epoch 5 batch 41 loss: 5.470101833343506\n",
      "epoch 5 batch 42 loss: 5.4983744621276855\n",
      "epoch 5 batch 43 loss: 5.526332855224609\n",
      "epoch 5 batch 44 loss: 5.55223274230957\n",
      "epoch 5 batch 45 loss: 5.539435863494873\n",
      "epoch 5 batch 46 loss: 5.524023532867432\n",
      "epoch 5 batch 47 loss: 5.533153057098389\n",
      "epoch 5 batch 48 loss: 5.511701583862305\n",
      "epoch 5 batch 49 loss: 5.484786033630371\n",
      "epoch 5 batch 50 loss: 5.551589012145996\n",
      "epoch 5 batch 51 loss: 5.534312725067139\n",
      "epoch 5 batch 52 loss: 5.535426139831543\n",
      "epoch 5 batch 53 loss: 5.507454872131348\n",
      "epoch 5 batch 54 loss: 5.517545223236084\n",
      "epoch 5 batch 55 loss: 5.480828762054443\n",
      "epoch 5 batch 56 loss: 5.542295932769775\n",
      "epoch 5 batch 57 loss: 5.546823501586914\n",
      "epoch 5 batch 58 loss: 5.48289680480957\n",
      "epoch 5 batch 59 loss: 5.514989376068115\n",
      "epoch 5 batch 60 loss: 5.537445545196533\n",
      "epoch 5 batch 61 loss: 5.498193264007568\n",
      "epoch 5 batch 62 loss: 5.54435920715332\n",
      "epoch 5 batch 63 loss: 5.501761436462402\n",
      "epoch 5 batch 64 loss: 5.48744535446167\n",
      "epoch 5 batch 65 loss: 5.492796421051025\n",
      "epoch 5 batch 66 loss: 5.495974540710449\n",
      "epoch 5 batch 67 loss: 5.52198600769043\n",
      "epoch 5 batch 68 loss: 5.496669769287109\n",
      "epoch 5 batch 69 loss: 5.512850761413574\n",
      "epoch 5 batch 70 loss: 5.574787616729736\n",
      "epoch 5 batch 71 loss: 5.491751194000244\n",
      "epoch 5 batch 72 loss: 5.509212017059326\n",
      "epoch 5 batch 73 loss: 5.526057720184326\n",
      "epoch 5 batch 74 loss: 5.526095867156982\n",
      "epoch 5 batch 75 loss: 5.48291015625\n",
      "epoch 5 batch 76 loss: 5.484556674957275\n",
      "epoch 5 batch 77 loss: 5.496273517608643\n",
      "epoch 5 batch 78 loss: 5.532425880432129\n",
      "epoch 5 batch 79 loss: 5.495998382568359\n",
      "epoch 5 batch 80 loss: 5.511098384857178\n",
      "epoch 5 batch 81 loss: 5.458494663238525\n",
      "epoch 5 batch 82 loss: 5.553189754486084\n",
      "epoch 5 batch 83 loss: 5.487544059753418\n",
      "epoch 5 batch 84 loss: 5.519106388092041\n",
      "epoch 5 batch 85 loss: 5.470577239990234\n",
      "epoch 5 batch 86 loss: 5.461668491363525\n",
      "epoch 5 batch 87 loss: 5.459948539733887\n",
      "epoch 5 batch 88 loss: 5.481019020080566\n",
      "epoch 5 batch 89 loss: 5.497306823730469\n",
      "epoch 5 batch 90 loss: 5.4945068359375\n",
      "epoch 5 batch 91 loss: 5.502116680145264\n",
      "epoch 5 batch 92 loss: 5.508035182952881\n",
      "epoch 5 batch 93 loss: 5.509379863739014\n",
      "epoch 5 batch 94 loss: 5.489199161529541\n",
      "epoch 5 batch 95 loss: 5.4843058586120605\n",
      "epoch 5 batch 96 loss: 5.489169597625732\n",
      "epoch 5 batch 97 loss: 5.476838111877441\n",
      "epoch 5 batch 98 loss: 5.4988226890563965\n",
      "epoch 5 batch 99 loss: 5.498260974884033\n",
      "epoch 5 batch 100 loss: 5.491259574890137\n",
      "epoch 5 batch 101 loss: 5.493439674377441\n",
      "epoch 5 batch 102 loss: 5.51422119140625\n",
      "epoch 5 batch 103 loss: 5.4549241065979\n",
      "epoch 5 batch 104 loss: 5.480727195739746\n",
      "epoch 5 batch 105 loss: 5.498729228973389\n",
      "epoch 5 batch 106 loss: 5.454317569732666\n",
      "epoch 5 batch 107 loss: 5.511283874511719\n",
      "epoch 5 batch 108 loss: 5.446265697479248\n",
      "epoch 5 batch 109 loss: 5.496235370635986\n",
      "epoch 5 batch 110 loss: 5.499195575714111\n",
      "epoch 5 batch 111 loss: 5.527745246887207\n",
      "epoch 5 batch 112 loss: 5.458465576171875\n",
      "epoch 5 batch 113 loss: 5.472362995147705\n",
      "epoch 5 batch 114 loss: 5.467020034790039\n",
      "epoch 5 batch 115 loss: 5.463940143585205\n",
      "epoch 5 batch 116 loss: 5.475092887878418\n",
      "epoch 5 batch 117 loss: 5.449423313140869\n",
      "epoch 5 batch 118 loss: 5.474223613739014\n",
      "epoch 5 batch 119 loss: 5.51708984375\n",
      "epoch 5 batch 120 loss: 5.503878116607666\n",
      "epoch 5 batch 121 loss: 5.453072547912598\n",
      "epoch 5 batch 122 loss: 5.5287370681762695\n",
      "epoch 5 batch 123 loss: 5.473818778991699\n",
      "epoch 5 batch 124 loss: 5.483898639678955\n",
      "epoch loss: 5.509005989074707\n",
      "epoch 6 batch 0 loss: 5.479975700378418\n",
      "epoch 6 batch 1 loss: 5.462699890136719\n",
      "epoch 6 batch 2 loss: 5.504587173461914\n",
      "epoch 6 batch 3 loss: 5.483715534210205\n",
      "epoch 6 batch 4 loss: 5.483891487121582\n",
      "epoch 6 batch 5 loss: 5.555333137512207\n",
      "epoch 6 batch 6 loss: 5.512519359588623\n",
      "epoch 6 batch 7 loss: 5.483736515045166\n",
      "epoch 6 batch 8 loss: 5.500565528869629\n",
      "epoch 6 batch 9 loss: 5.47953987121582\n",
      "epoch 6 batch 10 loss: 5.528162956237793\n",
      "epoch 6 batch 11 loss: 5.469663143157959\n",
      "epoch 6 batch 12 loss: 5.470276355743408\n",
      "epoch 6 batch 13 loss: 5.461115837097168\n",
      "epoch 6 batch 14 loss: 5.466936111450195\n",
      "epoch 6 batch 15 loss: 5.46019172668457\n",
      "epoch 6 batch 16 loss: 5.461676120758057\n",
      "epoch 6 batch 17 loss: 5.508170127868652\n",
      "epoch 6 batch 18 loss: 5.52790641784668\n",
      "epoch 6 batch 19 loss: 5.485395431518555\n",
      "epoch 6 batch 20 loss: 5.534252166748047\n",
      "epoch 6 batch 21 loss: 5.49110221862793\n",
      "epoch 6 batch 22 loss: 5.49721622467041\n",
      "epoch 6 batch 23 loss: 5.496721267700195\n",
      "epoch 6 batch 24 loss: 5.4872660636901855\n",
      "epoch 6 batch 25 loss: 5.497833251953125\n",
      "epoch 6 batch 26 loss: 5.481273651123047\n",
      "epoch 6 batch 27 loss: 5.545939922332764\n",
      "epoch 6 batch 28 loss: 5.521681785583496\n",
      "epoch 6 batch 29 loss: 5.478474140167236\n",
      "epoch 6 batch 30 loss: 5.452303409576416\n",
      "epoch 6 batch 31 loss: 5.452874660491943\n",
      "epoch 6 batch 32 loss: 5.475701808929443\n",
      "epoch 6 batch 33 loss: 5.5241193771362305\n",
      "epoch 6 batch 34 loss: 5.483282566070557\n",
      "epoch 6 batch 35 loss: 5.431992053985596\n",
      "epoch 6 batch 36 loss: 5.49619722366333\n",
      "epoch 6 batch 37 loss: 5.471434593200684\n",
      "epoch 6 batch 38 loss: 5.460636615753174\n",
      "epoch 6 batch 39 loss: 5.45849609375\n",
      "epoch 6 batch 40 loss: 5.545691967010498\n",
      "epoch 6 batch 41 loss: 5.477170467376709\n",
      "epoch 6 batch 42 loss: 5.539432048797607\n",
      "epoch 6 batch 43 loss: 5.454689979553223\n",
      "epoch 6 batch 44 loss: 5.529799461364746\n",
      "epoch 6 batch 45 loss: 5.488234519958496\n",
      "epoch 6 batch 46 loss: 5.499373435974121\n",
      "epoch 6 batch 47 loss: 5.460541248321533\n",
      "epoch 6 batch 48 loss: 5.498215675354004\n",
      "epoch 6 batch 49 loss: 5.4828972816467285\n",
      "epoch 6 batch 50 loss: 5.498340129852295\n",
      "epoch 6 batch 51 loss: 5.514374732971191\n",
      "epoch 6 batch 52 loss: 5.524081707000732\n",
      "epoch 6 batch 53 loss: 5.481363296508789\n",
      "epoch 6 batch 54 loss: 5.486214637756348\n",
      "epoch 6 batch 55 loss: 5.491080284118652\n",
      "epoch 6 batch 56 loss: 5.452597141265869\n",
      "epoch 6 batch 57 loss: 5.476085662841797\n",
      "epoch 6 batch 58 loss: 5.464797019958496\n",
      "epoch 6 batch 59 loss: 5.474507808685303\n",
      "epoch 6 batch 60 loss: 5.4611053466796875\n",
      "epoch 6 batch 61 loss: 5.464092254638672\n",
      "epoch 6 batch 62 loss: 5.471921920776367\n",
      "epoch 6 batch 63 loss: 5.468303203582764\n",
      "epoch 6 batch 64 loss: 5.430792808532715\n",
      "epoch 6 batch 65 loss: 5.482081413269043\n",
      "epoch 6 batch 66 loss: 5.436781883239746\n",
      "epoch 6 batch 67 loss: 5.470664978027344\n",
      "epoch 6 batch 68 loss: 5.456723690032959\n",
      "epoch 6 batch 69 loss: 5.482909202575684\n",
      "epoch 6 batch 70 loss: 5.47480583190918\n",
      "epoch 6 batch 71 loss: 5.512872219085693\n",
      "epoch 6 batch 72 loss: 5.467656135559082\n",
      "epoch 6 batch 73 loss: 5.493083477020264\n",
      "epoch 6 batch 74 loss: 5.47898530960083\n",
      "epoch 6 batch 75 loss: 5.4706315994262695\n",
      "epoch 6 batch 76 loss: 5.445101261138916\n",
      "epoch 6 batch 77 loss: 5.438577175140381\n",
      "epoch 6 batch 78 loss: 5.481728553771973\n",
      "epoch 6 batch 79 loss: 5.451087474822998\n",
      "epoch 6 batch 80 loss: 5.507136344909668\n",
      "epoch 6 batch 81 loss: 5.448203086853027\n",
      "epoch 6 batch 82 loss: 5.494513988494873\n",
      "epoch 6 batch 83 loss: 5.447268009185791\n",
      "epoch 6 batch 84 loss: 5.516449928283691\n",
      "epoch 6 batch 85 loss: 5.469264030456543\n",
      "epoch 6 batch 86 loss: 5.455580711364746\n",
      "epoch 6 batch 87 loss: 5.453218936920166\n",
      "epoch 6 batch 88 loss: 5.429090976715088\n",
      "epoch 6 batch 89 loss: 5.453922748565674\n",
      "epoch 6 batch 90 loss: 5.417844295501709\n",
      "epoch 6 batch 91 loss: 5.475902080535889\n",
      "epoch 6 batch 92 loss: 5.453719139099121\n",
      "epoch 6 batch 93 loss: 5.451684474945068\n",
      "epoch 6 batch 94 loss: 5.454938888549805\n",
      "epoch 6 batch 95 loss: 5.456456184387207\n",
      "epoch 6 batch 96 loss: 5.472901344299316\n",
      "epoch 6 batch 97 loss: 5.437443733215332\n",
      "epoch 6 batch 98 loss: 5.490193843841553\n",
      "epoch 6 batch 99 loss: 5.41999626159668\n",
      "epoch 6 batch 100 loss: 5.46592903137207\n",
      "epoch 6 batch 101 loss: 5.492583274841309\n",
      "epoch 6 batch 102 loss: 5.443614482879639\n",
      "epoch 6 batch 103 loss: 5.476537227630615\n",
      "epoch 6 batch 104 loss: 5.449472427368164\n",
      "epoch 6 batch 105 loss: 5.467979431152344\n",
      "epoch 6 batch 106 loss: 5.481053352355957\n",
      "epoch 6 batch 107 loss: 5.430511474609375\n",
      "epoch 6 batch 108 loss: 5.498173713684082\n",
      "epoch 6 batch 109 loss: 5.439990043640137\n",
      "epoch 6 batch 110 loss: 5.45057487487793\n",
      "epoch 6 batch 111 loss: 5.455128192901611\n",
      "epoch 6 batch 112 loss: 5.470956325531006\n",
      "epoch 6 batch 113 loss: 5.4301910400390625\n",
      "epoch 6 batch 114 loss: 5.465019226074219\n",
      "epoch 6 batch 115 loss: 5.490166664123535\n",
      "epoch 6 batch 116 loss: 5.478640079498291\n",
      "epoch 6 batch 117 loss: 5.443378925323486\n",
      "epoch 6 batch 118 loss: 5.449168682098389\n",
      "epoch 6 batch 119 loss: 5.4969892501831055\n",
      "epoch 6 batch 120 loss: 5.47589111328125\n",
      "epoch 6 batch 121 loss: 5.486721038818359\n",
      "epoch 6 batch 122 loss: 5.505607604980469\n",
      "epoch 6 batch 123 loss: 5.500972747802734\n",
      "epoch 6 batch 124 loss: 5.509742259979248\n",
      "epoch loss: 5.477319786071777\n",
      "epoch 7 batch 0 loss: 5.465559482574463\n",
      "epoch 7 batch 1 loss: 5.473247051239014\n",
      "epoch 7 batch 2 loss: 5.448780536651611\n",
      "epoch 7 batch 3 loss: 5.479914665222168\n",
      "epoch 7 batch 4 loss: 5.474503517150879\n",
      "epoch 7 batch 5 loss: 5.453919410705566\n",
      "epoch 7 batch 6 loss: 5.425804138183594\n",
      "epoch 7 batch 7 loss: 5.418182373046875\n",
      "epoch 7 batch 8 loss: 5.470587253570557\n",
      "epoch 7 batch 9 loss: 5.460910320281982\n",
      "epoch 7 batch 10 loss: 5.4175238609313965\n",
      "epoch 7 batch 11 loss: 5.446875095367432\n",
      "epoch 7 batch 12 loss: 5.467139720916748\n",
      "epoch 7 batch 13 loss: 5.442487716674805\n",
      "epoch 7 batch 14 loss: 5.475134372711182\n",
      "epoch 7 batch 15 loss: 5.445535182952881\n",
      "epoch 7 batch 16 loss: 5.4598917961120605\n",
      "epoch 7 batch 17 loss: 5.432466983795166\n",
      "epoch 7 batch 18 loss: 5.447274684906006\n",
      "epoch 7 batch 19 loss: 5.414169788360596\n",
      "epoch 7 batch 20 loss: 5.474202632904053\n",
      "epoch 7 batch 21 loss: 5.432613372802734\n",
      "epoch 7 batch 22 loss: 5.487046241760254\n",
      "epoch 7 batch 23 loss: 5.442370891571045\n",
      "epoch 7 batch 24 loss: 5.526695251464844\n",
      "epoch 7 batch 25 loss: 5.431412220001221\n",
      "epoch 7 batch 26 loss: 5.44169807434082\n",
      "epoch 7 batch 27 loss: 5.468924522399902\n",
      "epoch 7 batch 28 loss: 5.499116897583008\n",
      "epoch 7 batch 29 loss: 5.4480390548706055\n",
      "epoch 7 batch 30 loss: 5.475925922393799\n",
      "epoch 7 batch 31 loss: 5.455249309539795\n",
      "epoch 7 batch 32 loss: 5.429113864898682\n",
      "epoch 7 batch 33 loss: 5.416226863861084\n",
      "epoch 7 batch 34 loss: 5.438189506530762\n",
      "epoch 7 batch 35 loss: 5.472599983215332\n",
      "epoch 7 batch 36 loss: 5.476235389709473\n",
      "epoch 7 batch 37 loss: 5.475298881530762\n",
      "epoch 7 batch 38 loss: 5.466484069824219\n",
      "epoch 7 batch 39 loss: 5.461485862731934\n",
      "epoch 7 batch 40 loss: 5.446539878845215\n",
      "epoch 7 batch 41 loss: 5.393707275390625\n",
      "epoch 7 batch 42 loss: 5.406120300292969\n",
      "epoch 7 batch 43 loss: 5.431321144104004\n",
      "epoch 7 batch 44 loss: 5.448410511016846\n",
      "epoch 7 batch 45 loss: 5.451132297515869\n",
      "epoch 7 batch 46 loss: 5.478199481964111\n",
      "epoch 7 batch 47 loss: 5.446754455566406\n",
      "epoch 7 batch 48 loss: 5.426335334777832\n",
      "epoch 7 batch 49 loss: 5.481925010681152\n",
      "epoch 7 batch 50 loss: 5.445621967315674\n",
      "epoch 7 batch 51 loss: 5.448657512664795\n",
      "epoch 7 batch 52 loss: 5.418265342712402\n",
      "epoch 7 batch 53 loss: 5.408339023590088\n",
      "epoch 7 batch 54 loss: 5.402088642120361\n",
      "epoch 7 batch 55 loss: 5.448401927947998\n",
      "epoch 7 batch 56 loss: 5.463855266571045\n",
      "epoch 7 batch 57 loss: 5.455165863037109\n",
      "epoch 7 batch 58 loss: 5.479712009429932\n",
      "epoch 7 batch 59 loss: 5.419425964355469\n",
      "epoch 7 batch 60 loss: 5.419854640960693\n",
      "epoch 7 batch 61 loss: 5.445650577545166\n",
      "epoch 7 batch 62 loss: 5.454273700714111\n",
      "epoch 7 batch 63 loss: 5.452995300292969\n",
      "epoch 7 batch 64 loss: 5.441484451293945\n",
      "epoch 7 batch 65 loss: 5.489790916442871\n",
      "epoch 7 batch 66 loss: 5.470590114593506\n",
      "epoch 7 batch 67 loss: 5.399913787841797\n",
      "epoch 7 batch 68 loss: 5.427883148193359\n",
      "epoch 7 batch 69 loss: 5.432462215423584\n",
      "epoch 7 batch 70 loss: 5.46605110168457\n",
      "epoch 7 batch 71 loss: 5.451066017150879\n",
      "epoch 7 batch 72 loss: 5.435030937194824\n",
      "epoch 7 batch 73 loss: 5.423378944396973\n",
      "epoch 7 batch 74 loss: 5.447549343109131\n",
      "epoch 7 batch 75 loss: 5.483699798583984\n",
      "epoch 7 batch 76 loss: 5.449107646942139\n",
      "epoch 7 batch 77 loss: 5.45146369934082\n",
      "epoch 7 batch 78 loss: 5.427800178527832\n",
      "epoch 7 batch 79 loss: 5.432440280914307\n",
      "epoch 7 batch 80 loss: 5.42900276184082\n",
      "epoch 7 batch 81 loss: 5.392432689666748\n",
      "epoch 7 batch 82 loss: 5.439067363739014\n",
      "epoch 7 batch 83 loss: 5.380702495574951\n",
      "epoch 7 batch 84 loss: 5.468608856201172\n",
      "epoch 7 batch 85 loss: 5.416593551635742\n",
      "epoch 7 batch 86 loss: 5.441426753997803\n",
      "epoch 7 batch 87 loss: 5.4689435958862305\n",
      "epoch 7 batch 88 loss: 5.446053981781006\n",
      "epoch 7 batch 89 loss: 5.419308185577393\n",
      "epoch 7 batch 90 loss: 5.457511901855469\n",
      "epoch 7 batch 91 loss: 5.444380283355713\n",
      "epoch 7 batch 92 loss: 5.455124378204346\n",
      "epoch 7 batch 93 loss: 5.37837028503418\n",
      "epoch 7 batch 94 loss: 5.44207763671875\n",
      "epoch 7 batch 95 loss: 5.459578037261963\n",
      "epoch 7 batch 96 loss: 5.432177543640137\n",
      "epoch 7 batch 97 loss: 5.444878578186035\n",
      "epoch 7 batch 98 loss: 5.460556507110596\n",
      "epoch 7 batch 99 loss: 5.438154220581055\n",
      "epoch 7 batch 100 loss: 5.493831634521484\n",
      "epoch 7 batch 101 loss: 5.411961555480957\n",
      "epoch 7 batch 102 loss: 5.406293869018555\n",
      "epoch 7 batch 103 loss: 5.403141498565674\n",
      "epoch 7 batch 104 loss: 5.45811653137207\n",
      "epoch 7 batch 105 loss: 5.412728309631348\n",
      "epoch 7 batch 106 loss: 5.429545879364014\n",
      "epoch 7 batch 107 loss: 5.418061256408691\n",
      "epoch 7 batch 108 loss: 5.4074907302856445\n",
      "epoch 7 batch 109 loss: 5.41248893737793\n",
      "epoch 7 batch 110 loss: 5.43516731262207\n",
      "epoch 7 batch 111 loss: 5.421873092651367\n",
      "epoch 7 batch 112 loss: 5.422479152679443\n",
      "epoch 7 batch 113 loss: 5.426228046417236\n",
      "epoch 7 batch 114 loss: 5.424713134765625\n",
      "epoch 7 batch 115 loss: 5.410125732421875\n",
      "epoch 7 batch 116 loss: 5.392451286315918\n",
      "epoch 7 batch 117 loss: 5.469622611999512\n",
      "epoch 7 batch 118 loss: 5.4344658851623535\n",
      "epoch 7 batch 119 loss: 5.4254984855651855\n",
      "epoch 7 batch 120 loss: 5.435932636260986\n",
      "epoch 7 batch 121 loss: 5.4165802001953125\n",
      "epoch 7 batch 122 loss: 5.407327651977539\n",
      "epoch 7 batch 123 loss: 5.4150285720825195\n",
      "epoch 7 batch 124 loss: 5.458523273468018\n",
      "epoch loss: 5.4424474372863765\n",
      "epoch 8 batch 0 loss: 5.432190418243408\n",
      "epoch 8 batch 1 loss: 5.420808792114258\n",
      "epoch 8 batch 2 loss: 5.400524616241455\n",
      "epoch 8 batch 3 loss: 5.414204120635986\n",
      "epoch 8 batch 4 loss: 5.423909664154053\n",
      "epoch 8 batch 5 loss: 5.438283443450928\n",
      "epoch 8 batch 6 loss: 5.466616630554199\n",
      "epoch 8 batch 7 loss: 5.412035942077637\n",
      "epoch 8 batch 8 loss: 5.451395034790039\n",
      "epoch 8 batch 9 loss: 5.445317268371582\n",
      "epoch 8 batch 10 loss: 5.402885437011719\n",
      "epoch 8 batch 11 loss: 5.420936107635498\n",
      "epoch 8 batch 12 loss: 5.4252519607543945\n",
      "epoch 8 batch 13 loss: 5.458237648010254\n",
      "epoch 8 batch 14 loss: 5.424149036407471\n",
      "epoch 8 batch 15 loss: 5.3932952880859375\n",
      "epoch 8 batch 16 loss: 5.424295425415039\n",
      "epoch 8 batch 17 loss: 5.417192459106445\n",
      "epoch 8 batch 18 loss: 5.403167724609375\n",
      "epoch 8 batch 19 loss: 5.44434928894043\n",
      "epoch 8 batch 20 loss: 5.40374755859375\n",
      "epoch 8 batch 21 loss: 5.418417930603027\n",
      "epoch 8 batch 22 loss: 5.427924633026123\n",
      "epoch 8 batch 23 loss: 5.432533264160156\n",
      "epoch 8 batch 24 loss: 5.42071533203125\n",
      "epoch 8 batch 25 loss: 5.427353382110596\n",
      "epoch 8 batch 26 loss: 5.394789695739746\n",
      "epoch 8 batch 27 loss: 5.44129753112793\n",
      "epoch 8 batch 28 loss: 5.383911609649658\n",
      "epoch 8 batch 29 loss: 5.453592300415039\n",
      "epoch 8 batch 30 loss: 5.4363579750061035\n",
      "epoch 8 batch 31 loss: 5.434352397918701\n",
      "epoch 8 batch 32 loss: 5.393054485321045\n",
      "epoch 8 batch 33 loss: 5.455723762512207\n",
      "epoch 8 batch 34 loss: 5.473671913146973\n",
      "epoch 8 batch 35 loss: 5.410175800323486\n",
      "epoch 8 batch 36 loss: 5.474644660949707\n",
      "epoch 8 batch 37 loss: 5.428768157958984\n",
      "epoch 8 batch 38 loss: 5.468386173248291\n",
      "epoch 8 batch 39 loss: 5.423194408416748\n",
      "epoch 8 batch 40 loss: 5.447243690490723\n",
      "epoch 8 batch 41 loss: 5.450262546539307\n",
      "epoch 8 batch 42 loss: 5.438538074493408\n",
      "epoch 8 batch 43 loss: 5.45561408996582\n",
      "epoch 8 batch 44 loss: 5.398685932159424\n",
      "epoch 8 batch 45 loss: 5.378708839416504\n",
      "epoch 8 batch 46 loss: 5.441348552703857\n",
      "epoch 8 batch 47 loss: 5.411884784698486\n",
      "epoch 8 batch 48 loss: 5.448390007019043\n",
      "epoch 8 batch 49 loss: 5.39468240737915\n",
      "epoch 8 batch 50 loss: 5.418754577636719\n",
      "epoch 8 batch 51 loss: 5.418422222137451\n",
      "epoch 8 batch 52 loss: 5.410726070404053\n",
      "epoch 8 batch 53 loss: 5.433744430541992\n",
      "epoch 8 batch 54 loss: 5.4071946144104\n",
      "epoch 8 batch 55 loss: 5.384158134460449\n",
      "epoch 8 batch 56 loss: 5.42153787612915\n",
      "epoch 8 batch 57 loss: 5.4473557472229\n",
      "epoch 8 batch 58 loss: 5.434982776641846\n",
      "epoch 8 batch 59 loss: 5.388453960418701\n",
      "epoch 8 batch 60 loss: 5.385948181152344\n",
      "epoch 8 batch 61 loss: 5.417147636413574\n",
      "epoch 8 batch 62 loss: 5.413051605224609\n",
      "epoch 8 batch 63 loss: 5.474557876586914\n",
      "epoch 8 batch 64 loss: 5.4051947593688965\n",
      "epoch 8 batch 65 loss: 5.416399955749512\n",
      "epoch 8 batch 66 loss: 5.433818340301514\n",
      "epoch 8 batch 67 loss: 5.411747932434082\n",
      "epoch 8 batch 68 loss: 5.412705421447754\n",
      "epoch 8 batch 69 loss: 5.417518138885498\n",
      "epoch 8 batch 70 loss: 5.425530910491943\n",
      "epoch 8 batch 71 loss: 5.40037202835083\n",
      "epoch 8 batch 72 loss: 5.449511528015137\n",
      "epoch 8 batch 73 loss: 5.432465553283691\n",
      "epoch 8 batch 74 loss: 5.434976577758789\n",
      "epoch 8 batch 75 loss: 5.3932976722717285\n",
      "epoch 8 batch 76 loss: 5.41179084777832\n",
      "epoch 8 batch 77 loss: 5.448815822601318\n",
      "epoch 8 batch 78 loss: 5.373557090759277\n",
      "epoch 8 batch 79 loss: 5.402470111846924\n",
      "epoch 8 batch 80 loss: 5.4314494132995605\n",
      "epoch 8 batch 81 loss: 5.425844192504883\n",
      "epoch 8 batch 82 loss: 5.395516872406006\n",
      "epoch 8 batch 83 loss: 5.4270734786987305\n",
      "epoch 8 batch 84 loss: 5.452399730682373\n",
      "epoch 8 batch 85 loss: 5.424193382263184\n",
      "epoch 8 batch 86 loss: 5.451918601989746\n",
      "epoch 8 batch 87 loss: 5.410067081451416\n",
      "epoch 8 batch 88 loss: 5.447780609130859\n",
      "epoch 8 batch 89 loss: 5.423742771148682\n",
      "epoch 8 batch 90 loss: 5.438185214996338\n",
      "epoch 8 batch 91 loss: 5.417050838470459\n",
      "epoch 8 batch 92 loss: 5.38462495803833\n",
      "epoch 8 batch 93 loss: 5.447756290435791\n",
      "epoch 8 batch 94 loss: 5.43553352355957\n",
      "epoch 8 batch 95 loss: 5.434995174407959\n",
      "epoch 8 batch 96 loss: 5.464066028594971\n",
      "epoch 8 batch 97 loss: 5.410113334655762\n",
      "epoch 8 batch 98 loss: 5.407987117767334\n",
      "epoch 8 batch 99 loss: 5.420129776000977\n",
      "epoch 8 batch 100 loss: 5.408661842346191\n",
      "epoch 8 batch 101 loss: 5.419063568115234\n",
      "epoch 8 batch 102 loss: 5.391401290893555\n",
      "epoch 8 batch 103 loss: 5.404208660125732\n",
      "epoch 8 batch 104 loss: 5.411825656890869\n",
      "epoch 8 batch 105 loss: 5.433395862579346\n",
      "epoch 8 batch 106 loss: 5.404876708984375\n",
      "epoch 8 batch 107 loss: 5.367012977600098\n",
      "epoch 8 batch 108 loss: 5.398566722869873\n",
      "epoch 8 batch 109 loss: 5.387239933013916\n",
      "epoch 8 batch 110 loss: 5.403826713562012\n",
      "epoch 8 batch 111 loss: 5.442766189575195\n",
      "epoch 8 batch 112 loss: 5.39189338684082\n",
      "epoch 8 batch 113 loss: 5.413000583648682\n",
      "epoch 8 batch 114 loss: 5.377984523773193\n",
      "epoch 8 batch 115 loss: 5.354790687561035\n",
      "epoch 8 batch 116 loss: 5.371829509735107\n",
      "epoch 8 batch 117 loss: 5.43363094329834\n",
      "epoch 8 batch 118 loss: 5.418948173522949\n",
      "epoch 8 batch 119 loss: 5.4151611328125\n",
      "epoch 8 batch 120 loss: 5.421688079833984\n",
      "epoch 8 batch 121 loss: 5.418392658233643\n",
      "epoch 8 batch 122 loss: 5.383049964904785\n",
      "epoch 8 batch 123 loss: 5.4186835289001465\n",
      "epoch 8 batch 124 loss: 5.447447776794434\n",
      "epoch loss: 5.420824016571045\n",
      "epoch 9 batch 0 loss: 5.415608882904053\n",
      "epoch 9 batch 1 loss: 5.379772186279297\n",
      "epoch 9 batch 2 loss: 5.388611793518066\n",
      "epoch 9 batch 3 loss: 5.395010471343994\n",
      "epoch 9 batch 4 loss: 5.434291839599609\n",
      "epoch 9 batch 5 loss: 5.390710353851318\n",
      "epoch 9 batch 6 loss: 5.442493915557861\n",
      "epoch 9 batch 7 loss: 5.385331153869629\n",
      "epoch 9 batch 8 loss: 5.43065071105957\n",
      "epoch 9 batch 9 loss: 5.402636528015137\n",
      "epoch 9 batch 10 loss: 5.38447380065918\n",
      "epoch 9 batch 11 loss: 5.385185241699219\n",
      "epoch 9 batch 12 loss: 5.407004356384277\n",
      "epoch 9 batch 13 loss: 5.389016151428223\n",
      "epoch 9 batch 14 loss: 5.382864952087402\n",
      "epoch 9 batch 15 loss: 5.43265962600708\n",
      "epoch 9 batch 16 loss: 5.376046180725098\n",
      "epoch 9 batch 17 loss: 5.392411231994629\n",
      "epoch 9 batch 18 loss: 5.369456768035889\n",
      "epoch 9 batch 19 loss: 5.352880954742432\n",
      "epoch 9 batch 20 loss: 5.388844966888428\n",
      "epoch 9 batch 21 loss: 5.429785251617432\n",
      "epoch 9 batch 22 loss: 5.405665397644043\n",
      "epoch 9 batch 23 loss: 5.400063514709473\n",
      "epoch 9 batch 24 loss: 5.400173664093018\n",
      "epoch 9 batch 25 loss: 5.4046502113342285\n",
      "epoch 9 batch 26 loss: 5.421214580535889\n",
      "epoch 9 batch 27 loss: 5.38761043548584\n",
      "epoch 9 batch 28 loss: 5.41323709487915\n",
      "epoch 9 batch 29 loss: 5.400606632232666\n",
      "epoch 9 batch 30 loss: 5.413251876831055\n",
      "epoch 9 batch 31 loss: 5.423671722412109\n",
      "epoch 9 batch 32 loss: 5.409801959991455\n",
      "epoch 9 batch 33 loss: 5.420292854309082\n",
      "epoch 9 batch 34 loss: 5.463322639465332\n",
      "epoch 9 batch 35 loss: 5.397306442260742\n",
      "epoch 9 batch 36 loss: 5.408229827880859\n",
      "epoch 9 batch 37 loss: 5.4068284034729\n",
      "epoch 9 batch 38 loss: 5.411787986755371\n",
      "epoch 9 batch 39 loss: 5.363778114318848\n",
      "epoch 9 batch 40 loss: 5.384579181671143\n",
      "epoch 9 batch 41 loss: 5.40329647064209\n",
      "epoch 9 batch 42 loss: 5.3911333084106445\n",
      "epoch 9 batch 43 loss: 5.367709159851074\n",
      "epoch 9 batch 44 loss: 5.393320083618164\n",
      "epoch 9 batch 45 loss: 5.431070327758789\n",
      "epoch 9 batch 46 loss: 5.397636413574219\n",
      "epoch 9 batch 47 loss: 5.412769794464111\n",
      "epoch 9 batch 48 loss: 5.423961162567139\n",
      "epoch 9 batch 49 loss: 5.432058811187744\n",
      "epoch 9 batch 50 loss: 5.402960300445557\n",
      "epoch 9 batch 51 loss: 5.422967433929443\n",
      "epoch 9 batch 52 loss: 5.374808311462402\n",
      "epoch 9 batch 53 loss: 5.401476860046387\n",
      "epoch 9 batch 54 loss: 5.412448406219482\n",
      "epoch 9 batch 55 loss: 5.394342422485352\n",
      "epoch 9 batch 56 loss: 5.3876519203186035\n",
      "epoch 9 batch 57 loss: 5.413245677947998\n",
      "epoch 9 batch 58 loss: 5.425628662109375\n",
      "epoch 9 batch 59 loss: 5.415737152099609\n",
      "epoch 9 batch 60 loss: 5.416840553283691\n",
      "epoch 9 batch 61 loss: 5.371762752532959\n",
      "epoch 9 batch 62 loss: 5.454597473144531\n",
      "epoch 9 batch 63 loss: 5.414608478546143\n",
      "epoch 9 batch 64 loss: 5.37840461730957\n",
      "epoch 9 batch 65 loss: 5.373321533203125\n",
      "epoch 9 batch 66 loss: 5.3930277824401855\n",
      "epoch 9 batch 67 loss: 5.387156963348389\n",
      "epoch 9 batch 68 loss: 5.367895603179932\n",
      "epoch 9 batch 69 loss: 5.374588489532471\n",
      "epoch 9 batch 70 loss: 5.394084453582764\n",
      "epoch 9 batch 71 loss: 5.428589820861816\n",
      "epoch 9 batch 72 loss: 5.390957355499268\n",
      "epoch 9 batch 73 loss: 5.4209418296813965\n",
      "epoch 9 batch 74 loss: 5.395313739776611\n",
      "epoch 9 batch 75 loss: 5.404471397399902\n",
      "epoch 9 batch 76 loss: 5.367654323577881\n",
      "epoch 9 batch 77 loss: 5.397646427154541\n",
      "epoch 9 batch 78 loss: 5.417112827301025\n",
      "epoch 9 batch 79 loss: 5.397459506988525\n",
      "epoch 9 batch 80 loss: 5.374655723571777\n",
      "epoch 9 batch 81 loss: 5.349019527435303\n",
      "epoch 9 batch 82 loss: 5.392938613891602\n",
      "epoch 9 batch 83 loss: 5.363133430480957\n",
      "epoch 9 batch 84 loss: 5.376589298248291\n",
      "epoch 9 batch 85 loss: 5.37490177154541\n",
      "epoch 9 batch 86 loss: 5.396115779876709\n",
      "epoch 9 batch 87 loss: 5.371335506439209\n",
      "epoch 9 batch 88 loss: 5.451382160186768\n",
      "epoch 9 batch 89 loss: 5.358709812164307\n",
      "epoch 9 batch 90 loss: 5.370229244232178\n",
      "epoch 9 batch 91 loss: 5.36616849899292\n",
      "epoch 9 batch 92 loss: 5.356005668640137\n",
      "epoch 9 batch 93 loss: 5.375985145568848\n",
      "epoch 9 batch 94 loss: 5.397841453552246\n",
      "epoch 9 batch 95 loss: 5.388899803161621\n",
      "epoch 9 batch 96 loss: 5.40690279006958\n",
      "epoch 9 batch 97 loss: 5.405781269073486\n",
      "epoch 9 batch 98 loss: 5.409755229949951\n",
      "epoch 9 batch 99 loss: 5.410500526428223\n",
      "epoch 9 batch 100 loss: 5.431941032409668\n",
      "epoch 9 batch 101 loss: 5.39375114440918\n",
      "epoch 9 batch 102 loss: 5.382829666137695\n",
      "epoch 9 batch 103 loss: 5.414028167724609\n",
      "epoch 9 batch 104 loss: 5.423000335693359\n",
      "epoch 9 batch 105 loss: 5.391701698303223\n",
      "epoch 9 batch 106 loss: 5.416683197021484\n",
      "epoch 9 batch 107 loss: 5.435302734375\n",
      "epoch 9 batch 108 loss: 5.422321796417236\n",
      "epoch 9 batch 109 loss: 5.412700176239014\n",
      "epoch 9 batch 110 loss: 5.397346019744873\n",
      "epoch 9 batch 111 loss: 5.381340980529785\n",
      "epoch 9 batch 112 loss: 5.371193885803223\n",
      "epoch 9 batch 113 loss: 5.429844856262207\n",
      "epoch 9 batch 114 loss: 5.4193034172058105\n",
      "epoch 9 batch 115 loss: 5.394539833068848\n",
      "epoch 9 batch 116 loss: 5.421321392059326\n",
      "epoch 9 batch 117 loss: 5.457494258880615\n",
      "epoch 9 batch 118 loss: 5.413337707519531\n",
      "epoch 9 batch 119 loss: 5.394904613494873\n",
      "epoch 9 batch 120 loss: 5.4220404624938965\n",
      "epoch 9 batch 121 loss: 5.3887128829956055\n",
      "epoch 9 batch 122 loss: 5.390028476715088\n",
      "epoch 9 batch 123 loss: 5.407661437988281\n",
      "epoch 9 batch 124 loss: 5.3769025802612305\n",
      "epoch loss: 5.400236499786377\n",
      "epoch 10 batch 0 loss: 5.413668632507324\n",
      "epoch 10 batch 1 loss: 5.407468318939209\n",
      "epoch 10 batch 2 loss: 5.3916521072387695\n",
      "epoch 10 batch 3 loss: 5.415199279785156\n",
      "epoch 10 batch 4 loss: 5.427844047546387\n",
      "epoch 10 batch 5 loss: 5.412744045257568\n",
      "epoch 10 batch 6 loss: 5.377459526062012\n",
      "epoch 10 batch 7 loss: 5.445096492767334\n",
      "epoch 10 batch 8 loss: 5.354076862335205\n",
      "epoch 10 batch 9 loss: 5.364251613616943\n",
      "epoch 10 batch 10 loss: 5.374143600463867\n",
      "epoch 10 batch 11 loss: 5.408541202545166\n",
      "epoch 10 batch 12 loss: 5.394198894500732\n",
      "epoch 10 batch 13 loss: 5.354762554168701\n",
      "epoch 10 batch 14 loss: 5.394374847412109\n",
      "epoch 10 batch 15 loss: 5.36133337020874\n",
      "epoch 10 batch 16 loss: 5.410614013671875\n",
      "epoch 10 batch 17 loss: 5.3859124183654785\n",
      "epoch 10 batch 18 loss: 5.392249584197998\n",
      "epoch 10 batch 19 loss: 5.3894944190979\n",
      "epoch 10 batch 20 loss: 5.364601135253906\n",
      "epoch 10 batch 21 loss: 5.346247673034668\n",
      "epoch 10 batch 22 loss: 5.369304180145264\n",
      "epoch 10 batch 23 loss: 5.376010417938232\n",
      "epoch 10 batch 24 loss: 5.457126140594482\n",
      "epoch 10 batch 25 loss: 5.37523078918457\n",
      "epoch 10 batch 26 loss: 5.4068217277526855\n",
      "epoch 10 batch 27 loss: 5.39805793762207\n",
      "epoch 10 batch 28 loss: 5.372600078582764\n",
      "epoch 10 batch 29 loss: 5.329838275909424\n",
      "epoch 10 batch 30 loss: 5.351223468780518\n",
      "epoch 10 batch 31 loss: 5.443989276885986\n",
      "epoch 10 batch 32 loss: 5.414287090301514\n",
      "epoch 10 batch 33 loss: 5.390005588531494\n",
      "epoch 10 batch 34 loss: 5.355689525604248\n",
      "epoch 10 batch 35 loss: 5.395605564117432\n",
      "epoch 10 batch 36 loss: 5.376487731933594\n",
      "epoch 10 batch 37 loss: 5.393502235412598\n",
      "epoch 10 batch 38 loss: 5.369243144989014\n",
      "epoch 10 batch 39 loss: 5.365129470825195\n",
      "epoch 10 batch 40 loss: 5.413321018218994\n",
      "epoch 10 batch 41 loss: 5.427017688751221\n",
      "epoch 10 batch 42 loss: 5.4031524658203125\n",
      "epoch 10 batch 43 loss: 5.347741603851318\n",
      "epoch 10 batch 44 loss: 5.377408504486084\n",
      "epoch 10 batch 45 loss: 5.363221168518066\n",
      "epoch 10 batch 46 loss: 5.387092113494873\n",
      "epoch 10 batch 47 loss: 5.386246204376221\n",
      "epoch 10 batch 48 loss: 5.414465427398682\n",
      "epoch 10 batch 49 loss: 5.393862247467041\n",
      "epoch 10 batch 50 loss: 5.394843578338623\n",
      "epoch 10 batch 51 loss: 5.384406566619873\n",
      "epoch 10 batch 52 loss: 5.329770565032959\n",
      "epoch 10 batch 53 loss: 5.391467571258545\n",
      "epoch 10 batch 54 loss: 5.386519432067871\n",
      "epoch 10 batch 55 loss: 5.398553848266602\n",
      "epoch 10 batch 56 loss: 5.351510047912598\n",
      "epoch 10 batch 57 loss: 5.412249565124512\n",
      "epoch 10 batch 58 loss: 5.325722694396973\n",
      "epoch 10 batch 59 loss: 5.390634536743164\n",
      "epoch 10 batch 60 loss: 5.36881160736084\n",
      "epoch 10 batch 61 loss: 5.361289024353027\n",
      "epoch 10 batch 62 loss: 5.32636833190918\n",
      "epoch 10 batch 63 loss: 5.373526573181152\n",
      "epoch 10 batch 64 loss: 5.385791301727295\n",
      "epoch 10 batch 65 loss: 5.352241039276123\n",
      "epoch 10 batch 66 loss: 5.340338230133057\n",
      "epoch 10 batch 67 loss: 5.373320579528809\n",
      "epoch 10 batch 68 loss: 5.398309230804443\n",
      "epoch 10 batch 69 loss: 5.381156921386719\n",
      "epoch 10 batch 70 loss: 5.407429218292236\n",
      "epoch 10 batch 71 loss: 5.361145973205566\n",
      "epoch 10 batch 72 loss: 5.3363518714904785\n",
      "epoch 10 batch 73 loss: 5.36279821395874\n",
      "epoch 10 batch 74 loss: 5.385631084442139\n",
      "epoch 10 batch 75 loss: 5.336036205291748\n",
      "epoch 10 batch 76 loss: 5.372387409210205\n",
      "epoch 10 batch 77 loss: 5.371020793914795\n",
      "epoch 10 batch 78 loss: 5.382546424865723\n",
      "epoch 10 batch 79 loss: 5.368981838226318\n",
      "epoch 10 batch 80 loss: 5.381340026855469\n",
      "epoch 10 batch 81 loss: 5.361066818237305\n",
      "epoch 10 batch 82 loss: 5.40504264831543\n",
      "epoch 10 batch 83 loss: 5.348973274230957\n",
      "epoch 10 batch 84 loss: 5.399537086486816\n",
      "epoch 10 batch 85 loss: 5.405606746673584\n",
      "epoch 10 batch 86 loss: 5.354111194610596\n",
      "epoch 10 batch 87 loss: 5.364141941070557\n",
      "epoch 10 batch 88 loss: 5.3948564529418945\n",
      "epoch 10 batch 89 loss: 5.357481479644775\n",
      "epoch 10 batch 90 loss: 5.373221397399902\n",
      "epoch 10 batch 91 loss: 5.364358901977539\n",
      "epoch 10 batch 92 loss: 5.356478214263916\n",
      "epoch 10 batch 93 loss: 5.366714954376221\n",
      "epoch 10 batch 94 loss: 5.329897403717041\n",
      "epoch 10 batch 95 loss: 5.360034465789795\n",
      "epoch 10 batch 96 loss: 5.3575439453125\n",
      "epoch 10 batch 97 loss: 5.380779266357422\n",
      "epoch 10 batch 98 loss: 5.389604568481445\n",
      "epoch 10 batch 99 loss: 5.379931449890137\n",
      "epoch 10 batch 100 loss: 5.354407787322998\n",
      "epoch 10 batch 101 loss: 5.352865695953369\n",
      "epoch 10 batch 102 loss: 5.3181352615356445\n",
      "epoch 10 batch 103 loss: 5.384559154510498\n",
      "epoch 10 batch 104 loss: 5.376631736755371\n",
      "epoch 10 batch 105 loss: 5.3387885093688965\n",
      "epoch 10 batch 106 loss: 5.407637596130371\n",
      "epoch 10 batch 107 loss: 5.3973388671875\n",
      "epoch 10 batch 108 loss: 5.388986110687256\n",
      "epoch 10 batch 109 loss: 5.332016468048096\n",
      "epoch 10 batch 110 loss: 5.350368022918701\n",
      "epoch 10 batch 111 loss: 5.344223976135254\n",
      "epoch 10 batch 112 loss: 5.366121768951416\n",
      "epoch 10 batch 113 loss: 5.345656394958496\n",
      "epoch 10 batch 114 loss: 5.324000358581543\n",
      "epoch 10 batch 115 loss: 5.340547561645508\n",
      "epoch 10 batch 116 loss: 5.373522758483887\n",
      "epoch 10 batch 117 loss: 5.391841888427734\n",
      "epoch 10 batch 118 loss: 5.339738845825195\n",
      "epoch 10 batch 119 loss: 5.312947750091553\n",
      "epoch 10 batch 120 loss: 5.387063026428223\n",
      "epoch 10 batch 121 loss: 5.331449031829834\n",
      "epoch 10 batch 122 loss: 5.3970627784729\n",
      "epoch 10 batch 123 loss: 5.372490882873535\n",
      "epoch 10 batch 124 loss: 5.367326736450195\n",
      "epoch loss: 5.375817817687988\n",
      "epoch 11 batch 0 loss: 5.328001499176025\n",
      "epoch 11 batch 1 loss: 5.361805438995361\n",
      "epoch 11 batch 2 loss: 5.346471309661865\n",
      "epoch 11 batch 3 loss: 5.32518196105957\n",
      "epoch 11 batch 4 loss: 5.341184139251709\n",
      "epoch 11 batch 5 loss: 5.390419006347656\n",
      "epoch 11 batch 6 loss: 5.323248863220215\n",
      "epoch 11 batch 7 loss: 5.373619079589844\n",
      "epoch 11 batch 8 loss: 5.3700761795043945\n",
      "epoch 11 batch 9 loss: 5.407676696777344\n",
      "epoch 11 batch 10 loss: 5.349167346954346\n",
      "epoch 11 batch 11 loss: 5.367377758026123\n",
      "epoch 11 batch 12 loss: 5.33604621887207\n",
      "epoch 11 batch 13 loss: 5.384566307067871\n",
      "epoch 11 batch 14 loss: 5.399310111999512\n",
      "epoch 11 batch 15 loss: 5.356334209442139\n",
      "epoch 11 batch 16 loss: 5.384493350982666\n",
      "epoch 11 batch 17 loss: 5.325009822845459\n",
      "epoch 11 batch 18 loss: 5.367961406707764\n",
      "epoch 11 batch 19 loss: 5.360036373138428\n",
      "epoch 11 batch 20 loss: 5.342410564422607\n",
      "epoch 11 batch 21 loss: 5.316753387451172\n",
      "epoch 11 batch 22 loss: 5.3633294105529785\n",
      "epoch 11 batch 23 loss: 5.360156059265137\n",
      "epoch 11 batch 24 loss: 5.336153507232666\n",
      "epoch 11 batch 25 loss: 5.348484516143799\n",
      "epoch 11 batch 26 loss: 5.373011589050293\n",
      "epoch 11 batch 27 loss: 5.375220775604248\n",
      "epoch 11 batch 28 loss: 5.367776393890381\n",
      "epoch 11 batch 29 loss: 5.3556342124938965\n",
      "epoch 11 batch 30 loss: 5.346039772033691\n",
      "epoch 11 batch 31 loss: 5.380757808685303\n",
      "epoch 11 batch 32 loss: 5.370566368103027\n",
      "epoch 11 batch 33 loss: 5.365725994110107\n",
      "epoch 11 batch 34 loss: 5.33833122253418\n",
      "epoch 11 batch 35 loss: 5.357990741729736\n",
      "epoch 11 batch 36 loss: 5.399594783782959\n",
      "epoch 11 batch 37 loss: 5.322380065917969\n",
      "epoch 11 batch 38 loss: 5.344239234924316\n",
      "epoch 11 batch 39 loss: 5.316882133483887\n",
      "epoch 11 batch 40 loss: 5.391383647918701\n",
      "epoch 11 batch 41 loss: 5.31757926940918\n",
      "epoch 11 batch 42 loss: 5.353029251098633\n",
      "epoch 11 batch 43 loss: 5.342263698577881\n",
      "epoch 11 batch 44 loss: 5.379164695739746\n",
      "epoch 11 batch 45 loss: 5.343118190765381\n",
      "epoch 11 batch 46 loss: 5.383370399475098\n",
      "epoch 11 batch 47 loss: 5.341267108917236\n",
      "epoch 11 batch 48 loss: 5.3614630699157715\n",
      "epoch 11 batch 49 loss: 5.332596302032471\n",
      "epoch 11 batch 50 loss: 5.357359409332275\n",
      "epoch 11 batch 51 loss: 5.3626885414123535\n",
      "epoch 11 batch 52 loss: 5.361688613891602\n",
      "epoch 11 batch 53 loss: 5.315787315368652\n",
      "epoch 11 batch 54 loss: 5.348732948303223\n",
      "epoch 11 batch 55 loss: 5.405768871307373\n",
      "epoch 11 batch 56 loss: 5.336112022399902\n",
      "epoch 11 batch 57 loss: 5.366433620452881\n",
      "epoch 11 batch 58 loss: 5.410895824432373\n",
      "epoch 11 batch 59 loss: 5.3810715675354\n",
      "epoch 11 batch 60 loss: 5.359169960021973\n",
      "epoch 11 batch 61 loss: 5.350677490234375\n",
      "epoch 11 batch 62 loss: 5.355501651763916\n",
      "epoch 11 batch 63 loss: 5.397467136383057\n",
      "epoch 11 batch 64 loss: 5.333537578582764\n",
      "epoch 11 batch 65 loss: 5.404713153839111\n",
      "epoch 11 batch 66 loss: 5.373970985412598\n",
      "epoch 11 batch 67 loss: 5.370748043060303\n",
      "epoch 11 batch 68 loss: 5.34198522567749\n",
      "epoch 11 batch 69 loss: 5.39509916305542\n",
      "epoch 11 batch 70 loss: 5.346372127532959\n",
      "epoch 11 batch 71 loss: 5.372920513153076\n",
      "epoch 11 batch 72 loss: 5.340614318847656\n",
      "epoch 11 batch 73 loss: 5.35859489440918\n",
      "epoch 11 batch 74 loss: 5.372321605682373\n",
      "epoch 11 batch 75 loss: 5.372064113616943\n",
      "epoch 11 batch 76 loss: 5.388296604156494\n",
      "epoch 11 batch 77 loss: 5.332505226135254\n",
      "epoch 11 batch 78 loss: 5.321259021759033\n",
      "epoch 11 batch 79 loss: 5.364253044128418\n",
      "epoch 11 batch 80 loss: 5.3642144203186035\n",
      "epoch 11 batch 81 loss: 5.350719928741455\n",
      "epoch 11 batch 82 loss: 5.362016677856445\n",
      "epoch 11 batch 83 loss: 5.356945037841797\n",
      "epoch 11 batch 84 loss: 5.336726188659668\n",
      "epoch 11 batch 85 loss: 5.366230487823486\n",
      "epoch 11 batch 86 loss: 5.351759910583496\n",
      "epoch 11 batch 87 loss: 5.3394317626953125\n",
      "epoch 11 batch 88 loss: 5.34913444519043\n",
      "epoch 11 batch 89 loss: 5.381279945373535\n",
      "epoch 11 batch 90 loss: 5.364083290100098\n",
      "epoch 11 batch 91 loss: 5.396299839019775\n",
      "epoch 11 batch 92 loss: 5.3528265953063965\n",
      "epoch 11 batch 93 loss: 5.351618766784668\n",
      "epoch 11 batch 94 loss: 5.327617168426514\n",
      "epoch 11 batch 95 loss: 5.365287780761719\n",
      "epoch 11 batch 96 loss: 5.342819690704346\n",
      "epoch 11 batch 97 loss: 5.376479148864746\n",
      "epoch 11 batch 98 loss: 5.349371433258057\n",
      "epoch 11 batch 99 loss: 5.318450927734375\n",
      "epoch 11 batch 100 loss: 5.372607231140137\n",
      "epoch 11 batch 101 loss: 5.377899169921875\n",
      "epoch 11 batch 102 loss: 5.330831050872803\n",
      "epoch 11 batch 103 loss: 5.331252098083496\n",
      "epoch 11 batch 104 loss: 5.3622727394104\n",
      "epoch 11 batch 105 loss: 5.338623046875\n",
      "epoch 11 batch 106 loss: 5.342419147491455\n",
      "epoch 11 batch 107 loss: 5.339415550231934\n",
      "epoch 11 batch 108 loss: 5.367889404296875\n",
      "epoch 11 batch 109 loss: 5.346963882446289\n",
      "epoch 11 batch 110 loss: 5.357172966003418\n",
      "epoch 11 batch 111 loss: 5.348293781280518\n",
      "epoch 11 batch 112 loss: 5.352840900421143\n",
      "epoch 11 batch 113 loss: 5.364136695861816\n",
      "epoch 11 batch 114 loss: 5.380946636199951\n",
      "epoch 11 batch 115 loss: 5.382073402404785\n",
      "epoch 11 batch 116 loss: 5.349295139312744\n",
      "epoch 11 batch 117 loss: 5.372136116027832\n",
      "epoch 11 batch 118 loss: 5.336081504821777\n",
      "epoch 11 batch 119 loss: 5.357087135314941\n",
      "epoch 11 batch 120 loss: 5.3533935546875\n",
      "epoch 11 batch 121 loss: 5.363083362579346\n",
      "epoch 11 batch 122 loss: 5.375361919403076\n",
      "epoch 11 batch 123 loss: 5.37605094909668\n",
      "epoch 11 batch 124 loss: 5.365178108215332\n",
      "epoch loss: 5.3581591262817385\n",
      "epoch 12 batch 0 loss: 5.341619968414307\n",
      "epoch 12 batch 1 loss: 5.352249622344971\n",
      "epoch 12 batch 2 loss: 5.3334641456604\n",
      "epoch 12 batch 3 loss: 5.3658037185668945\n",
      "epoch 12 batch 4 loss: 5.313908100128174\n",
      "epoch 12 batch 5 loss: 5.37925910949707\n",
      "epoch 12 batch 6 loss: 5.314590930938721\n",
      "epoch 12 batch 7 loss: 5.328393936157227\n",
      "epoch 12 batch 8 loss: 5.350499153137207\n",
      "epoch 12 batch 9 loss: 5.35637092590332\n",
      "epoch 12 batch 10 loss: 5.376297473907471\n",
      "epoch 12 batch 11 loss: 5.373591423034668\n",
      "epoch 12 batch 12 loss: 5.320030212402344\n",
      "epoch 12 batch 13 loss: 5.338075637817383\n",
      "epoch 12 batch 14 loss: 5.3491692543029785\n",
      "epoch 12 batch 15 loss: 5.322547435760498\n",
      "epoch 12 batch 16 loss: 5.3548994064331055\n",
      "epoch 12 batch 17 loss: 5.358069896697998\n",
      "epoch 12 batch 18 loss: 5.34744930267334\n",
      "epoch 12 batch 19 loss: 5.315441131591797\n",
      "epoch 12 batch 20 loss: 5.327467918395996\n",
      "epoch 12 batch 21 loss: 5.339711666107178\n",
      "epoch 12 batch 22 loss: 5.298368453979492\n",
      "epoch 12 batch 23 loss: 5.331648826599121\n",
      "epoch 12 batch 24 loss: 5.341372966766357\n",
      "epoch 12 batch 25 loss: 5.333284378051758\n",
      "epoch 12 batch 26 loss: 5.352665901184082\n",
      "epoch 12 batch 27 loss: 5.37406587600708\n",
      "epoch 12 batch 28 loss: 5.32376766204834\n",
      "epoch 12 batch 29 loss: 5.34476375579834\n",
      "epoch 12 batch 30 loss: 5.349829196929932\n",
      "epoch 12 batch 31 loss: 5.331214904785156\n",
      "epoch 12 batch 32 loss: 5.346181392669678\n",
      "epoch 12 batch 33 loss: 5.341651439666748\n",
      "epoch 12 batch 34 loss: 5.316042423248291\n",
      "epoch 12 batch 35 loss: 5.344939231872559\n",
      "epoch 12 batch 36 loss: 5.341386795043945\n",
      "epoch 12 batch 37 loss: 5.30100679397583\n",
      "epoch 12 batch 38 loss: 5.365450382232666\n",
      "epoch 12 batch 39 loss: 5.379327297210693\n",
      "epoch 12 batch 40 loss: 5.361799240112305\n",
      "epoch 12 batch 41 loss: 5.299567222595215\n",
      "epoch 12 batch 42 loss: 5.323798179626465\n",
      "epoch 12 batch 43 loss: 5.325677394866943\n",
      "epoch 12 batch 44 loss: 5.316951751708984\n",
      "epoch 12 batch 45 loss: 5.333985328674316\n",
      "epoch 12 batch 46 loss: 5.36611795425415\n",
      "epoch 12 batch 47 loss: 5.356231689453125\n",
      "epoch 12 batch 48 loss: 5.361091136932373\n",
      "epoch 12 batch 49 loss: 5.339637756347656\n",
      "epoch 12 batch 50 loss: 5.402130126953125\n",
      "epoch 12 batch 51 loss: 5.384239196777344\n",
      "epoch 12 batch 52 loss: 5.309418678283691\n",
      "epoch 12 batch 53 loss: 5.373085975646973\n",
      "epoch 12 batch 54 loss: 5.3417649269104\n",
      "epoch 12 batch 55 loss: 5.338945388793945\n",
      "epoch 12 batch 56 loss: 5.352590084075928\n",
      "epoch 12 batch 57 loss: 5.323236465454102\n",
      "epoch 12 batch 58 loss: 5.339820384979248\n",
      "epoch 12 batch 59 loss: 5.3158793449401855\n",
      "epoch 12 batch 60 loss: 5.361155986785889\n",
      "epoch 12 batch 61 loss: 5.3545122146606445\n",
      "epoch 12 batch 62 loss: 5.3259429931640625\n",
      "epoch 12 batch 63 loss: 5.32102108001709\n",
      "epoch 12 batch 64 loss: 5.329817771911621\n",
      "epoch 12 batch 65 loss: 5.340712547302246\n",
      "epoch 12 batch 66 loss: 5.344752311706543\n",
      "epoch 12 batch 67 loss: 5.311030387878418\n",
      "epoch 12 batch 68 loss: 5.329169273376465\n",
      "epoch 12 batch 69 loss: 5.347530364990234\n",
      "epoch 12 batch 70 loss: 5.355403423309326\n",
      "epoch 12 batch 71 loss: 5.33636474609375\n",
      "epoch 12 batch 72 loss: 5.317234992980957\n",
      "epoch 12 batch 73 loss: 5.309970378875732\n",
      "epoch 12 batch 74 loss: 5.329136848449707\n",
      "epoch 12 batch 75 loss: 5.337014198303223\n",
      "epoch 12 batch 76 loss: 5.3383283615112305\n",
      "epoch 12 batch 77 loss: 5.334219932556152\n",
      "epoch 12 batch 78 loss: 5.318612098693848\n",
      "epoch 12 batch 79 loss: 5.353045463562012\n",
      "epoch 12 batch 80 loss: 5.356202125549316\n",
      "epoch 12 batch 81 loss: 5.320582389831543\n",
      "epoch 12 batch 82 loss: 5.297214031219482\n",
      "epoch 12 batch 83 loss: 5.3724541664123535\n",
      "epoch 12 batch 84 loss: 5.334026336669922\n",
      "epoch 12 batch 85 loss: 5.320718288421631\n",
      "epoch 12 batch 86 loss: 5.314475059509277\n",
      "epoch 12 batch 87 loss: 5.324502944946289\n",
      "epoch 12 batch 88 loss: 5.313080787658691\n",
      "epoch 12 batch 89 loss: 5.353062152862549\n",
      "epoch 12 batch 90 loss: 5.353020668029785\n",
      "epoch 12 batch 91 loss: 5.375981330871582\n",
      "epoch 12 batch 92 loss: 5.333826065063477\n",
      "epoch 12 batch 93 loss: 5.356378078460693\n",
      "epoch 12 batch 94 loss: 5.318591117858887\n",
      "epoch 12 batch 95 loss: 5.336188316345215\n",
      "epoch 12 batch 96 loss: 5.325200080871582\n",
      "epoch 12 batch 97 loss: 5.33066987991333\n",
      "epoch 12 batch 98 loss: 5.308413505554199\n",
      "epoch 12 batch 99 loss: 5.309466361999512\n",
      "epoch 12 batch 100 loss: 5.350573539733887\n",
      "epoch 12 batch 101 loss: 5.337066650390625\n",
      "epoch 12 batch 102 loss: 5.374380588531494\n",
      "epoch 12 batch 103 loss: 5.327714920043945\n",
      "epoch 12 batch 104 loss: 5.314985275268555\n",
      "epoch 12 batch 105 loss: 5.3743462562561035\n",
      "epoch 12 batch 106 loss: 5.298303127288818\n",
      "epoch 12 batch 107 loss: 5.307565212249756\n",
      "epoch 12 batch 108 loss: 5.333715915679932\n",
      "epoch 12 batch 109 loss: 5.3631591796875\n",
      "epoch 12 batch 110 loss: 5.323541164398193\n",
      "epoch 12 batch 111 loss: 5.295467853546143\n",
      "epoch 12 batch 112 loss: 5.347765922546387\n",
      "epoch 12 batch 113 loss: 5.324696063995361\n",
      "epoch 12 batch 114 loss: 5.3546953201293945\n",
      "epoch 12 batch 115 loss: 5.333795547485352\n",
      "epoch 12 batch 116 loss: 5.36848258972168\n",
      "epoch 12 batch 117 loss: 5.332054138183594\n",
      "epoch 12 batch 118 loss: 5.311388969421387\n",
      "epoch 12 batch 119 loss: 5.314727783203125\n",
      "epoch 12 batch 120 loss: 5.32937479019165\n",
      "epoch 12 batch 121 loss: 5.355225563049316\n",
      "epoch 12 batch 122 loss: 5.347137451171875\n",
      "epoch 12 batch 123 loss: 5.325678825378418\n",
      "epoch 12 batch 124 loss: 5.320668697357178\n",
      "epoch loss: 5.338059093475342\n",
      "epoch 13 batch 0 loss: 5.353137016296387\n",
      "epoch 13 batch 1 loss: 5.329344749450684\n",
      "epoch 13 batch 2 loss: 5.352261066436768\n",
      "epoch 13 batch 3 loss: 5.304443836212158\n",
      "epoch 13 batch 4 loss: 5.328395366668701\n",
      "epoch 13 batch 5 loss: 5.359246730804443\n",
      "epoch 13 batch 6 loss: 5.352387428283691\n",
      "epoch 13 batch 7 loss: 5.361058235168457\n",
      "epoch 13 batch 8 loss: 5.316055774688721\n",
      "epoch 13 batch 9 loss: 5.362449645996094\n",
      "epoch 13 batch 10 loss: 5.322000503540039\n",
      "epoch 13 batch 11 loss: 5.368138313293457\n",
      "epoch 13 batch 12 loss: 5.353049278259277\n",
      "epoch 13 batch 13 loss: 5.312090873718262\n",
      "epoch 13 batch 14 loss: 5.314949989318848\n",
      "epoch 13 batch 15 loss: 5.3872857093811035\n",
      "epoch 13 batch 16 loss: 5.359455585479736\n",
      "epoch 13 batch 17 loss: 5.314672946929932\n",
      "epoch 13 batch 18 loss: 5.2930402755737305\n",
      "epoch 13 batch 19 loss: 5.3218278884887695\n",
      "epoch 13 batch 20 loss: 5.352076530456543\n",
      "epoch 13 batch 21 loss: 5.327511310577393\n",
      "epoch 13 batch 22 loss: 5.319209098815918\n",
      "epoch 13 batch 23 loss: 5.337566375732422\n",
      "epoch 13 batch 24 loss: 5.289645671844482\n",
      "epoch 13 batch 25 loss: 5.350972652435303\n",
      "epoch 13 batch 26 loss: 5.334561824798584\n",
      "epoch 13 batch 27 loss: 5.337000846862793\n",
      "epoch 13 batch 28 loss: 5.328859806060791\n",
      "epoch 13 batch 29 loss: 5.358094215393066\n",
      "epoch 13 batch 30 loss: 5.30902099609375\n",
      "epoch 13 batch 31 loss: 5.336374282836914\n",
      "epoch 13 batch 32 loss: 5.319593906402588\n",
      "epoch 13 batch 33 loss: 5.275806903839111\n",
      "epoch 13 batch 34 loss: 5.342789649963379\n",
      "epoch 13 batch 35 loss: 5.291096210479736\n",
      "epoch 13 batch 36 loss: 5.357278347015381\n",
      "epoch 13 batch 37 loss: 5.333376407623291\n",
      "epoch 13 batch 38 loss: 5.3516106605529785\n",
      "epoch 13 batch 39 loss: 5.31678581237793\n",
      "epoch 13 batch 40 loss: 5.305407524108887\n",
      "epoch 13 batch 41 loss: 5.32066535949707\n",
      "epoch 13 batch 42 loss: 5.357439041137695\n",
      "epoch 13 batch 43 loss: 5.3311767578125\n",
      "epoch 13 batch 44 loss: 5.320091247558594\n",
      "epoch 13 batch 45 loss: 5.317546844482422\n",
      "epoch 13 batch 46 loss: 5.333716869354248\n",
      "epoch 13 batch 47 loss: 5.3285722732543945\n",
      "epoch 13 batch 48 loss: 5.317503452301025\n",
      "epoch 13 batch 49 loss: 5.3793511390686035\n",
      "epoch 13 batch 50 loss: 5.345673084259033\n",
      "epoch 13 batch 51 loss: 5.320372581481934\n",
      "epoch 13 batch 52 loss: 5.310353755950928\n",
      "epoch 13 batch 53 loss: 5.358378887176514\n",
      "epoch 13 batch 54 loss: 5.304218769073486\n",
      "epoch 13 batch 55 loss: 5.333592414855957\n",
      "epoch 13 batch 56 loss: 5.357383728027344\n",
      "epoch 13 batch 57 loss: 5.347105979919434\n",
      "epoch 13 batch 58 loss: 5.351319313049316\n",
      "epoch 13 batch 59 loss: 5.371101379394531\n",
      "epoch 13 batch 60 loss: 5.309311389923096\n",
      "epoch 13 batch 61 loss: 5.326292514801025\n",
      "epoch 13 batch 62 loss: 5.325578689575195\n",
      "epoch 13 batch 63 loss: 5.326296329498291\n",
      "epoch 13 batch 64 loss: 5.31820011138916\n",
      "epoch 13 batch 65 loss: 5.331799030303955\n",
      "epoch 13 batch 66 loss: 5.296499729156494\n",
      "epoch 13 batch 67 loss: 5.336430549621582\n",
      "epoch 13 batch 68 loss: 5.318980693817139\n",
      "epoch 13 batch 69 loss: 5.337320327758789\n",
      "epoch 13 batch 70 loss: 5.29838228225708\n",
      "epoch 13 batch 71 loss: 5.316276550292969\n",
      "epoch 13 batch 72 loss: 5.341997146606445\n",
      "epoch 13 batch 73 loss: 5.347773551940918\n",
      "epoch 13 batch 74 loss: 5.301075458526611\n",
      "epoch 13 batch 75 loss: 5.35806131362915\n",
      "epoch 13 batch 76 loss: 5.331732273101807\n",
      "epoch 13 batch 77 loss: 5.33697509765625\n",
      "epoch 13 batch 78 loss: 5.319141864776611\n",
      "epoch 13 batch 79 loss: 5.372099876403809\n",
      "epoch 13 batch 80 loss: 5.323143005371094\n",
      "epoch 13 batch 81 loss: 5.316356658935547\n",
      "epoch 13 batch 82 loss: 5.356568336486816\n",
      "epoch 13 batch 83 loss: 5.305817127227783\n",
      "epoch 13 batch 84 loss: 5.3067851066589355\n",
      "epoch 13 batch 85 loss: 5.34090518951416\n",
      "epoch 13 batch 86 loss: 5.33662748336792\n",
      "epoch 13 batch 87 loss: 5.346778392791748\n",
      "epoch 13 batch 88 loss: 5.323876857757568\n",
      "epoch 13 batch 89 loss: 5.325972080230713\n",
      "epoch 13 batch 90 loss: 5.340826988220215\n",
      "epoch 13 batch 91 loss: 5.316181659698486\n",
      "epoch 13 batch 92 loss: 5.340471267700195\n",
      "epoch 13 batch 93 loss: 5.307284832000732\n",
      "epoch 13 batch 94 loss: 5.323952674865723\n",
      "epoch 13 batch 95 loss: 5.299461364746094\n",
      "epoch 13 batch 96 loss: 5.341833591461182\n",
      "epoch 13 batch 97 loss: 5.333150386810303\n",
      "epoch 13 batch 98 loss: 5.325601100921631\n",
      "epoch 13 batch 99 loss: 5.341319561004639\n",
      "epoch 13 batch 100 loss: 5.340801239013672\n",
      "epoch 13 batch 101 loss: 5.340016841888428\n",
      "epoch 13 batch 102 loss: 5.351423263549805\n",
      "epoch 13 batch 103 loss: 5.300177574157715\n",
      "epoch 13 batch 104 loss: 5.338578224182129\n",
      "epoch 13 batch 105 loss: 5.302829265594482\n",
      "epoch 13 batch 106 loss: 5.311087608337402\n",
      "epoch 13 batch 107 loss: 5.3209228515625\n",
      "epoch 13 batch 108 loss: 5.294212341308594\n",
      "epoch 13 batch 109 loss: 5.307938098907471\n",
      "epoch 13 batch 110 loss: 5.314243793487549\n",
      "epoch 13 batch 111 loss: 5.356593132019043\n",
      "epoch 13 batch 112 loss: 5.328271865844727\n",
      "epoch 13 batch 113 loss: 5.307669639587402\n",
      "epoch 13 batch 114 loss: 5.32086706161499\n",
      "epoch 13 batch 115 loss: 5.301534175872803\n",
      "epoch 13 batch 116 loss: 5.312762260437012\n",
      "epoch 13 batch 117 loss: 5.3765869140625\n",
      "epoch 13 batch 118 loss: 5.3037567138671875\n",
      "epoch 13 batch 119 loss: 5.268867492675781\n",
      "epoch 13 batch 120 loss: 5.306977272033691\n",
      "epoch 13 batch 121 loss: 5.311914443969727\n",
      "epoch 13 batch 122 loss: 5.320687294006348\n",
      "epoch 13 batch 123 loss: 5.289243221282959\n",
      "epoch 13 batch 124 loss: 5.303759574890137\n",
      "epoch loss: 5.32864282989502\n",
      "epoch 14 batch 0 loss: 5.344698905944824\n",
      "epoch 14 batch 1 loss: 5.3388671875\n",
      "epoch 14 batch 2 loss: 5.30980110168457\n",
      "epoch 14 batch 3 loss: 5.318514347076416\n",
      "epoch 14 batch 4 loss: 5.301245212554932\n",
      "epoch 14 batch 5 loss: 5.351207733154297\n",
      "epoch 14 batch 6 loss: 5.339396953582764\n",
      "epoch 14 batch 7 loss: 5.328099250793457\n",
      "epoch 14 batch 8 loss: 5.300373554229736\n",
      "epoch 14 batch 9 loss: 5.333008766174316\n",
      "epoch 14 batch 10 loss: 5.276867389678955\n",
      "epoch 14 batch 11 loss: 5.280470848083496\n",
      "epoch 14 batch 12 loss: 5.303465366363525\n",
      "epoch 14 batch 13 loss: 5.287881374359131\n",
      "epoch 14 batch 14 loss: 5.336957931518555\n",
      "epoch 14 batch 15 loss: 5.316075325012207\n",
      "epoch 14 batch 16 loss: 5.343552112579346\n",
      "epoch 14 batch 17 loss: 5.348168849945068\n",
      "epoch 14 batch 18 loss: 5.313405513763428\n",
      "epoch 14 batch 19 loss: 5.325451374053955\n",
      "epoch 14 batch 20 loss: 5.361783981323242\n",
      "epoch 14 batch 21 loss: 5.3486008644104\n",
      "epoch 14 batch 22 loss: 5.334124565124512\n",
      "epoch 14 batch 23 loss: 5.299017906188965\n",
      "epoch 14 batch 24 loss: 5.338881015777588\n",
      "epoch 14 batch 25 loss: 5.325679302215576\n",
      "epoch 14 batch 26 loss: 5.348538875579834\n",
      "epoch 14 batch 27 loss: 5.349289417266846\n",
      "epoch 14 batch 28 loss: 5.3298821449279785\n",
      "epoch 14 batch 29 loss: 5.309456825256348\n",
      "epoch 14 batch 30 loss: 5.31596565246582\n",
      "epoch 14 batch 31 loss: 5.31795072555542\n",
      "epoch 14 batch 32 loss: 5.316158294677734\n",
      "epoch 14 batch 33 loss: 5.284546852111816\n",
      "epoch 14 batch 34 loss: 5.3532395362854\n",
      "epoch 14 batch 35 loss: 5.282894134521484\n",
      "epoch 14 batch 36 loss: 5.341729640960693\n",
      "epoch 14 batch 37 loss: 5.311232566833496\n",
      "epoch 14 batch 38 loss: 5.363205432891846\n",
      "epoch 14 batch 39 loss: 5.315788269042969\n",
      "epoch 14 batch 40 loss: 5.311944484710693\n",
      "epoch 14 batch 41 loss: 5.318363666534424\n",
      "epoch 14 batch 42 loss: 5.299934387207031\n",
      "epoch 14 batch 43 loss: 5.352755069732666\n",
      "epoch 14 batch 44 loss: 5.306879997253418\n",
      "epoch 14 batch 45 loss: 5.327262878417969\n",
      "epoch 14 batch 46 loss: 5.3429460525512695\n",
      "epoch 14 batch 47 loss: 5.280402660369873\n",
      "epoch 14 batch 48 loss: 5.316688060760498\n",
      "epoch 14 batch 49 loss: 5.281741619110107\n",
      "epoch 14 batch 50 loss: 5.308773517608643\n",
      "epoch 14 batch 51 loss: 5.332676887512207\n",
      "epoch 14 batch 52 loss: 5.282944202423096\n",
      "epoch 14 batch 53 loss: 5.292971134185791\n",
      "epoch 14 batch 54 loss: 5.34113883972168\n",
      "epoch 14 batch 55 loss: 5.334288597106934\n",
      "epoch 14 batch 56 loss: 5.299060344696045\n",
      "epoch 14 batch 57 loss: 5.330544471740723\n",
      "epoch 14 batch 58 loss: 5.309352874755859\n",
      "epoch 14 batch 59 loss: 5.293837070465088\n",
      "epoch 14 batch 60 loss: 5.294899940490723\n",
      "epoch 14 batch 61 loss: 5.3153510093688965\n",
      "epoch 14 batch 62 loss: 5.282905578613281\n",
      "epoch 14 batch 63 loss: 5.3164777755737305\n",
      "epoch 14 batch 64 loss: 5.300854206085205\n",
      "epoch 14 batch 65 loss: 5.295713424682617\n",
      "epoch 14 batch 66 loss: 5.308341979980469\n",
      "epoch 14 batch 67 loss: 5.349301815032959\n",
      "epoch 14 batch 68 loss: 5.316826820373535\n",
      "epoch 14 batch 69 loss: 5.289173603057861\n",
      "epoch 14 batch 70 loss: 5.314056396484375\n",
      "epoch 14 batch 71 loss: 5.334865570068359\n",
      "epoch 14 batch 72 loss: 5.333099365234375\n",
      "epoch 14 batch 73 loss: 5.321555137634277\n",
      "epoch 14 batch 74 loss: 5.331473350524902\n",
      "epoch 14 batch 75 loss: 5.333248138427734\n",
      "epoch 14 batch 76 loss: 5.349368572235107\n",
      "epoch 14 batch 77 loss: 5.304234504699707\n",
      "epoch 14 batch 78 loss: 5.293350696563721\n",
      "epoch 14 batch 79 loss: 5.286288738250732\n",
      "epoch 14 batch 80 loss: 5.3150482177734375\n",
      "epoch 14 batch 81 loss: 5.301004409790039\n",
      "epoch 14 batch 82 loss: 5.301513671875\n",
      "epoch 14 batch 83 loss: 5.337619304656982\n",
      "epoch 14 batch 84 loss: 5.309719562530518\n",
      "epoch 14 batch 85 loss: 5.328160762786865\n",
      "epoch 14 batch 86 loss: 5.345302104949951\n",
      "epoch 14 batch 87 loss: 5.294504165649414\n",
      "epoch 14 batch 88 loss: 5.298800945281982\n",
      "epoch 14 batch 89 loss: 5.318521499633789\n",
      "epoch 14 batch 90 loss: 5.356478214263916\n",
      "epoch 14 batch 91 loss: 5.287560939788818\n",
      "epoch 14 batch 92 loss: 5.3540520668029785\n",
      "epoch 14 batch 93 loss: 5.298571586608887\n",
      "epoch 14 batch 94 loss: 5.325446128845215\n",
      "epoch 14 batch 95 loss: 5.252205848693848\n",
      "epoch 14 batch 96 loss: 5.275763988494873\n",
      "epoch 14 batch 97 loss: 5.2780303955078125\n",
      "epoch 14 batch 98 loss: 5.357494831085205\n",
      "epoch 14 batch 99 loss: 5.292906284332275\n",
      "epoch 14 batch 100 loss: 5.3085503578186035\n",
      "epoch 14 batch 101 loss: 5.310375213623047\n",
      "epoch 14 batch 102 loss: 5.31306266784668\n",
      "epoch 14 batch 103 loss: 5.348565101623535\n",
      "epoch 14 batch 104 loss: 5.352447986602783\n",
      "epoch 14 batch 105 loss: 5.324682712554932\n",
      "epoch 14 batch 106 loss: 5.337535381317139\n",
      "epoch 14 batch 107 loss: 5.30377721786499\n",
      "epoch 14 batch 108 loss: 5.293661594390869\n",
      "epoch 14 batch 109 loss: 5.319853782653809\n",
      "epoch 14 batch 110 loss: 5.308992862701416\n",
      "epoch 14 batch 111 loss: 5.337719440460205\n",
      "epoch 14 batch 112 loss: 5.321780681610107\n",
      "epoch 14 batch 113 loss: 5.297900199890137\n",
      "epoch 14 batch 114 loss: 5.319966793060303\n",
      "epoch 14 batch 115 loss: 5.299839973449707\n",
      "epoch 14 batch 116 loss: 5.313961505889893\n",
      "epoch 14 batch 117 loss: 5.281260967254639\n",
      "epoch 14 batch 118 loss: 5.315380573272705\n",
      "epoch 14 batch 119 loss: 5.296046257019043\n",
      "epoch 14 batch 120 loss: 5.354104042053223\n",
      "epoch 14 batch 121 loss: 5.307698726654053\n",
      "epoch 14 batch 122 loss: 5.324909687042236\n",
      "epoch 14 batch 123 loss: 5.2938971519470215\n",
      "epoch 14 batch 124 loss: 5.289018630981445\n",
      "epoch loss: 5.316680503845215\n",
      "epoch 15 batch 0 loss: 5.317590713500977\n",
      "epoch 15 batch 1 loss: 5.300190448760986\n",
      "epoch 15 batch 2 loss: 5.284332275390625\n",
      "epoch 15 batch 3 loss: 5.284029006958008\n",
      "epoch 15 batch 4 loss: 5.285405158996582\n",
      "epoch 15 batch 5 loss: 5.307266712188721\n",
      "epoch 15 batch 6 loss: 5.289335250854492\n",
      "epoch 15 batch 7 loss: 5.321137428283691\n",
      "epoch 15 batch 8 loss: 5.343477249145508\n",
      "epoch 15 batch 9 loss: 5.297812461853027\n",
      "epoch 15 batch 10 loss: 5.302619457244873\n",
      "epoch 15 batch 11 loss: 5.281331539154053\n",
      "epoch 15 batch 12 loss: 5.30325984954834\n",
      "epoch 15 batch 13 loss: 5.292284965515137\n",
      "epoch 15 batch 14 loss: 5.3145623207092285\n",
      "epoch 15 batch 15 loss: 5.3061676025390625\n",
      "epoch 15 batch 16 loss: 5.293381214141846\n",
      "epoch 15 batch 17 loss: 5.292452335357666\n",
      "epoch 15 batch 18 loss: 5.310217380523682\n",
      "epoch 15 batch 19 loss: 5.335381507873535\n",
      "epoch 15 batch 20 loss: 5.295048713684082\n",
      "epoch 15 batch 21 loss: 5.275818824768066\n",
      "epoch 15 batch 22 loss: 5.3195977210998535\n",
      "epoch 15 batch 23 loss: 5.280925273895264\n",
      "epoch 15 batch 24 loss: 5.307407855987549\n",
      "epoch 15 batch 25 loss: 5.327209949493408\n",
      "epoch 15 batch 26 loss: 5.350831985473633\n",
      "epoch 15 batch 27 loss: 5.306657791137695\n",
      "epoch 15 batch 28 loss: 5.349707126617432\n",
      "epoch 15 batch 29 loss: 5.313673973083496\n",
      "epoch 15 batch 30 loss: 5.314881324768066\n",
      "epoch 15 batch 31 loss: 5.32087516784668\n",
      "epoch 15 batch 32 loss: 5.296607494354248\n",
      "epoch 15 batch 33 loss: 5.329097747802734\n",
      "epoch 15 batch 34 loss: 5.315987586975098\n",
      "epoch 15 batch 35 loss: 5.323115348815918\n",
      "epoch 15 batch 36 loss: 5.331696033477783\n",
      "epoch 15 batch 37 loss: 5.300110816955566\n",
      "epoch 15 batch 38 loss: 5.2841796875\n",
      "epoch 15 batch 39 loss: 5.344103813171387\n",
      "epoch 15 batch 40 loss: 5.330348014831543\n",
      "epoch 15 batch 41 loss: 5.296430587768555\n",
      "epoch 15 batch 42 loss: 5.313342094421387\n",
      "epoch 15 batch 43 loss: 5.298970699310303\n",
      "epoch 15 batch 44 loss: 5.295029163360596\n",
      "epoch 15 batch 45 loss: 5.317525386810303\n",
      "epoch 15 batch 46 loss: 5.272371768951416\n",
      "epoch 15 batch 47 loss: 5.305525779724121\n",
      "epoch 15 batch 48 loss: 5.321677207946777\n",
      "epoch 15 batch 49 loss: 5.35383939743042\n",
      "epoch 15 batch 50 loss: 5.285450458526611\n",
      "epoch 15 batch 51 loss: 5.279237747192383\n",
      "epoch 15 batch 52 loss: 5.323639869689941\n",
      "epoch 15 batch 53 loss: 5.3066582679748535\n",
      "epoch 15 batch 54 loss: 5.277453422546387\n",
      "epoch 15 batch 55 loss: 5.298158645629883\n",
      "epoch 15 batch 56 loss: 5.297755241394043\n",
      "epoch 15 batch 57 loss: 5.318835735321045\n",
      "epoch 15 batch 58 loss: 5.298584938049316\n",
      "epoch 15 batch 59 loss: 5.306706428527832\n",
      "epoch 15 batch 60 loss: 5.277810096740723\n",
      "epoch 15 batch 61 loss: 5.312081813812256\n",
      "epoch 15 batch 62 loss: 5.302944660186768\n",
      "epoch 15 batch 63 loss: 5.31865119934082\n",
      "epoch 15 batch 64 loss: 5.349459171295166\n",
      "epoch 15 batch 65 loss: 5.340761661529541\n",
      "epoch 15 batch 66 loss: 5.301491737365723\n",
      "epoch 15 batch 67 loss: 5.276694297790527\n",
      "epoch 15 batch 68 loss: 5.353453159332275\n",
      "epoch 15 batch 69 loss: 5.342864990234375\n",
      "epoch 15 batch 70 loss: 5.308415412902832\n",
      "epoch 15 batch 71 loss: 5.336384296417236\n",
      "epoch 15 batch 72 loss: 5.339842796325684\n",
      "epoch 15 batch 73 loss: 5.33427619934082\n",
      "epoch 15 batch 74 loss: 5.301827430725098\n",
      "epoch 15 batch 75 loss: 5.305503845214844\n",
      "epoch 15 batch 76 loss: 5.333394527435303\n",
      "epoch 15 batch 77 loss: 5.289099216461182\n",
      "epoch 15 batch 78 loss: 5.2784037590026855\n",
      "epoch 15 batch 79 loss: 5.316560745239258\n",
      "epoch 15 batch 80 loss: 5.341146945953369\n",
      "epoch 15 batch 81 loss: 5.336857795715332\n",
      "epoch 15 batch 82 loss: 5.310874938964844\n",
      "epoch 15 batch 83 loss: 5.302109241485596\n",
      "epoch 15 batch 84 loss: 5.306337833404541\n",
      "epoch 15 batch 85 loss: 5.3222150802612305\n",
      "epoch 15 batch 86 loss: 5.327352046966553\n",
      "epoch 15 batch 87 loss: 5.293838977813721\n",
      "epoch 15 batch 88 loss: 5.297664165496826\n",
      "epoch 15 batch 89 loss: 5.286538124084473\n",
      "epoch 15 batch 90 loss: 5.267397880554199\n",
      "epoch 15 batch 91 loss: 5.348387241363525\n",
      "epoch 15 batch 92 loss: 5.307267189025879\n",
      "epoch 15 batch 93 loss: 5.30931282043457\n",
      "epoch 15 batch 94 loss: 5.299205780029297\n",
      "epoch 15 batch 95 loss: 5.286556243896484\n",
      "epoch 15 batch 96 loss: 5.298597812652588\n",
      "epoch 15 batch 97 loss: 5.334946155548096\n",
      "epoch 15 batch 98 loss: 5.338743686676025\n",
      "epoch 15 batch 99 loss: 5.295187950134277\n",
      "epoch 15 batch 100 loss: 5.322556018829346\n",
      "epoch 15 batch 101 loss: 5.289029598236084\n",
      "epoch 15 batch 102 loss: 5.308866024017334\n",
      "epoch 15 batch 103 loss: 5.278584003448486\n",
      "epoch 15 batch 104 loss: 5.293905735015869\n",
      "epoch 15 batch 105 loss: 5.30145788192749\n",
      "epoch 15 batch 106 loss: 5.328199863433838\n",
      "epoch 15 batch 107 loss: 5.301853656768799\n",
      "epoch 15 batch 108 loss: 5.295188903808594\n",
      "epoch 15 batch 109 loss: 5.2657952308654785\n",
      "epoch 15 batch 110 loss: 5.300903797149658\n",
      "epoch 15 batch 111 loss: 5.283005714416504\n",
      "epoch 15 batch 112 loss: 5.308047771453857\n",
      "epoch 15 batch 113 loss: 5.302457809448242\n",
      "epoch 15 batch 114 loss: 5.295198440551758\n",
      "epoch 15 batch 115 loss: 5.321946144104004\n",
      "epoch 15 batch 116 loss: 5.288517951965332\n",
      "epoch 15 batch 117 loss: 5.346240043640137\n",
      "epoch 15 batch 118 loss: 5.281610012054443\n",
      "epoch 15 batch 119 loss: 5.2972612380981445\n",
      "epoch 15 batch 120 loss: 5.290870189666748\n",
      "epoch 15 batch 121 loss: 5.290689468383789\n",
      "epoch 15 batch 122 loss: 5.293664455413818\n",
      "epoch 15 batch 123 loss: 5.333719253540039\n",
      "epoch 15 batch 124 loss: 5.3185954093933105\n",
      "epoch loss: 5.308127468109131\n",
      "epoch 16 batch 0 loss: 5.299283504486084\n",
      "epoch 16 batch 1 loss: 5.285254001617432\n",
      "epoch 16 batch 2 loss: 5.304771423339844\n",
      "epoch 16 batch 3 loss: 5.326413631439209\n",
      "epoch 16 batch 4 loss: 5.305020809173584\n",
      "epoch 16 batch 5 loss: 5.249440670013428\n",
      "epoch 16 batch 6 loss: 5.315031051635742\n",
      "epoch 16 batch 7 loss: 5.313197135925293\n",
      "epoch 16 batch 8 loss: 5.291569709777832\n",
      "epoch 16 batch 9 loss: 5.294848918914795\n",
      "epoch 16 batch 10 loss: 5.351711273193359\n",
      "epoch 16 batch 11 loss: 5.340787410736084\n",
      "epoch 16 batch 12 loss: 5.297743797302246\n",
      "epoch 16 batch 13 loss: 5.298080921173096\n",
      "epoch 16 batch 14 loss: 5.301503658294678\n",
      "epoch 16 batch 15 loss: 5.334639549255371\n",
      "epoch 16 batch 16 loss: 5.261552810668945\n",
      "epoch 16 batch 17 loss: 5.250833511352539\n",
      "epoch 16 batch 18 loss: 5.292447566986084\n",
      "epoch 16 batch 19 loss: 5.261663913726807\n",
      "epoch 16 batch 20 loss: 5.304157733917236\n",
      "epoch 16 batch 21 loss: 5.3177170753479\n",
      "epoch 16 batch 22 loss: 5.320208549499512\n",
      "epoch 16 batch 23 loss: 5.329346656799316\n",
      "epoch 16 batch 24 loss: 5.27863883972168\n",
      "epoch 16 batch 25 loss: 5.3003644943237305\n",
      "epoch 16 batch 26 loss: 5.295765399932861\n",
      "epoch 16 batch 27 loss: 5.320709705352783\n",
      "epoch 16 batch 28 loss: 5.276866436004639\n",
      "epoch 16 batch 29 loss: 5.308591365814209\n",
      "epoch 16 batch 30 loss: 5.308743953704834\n",
      "epoch 16 batch 31 loss: 5.301422119140625\n",
      "epoch 16 batch 32 loss: 5.355207443237305\n",
      "epoch 16 batch 33 loss: 5.295291423797607\n",
      "epoch 16 batch 34 loss: 5.2599778175354\n",
      "epoch 16 batch 35 loss: 5.286369323730469\n",
      "epoch 16 batch 36 loss: 5.341644763946533\n",
      "epoch 16 batch 37 loss: 5.323847770690918\n",
      "epoch 16 batch 38 loss: 5.277787208557129\n",
      "epoch 16 batch 39 loss: 5.308897495269775\n",
      "epoch 16 batch 40 loss: 5.307033538818359\n",
      "epoch 16 batch 41 loss: 5.26408052444458\n",
      "epoch 16 batch 42 loss: 5.315864562988281\n",
      "epoch 16 batch 43 loss: 5.305700778961182\n",
      "epoch 16 batch 44 loss: 5.270632743835449\n",
      "epoch 16 batch 45 loss: 5.341226577758789\n",
      "epoch 16 batch 46 loss: 5.321304798126221\n",
      "epoch 16 batch 47 loss: 5.3189697265625\n",
      "epoch 16 batch 48 loss: 5.29905366897583\n",
      "epoch 16 batch 49 loss: 5.330381870269775\n",
      "epoch 16 batch 50 loss: 5.298096656799316\n",
      "epoch 16 batch 51 loss: 5.304142475128174\n",
      "epoch 16 batch 52 loss: 5.324473857879639\n",
      "epoch 16 batch 53 loss: 5.3080854415893555\n",
      "epoch 16 batch 54 loss: 5.322798728942871\n",
      "epoch 16 batch 55 loss: 5.272164344787598\n",
      "epoch 16 batch 56 loss: 5.30118465423584\n",
      "epoch 16 batch 57 loss: 5.295151710510254\n",
      "epoch 16 batch 58 loss: 5.280418395996094\n",
      "epoch 16 batch 59 loss: 5.260534763336182\n",
      "epoch 16 batch 60 loss: 5.288804054260254\n",
      "epoch 16 batch 61 loss: 5.29911994934082\n",
      "epoch 16 batch 62 loss: 5.310395240783691\n",
      "epoch 16 batch 63 loss: 5.277676105499268\n",
      "epoch 16 batch 64 loss: 5.3510422706604\n",
      "epoch 16 batch 65 loss: 5.305734634399414\n",
      "epoch 16 batch 66 loss: 5.3313188552856445\n",
      "epoch 16 batch 67 loss: 5.27085542678833\n",
      "epoch 16 batch 68 loss: 5.314373016357422\n",
      "epoch 16 batch 69 loss: 5.289327144622803\n",
      "epoch 16 batch 70 loss: 5.268808364868164\n",
      "epoch 16 batch 71 loss: 5.265213489532471\n",
      "epoch 16 batch 72 loss: 5.268957614898682\n",
      "epoch 16 batch 73 loss: 5.301514625549316\n",
      "epoch 16 batch 74 loss: 5.283225059509277\n",
      "epoch 16 batch 75 loss: 5.327779293060303\n",
      "epoch 16 batch 76 loss: 5.330297946929932\n",
      "epoch 16 batch 77 loss: 5.29593563079834\n",
      "epoch 16 batch 78 loss: 5.320961952209473\n",
      "epoch 16 batch 79 loss: 5.3115153312683105\n",
      "epoch 16 batch 80 loss: 5.294536113739014\n",
      "epoch 16 batch 81 loss: 5.309901714324951\n",
      "epoch 16 batch 82 loss: 5.297415733337402\n",
      "epoch 16 batch 83 loss: 5.315201759338379\n",
      "epoch 16 batch 84 loss: 5.312961101531982\n",
      "epoch 16 batch 85 loss: 5.2959771156311035\n",
      "epoch 16 batch 86 loss: 5.305532932281494\n",
      "epoch 16 batch 87 loss: 5.299328327178955\n",
      "epoch 16 batch 88 loss: 5.299779415130615\n",
      "epoch 16 batch 89 loss: 5.320938587188721\n",
      "epoch 16 batch 90 loss: 5.3260393142700195\n",
      "epoch 16 batch 91 loss: 5.351496696472168\n",
      "epoch 16 batch 92 loss: 5.31226921081543\n",
      "epoch 16 batch 93 loss: 5.282013893127441\n",
      "epoch 16 batch 94 loss: 5.326171875\n",
      "epoch 16 batch 95 loss: 5.310663223266602\n",
      "epoch 16 batch 96 loss: 5.320833206176758\n",
      "epoch 16 batch 97 loss: 5.30607271194458\n",
      "epoch 16 batch 98 loss: 5.294538497924805\n",
      "epoch 16 batch 99 loss: 5.286333084106445\n",
      "epoch 16 batch 100 loss: 5.2866950035095215\n",
      "epoch 16 batch 101 loss: 5.281897068023682\n",
      "epoch 16 batch 102 loss: 5.242041110992432\n",
      "epoch 16 batch 103 loss: 5.271111965179443\n",
      "epoch 16 batch 104 loss: 5.304678916931152\n",
      "epoch 16 batch 105 loss: 5.329801559448242\n",
      "epoch 16 batch 106 loss: 5.302335262298584\n",
      "epoch 16 batch 107 loss: 5.278953552246094\n",
      "epoch 16 batch 108 loss: 5.2963547706604\n",
      "epoch 16 batch 109 loss: 5.298539161682129\n",
      "epoch 16 batch 110 loss: 5.287161827087402\n",
      "epoch 16 batch 111 loss: 5.285405158996582\n",
      "epoch 16 batch 112 loss: 5.295743942260742\n",
      "epoch 16 batch 113 loss: 5.2935380935668945\n",
      "epoch 16 batch 114 loss: 5.312695026397705\n",
      "epoch 16 batch 115 loss: 5.325071811676025\n",
      "epoch 16 batch 116 loss: 5.29122257232666\n",
      "epoch 16 batch 117 loss: 5.291428089141846\n",
      "epoch 16 batch 118 loss: 5.291603088378906\n",
      "epoch 16 batch 119 loss: 5.23148775100708\n",
      "epoch 16 batch 120 loss: 5.29492712020874\n",
      "epoch 16 batch 121 loss: 5.31755256652832\n",
      "epoch 16 batch 122 loss: 5.283443450927734\n",
      "epoch 16 batch 123 loss: 5.326113224029541\n",
      "epoch 16 batch 124 loss: 5.282440185546875\n",
      "epoch loss: 5.300894027709961\n",
      "epoch 17 batch 0 loss: 5.293845176696777\n",
      "epoch 17 batch 1 loss: 5.288985252380371\n",
      "epoch 17 batch 2 loss: 5.279526710510254\n",
      "epoch 17 batch 3 loss: 5.294731140136719\n",
      "epoch 17 batch 4 loss: 5.314523696899414\n",
      "epoch 17 batch 5 loss: 5.290106296539307\n",
      "epoch 17 batch 6 loss: 5.289819240570068\n",
      "epoch 17 batch 7 loss: 5.275697708129883\n",
      "epoch 17 batch 8 loss: 5.26121711730957\n",
      "epoch 17 batch 9 loss: 5.261357307434082\n",
      "epoch 17 batch 10 loss: 5.26739501953125\n",
      "epoch 17 batch 11 loss: 5.287538528442383\n",
      "epoch 17 batch 12 loss: 5.306526184082031\n",
      "epoch 17 batch 13 loss: 5.273076057434082\n",
      "epoch 17 batch 14 loss: 5.288494110107422\n",
      "epoch 17 batch 15 loss: 5.30872917175293\n",
      "epoch 17 batch 16 loss: 5.277277946472168\n",
      "epoch 17 batch 17 loss: 5.335768222808838\n",
      "epoch 17 batch 18 loss: 5.279399394989014\n",
      "epoch 17 batch 19 loss: 5.299410343170166\n",
      "epoch 17 batch 20 loss: 5.333856105804443\n",
      "epoch 17 batch 21 loss: 5.293065071105957\n",
      "epoch 17 batch 22 loss: 5.266584396362305\n",
      "epoch 17 batch 23 loss: 5.325187683105469\n",
      "epoch 17 batch 24 loss: 5.277626037597656\n",
      "epoch 17 batch 25 loss: 5.261890888214111\n",
      "epoch 17 batch 26 loss: 5.312962532043457\n",
      "epoch 17 batch 27 loss: 5.320550918579102\n",
      "epoch 17 batch 28 loss: 5.332283973693848\n",
      "epoch 17 batch 29 loss: 5.298591136932373\n",
      "epoch 17 batch 30 loss: 5.309556007385254\n",
      "epoch 17 batch 31 loss: 5.282485008239746\n",
      "epoch 17 batch 32 loss: 5.3133544921875\n",
      "epoch 17 batch 33 loss: 5.270519733428955\n",
      "epoch 17 batch 34 loss: 5.281121253967285\n",
      "epoch 17 batch 35 loss: 5.329113006591797\n",
      "epoch 17 batch 36 loss: 5.305001258850098\n",
      "epoch 17 batch 37 loss: 5.271666049957275\n",
      "epoch 17 batch 38 loss: 5.329919338226318\n",
      "epoch 17 batch 39 loss: 5.27142858505249\n",
      "epoch 17 batch 40 loss: 5.325662612915039\n",
      "epoch 17 batch 41 loss: 5.319025039672852\n",
      "epoch 17 batch 42 loss: 5.29346227645874\n",
      "epoch 17 batch 43 loss: 5.265463829040527\n",
      "epoch 17 batch 44 loss: 5.314051628112793\n",
      "epoch 17 batch 45 loss: 5.312337398529053\n",
      "epoch 17 batch 46 loss: 5.332086086273193\n",
      "epoch 17 batch 47 loss: 5.337531566619873\n",
      "epoch 17 batch 48 loss: 5.296945571899414\n",
      "epoch 17 batch 49 loss: 5.330891132354736\n",
      "epoch 17 batch 50 loss: 5.293010234832764\n",
      "epoch 17 batch 51 loss: 5.312056541442871\n",
      "epoch 17 batch 52 loss: 5.309290885925293\n",
      "epoch 17 batch 53 loss: 5.266381740570068\n",
      "epoch 17 batch 54 loss: 5.2875285148620605\n",
      "epoch 17 batch 55 loss: 5.279212474822998\n",
      "epoch 17 batch 56 loss: 5.323428630828857\n",
      "epoch 17 batch 57 loss: 5.332944393157959\n",
      "epoch 17 batch 58 loss: 5.2906060218811035\n",
      "epoch 17 batch 59 loss: 5.28179931640625\n",
      "epoch 17 batch 60 loss: 5.299386978149414\n",
      "epoch 17 batch 61 loss: 5.24396276473999\n",
      "epoch 17 batch 62 loss: 5.2866387367248535\n",
      "epoch 17 batch 63 loss: 5.2948102951049805\n",
      "epoch 17 batch 64 loss: 5.285590648651123\n",
      "epoch 17 batch 65 loss: 5.27530574798584\n",
      "epoch 17 batch 66 loss: 5.306612491607666\n",
      "epoch 17 batch 67 loss: 5.298408031463623\n",
      "epoch 17 batch 68 loss: 5.287997245788574\n",
      "epoch 17 batch 69 loss: 5.259675025939941\n",
      "epoch 17 batch 70 loss: 5.2792768478393555\n",
      "epoch 17 batch 71 loss: 5.290244102478027\n",
      "epoch 17 batch 72 loss: 5.320894718170166\n",
      "epoch 17 batch 73 loss: 5.314996242523193\n",
      "epoch 17 batch 74 loss: 5.290433406829834\n",
      "epoch 17 batch 75 loss: 5.271570205688477\n",
      "epoch 17 batch 76 loss: 5.267348289489746\n",
      "epoch 17 batch 77 loss: 5.2691826820373535\n",
      "epoch 17 batch 78 loss: 5.304877281188965\n",
      "epoch 17 batch 79 loss: 5.285778522491455\n",
      "epoch 17 batch 80 loss: 5.313727855682373\n",
      "epoch 17 batch 81 loss: 5.3414506912231445\n",
      "epoch 17 batch 82 loss: 5.304269313812256\n",
      "epoch 17 batch 83 loss: 5.28957462310791\n",
      "epoch 17 batch 84 loss: 5.27226448059082\n",
      "epoch 17 batch 85 loss: 5.277514457702637\n",
      "epoch 17 batch 86 loss: 5.265636920928955\n",
      "epoch 17 batch 87 loss: 5.264725208282471\n",
      "epoch 17 batch 88 loss: 5.295770168304443\n",
      "epoch 17 batch 89 loss: 5.274338722229004\n",
      "epoch 17 batch 90 loss: 5.287214279174805\n",
      "epoch 17 batch 91 loss: 5.301082611083984\n",
      "epoch 17 batch 92 loss: 5.267830848693848\n",
      "epoch 17 batch 93 loss: 5.268400192260742\n",
      "epoch 17 batch 94 loss: 5.281803607940674\n",
      "epoch 17 batch 95 loss: 5.308427810668945\n",
      "epoch 17 batch 96 loss: 5.279913425445557\n",
      "epoch 17 batch 97 loss: 5.299386501312256\n",
      "epoch 17 batch 98 loss: 5.284432888031006\n",
      "epoch 17 batch 99 loss: 5.296308994293213\n",
      "epoch 17 batch 100 loss: 5.307531833648682\n",
      "epoch 17 batch 101 loss: 5.259455680847168\n",
      "epoch 17 batch 102 loss: 5.289790153503418\n",
      "epoch 17 batch 103 loss: 5.279617786407471\n",
      "epoch 17 batch 104 loss: 5.278876781463623\n",
      "epoch 17 batch 105 loss: 5.28582763671875\n",
      "epoch 17 batch 106 loss: 5.282669544219971\n",
      "epoch 17 batch 107 loss: 5.287199020385742\n",
      "epoch 17 batch 108 loss: 5.311916351318359\n",
      "epoch 17 batch 109 loss: 5.306696891784668\n",
      "epoch 17 batch 110 loss: 5.2822065353393555\n",
      "epoch 17 batch 111 loss: 5.257595062255859\n",
      "epoch 17 batch 112 loss: 5.255595684051514\n",
      "epoch 17 batch 113 loss: 5.2910380363464355\n",
      "epoch 17 batch 114 loss: 5.30448579788208\n",
      "epoch 17 batch 115 loss: 5.301003456115723\n",
      "epoch 17 batch 116 loss: 5.34623908996582\n",
      "epoch 17 batch 117 loss: 5.295228004455566\n",
      "epoch 17 batch 118 loss: 5.273599624633789\n",
      "epoch 17 batch 119 loss: 5.26165771484375\n",
      "epoch 17 batch 120 loss: 5.262316703796387\n",
      "epoch 17 batch 121 loss: 5.299211502075195\n",
      "epoch 17 batch 122 loss: 5.283034324645996\n",
      "epoch 17 batch 123 loss: 5.310840129852295\n",
      "epoch 17 batch 124 loss: 5.269389629364014\n",
      "epoch loss: 5.2924319267272955\n",
      "epoch 18 batch 0 loss: 5.293330192565918\n",
      "epoch 18 batch 1 loss: 5.299917697906494\n",
      "epoch 18 batch 2 loss: 5.292260646820068\n",
      "epoch 18 batch 3 loss: 5.261937141418457\n",
      "epoch 18 batch 4 loss: 5.25257682800293\n",
      "epoch 18 batch 5 loss: 5.268886089324951\n",
      "epoch 18 batch 6 loss: 5.258151531219482\n",
      "epoch 18 batch 7 loss: 5.291179180145264\n",
      "epoch 18 batch 8 loss: 5.286839485168457\n",
      "epoch 18 batch 9 loss: 5.285942077636719\n",
      "epoch 18 batch 10 loss: 5.299045085906982\n",
      "epoch 18 batch 11 loss: 5.30643892288208\n",
      "epoch 18 batch 12 loss: 5.337977886199951\n",
      "epoch 18 batch 13 loss: 5.271153926849365\n",
      "epoch 18 batch 14 loss: 5.321069240570068\n",
      "epoch 18 batch 15 loss: 5.277534008026123\n",
      "epoch 18 batch 16 loss: 5.344031810760498\n",
      "epoch 18 batch 17 loss: 5.281374454498291\n",
      "epoch 18 batch 18 loss: 5.253758430480957\n",
      "epoch 18 batch 19 loss: 5.28835391998291\n",
      "epoch 18 batch 20 loss: 5.294215679168701\n",
      "epoch 18 batch 21 loss: 5.2778449058532715\n",
      "epoch 18 batch 22 loss: 5.271202087402344\n",
      "epoch 18 batch 23 loss: 5.286835670471191\n",
      "epoch 18 batch 24 loss: 5.249957084655762\n",
      "epoch 18 batch 25 loss: 5.296511173248291\n",
      "epoch 18 batch 26 loss: 5.26691198348999\n",
      "epoch 18 batch 27 loss: 5.219061374664307\n",
      "epoch 18 batch 28 loss: 5.285281658172607\n",
      "epoch 18 batch 29 loss: 5.291251182556152\n",
      "epoch 18 batch 30 loss: 5.284886360168457\n",
      "epoch 18 batch 31 loss: 5.273337364196777\n",
      "epoch 18 batch 32 loss: 5.293621063232422\n",
      "epoch 18 batch 33 loss: 5.272634983062744\n",
      "epoch 18 batch 34 loss: 5.322096347808838\n",
      "epoch 18 batch 35 loss: 5.308167457580566\n",
      "epoch 18 batch 36 loss: 5.277967929840088\n",
      "epoch 18 batch 37 loss: 5.279537200927734\n",
      "epoch 18 batch 38 loss: 5.292507648468018\n",
      "epoch 18 batch 39 loss: 5.303292274475098\n",
      "epoch 18 batch 40 loss: 5.2965087890625\n",
      "epoch 18 batch 41 loss: 5.305811882019043\n",
      "epoch 18 batch 42 loss: 5.311888694763184\n",
      "epoch 18 batch 43 loss: 5.318874359130859\n",
      "epoch 18 batch 44 loss: 5.254390716552734\n",
      "epoch 18 batch 45 loss: 5.291003227233887\n",
      "epoch 18 batch 46 loss: 5.317080974578857\n",
      "epoch 18 batch 47 loss: 5.254605770111084\n",
      "epoch 18 batch 48 loss: 5.294609069824219\n",
      "epoch 18 batch 49 loss: 5.252079963684082\n",
      "epoch 18 batch 50 loss: 5.318199157714844\n",
      "epoch 18 batch 51 loss: 5.297914505004883\n",
      "epoch 18 batch 52 loss: 5.289804458618164\n",
      "epoch 18 batch 53 loss: 5.282917499542236\n",
      "epoch 18 batch 54 loss: 5.299949645996094\n",
      "epoch 18 batch 55 loss: 5.262628078460693\n",
      "epoch 18 batch 56 loss: 5.249138832092285\n",
      "epoch 18 batch 57 loss: 5.277599811553955\n",
      "epoch 18 batch 58 loss: 5.323715686798096\n",
      "epoch 18 batch 59 loss: 5.267861366271973\n",
      "epoch 18 batch 60 loss: 5.291572093963623\n",
      "epoch 18 batch 61 loss: 5.276594161987305\n",
      "epoch 18 batch 62 loss: 5.275358200073242\n",
      "epoch 18 batch 63 loss: 5.269150257110596\n",
      "epoch 18 batch 64 loss: 5.206680774688721\n",
      "epoch 18 batch 65 loss: 5.265468597412109\n",
      "epoch 18 batch 66 loss: 5.32194709777832\n",
      "epoch 18 batch 67 loss: 5.249507904052734\n",
      "epoch 18 batch 68 loss: 5.272204875946045\n",
      "epoch 18 batch 69 loss: 5.30628776550293\n",
      "epoch 18 batch 70 loss: 5.303532123565674\n",
      "epoch 18 batch 71 loss: 5.303502082824707\n",
      "epoch 18 batch 72 loss: 5.313063144683838\n",
      "epoch 18 batch 73 loss: 5.294594764709473\n",
      "epoch 18 batch 74 loss: 5.295400142669678\n",
      "epoch 18 batch 75 loss: 5.342912673950195\n",
      "epoch 18 batch 76 loss: 5.287989616394043\n",
      "epoch 18 batch 77 loss: 5.259283542633057\n",
      "epoch 18 batch 78 loss: 5.302484512329102\n",
      "epoch 18 batch 79 loss: 5.3030900955200195\n",
      "epoch 18 batch 80 loss: 5.303203105926514\n",
      "epoch 18 batch 81 loss: 5.29295015335083\n",
      "epoch 18 batch 82 loss: 5.300363540649414\n",
      "epoch 18 batch 83 loss: 5.317821502685547\n",
      "epoch 18 batch 84 loss: 5.26027250289917\n",
      "epoch 18 batch 85 loss: 5.293749809265137\n",
      "epoch 18 batch 86 loss: 5.28788948059082\n",
      "epoch 18 batch 87 loss: 5.307646751403809\n",
      "epoch 18 batch 88 loss: 5.290968894958496\n",
      "epoch 18 batch 89 loss: 5.267337322235107\n",
      "epoch 18 batch 90 loss: 5.27818489074707\n",
      "epoch 18 batch 91 loss: 5.276790142059326\n",
      "epoch 18 batch 92 loss: 5.275221824645996\n",
      "epoch 18 batch 93 loss: 5.259222984313965\n",
      "epoch 18 batch 94 loss: 5.25460147857666\n",
      "epoch 18 batch 95 loss: 5.292818546295166\n",
      "epoch 18 batch 96 loss: 5.262991905212402\n",
      "epoch 18 batch 97 loss: 5.29498291015625\n",
      "epoch 18 batch 98 loss: 5.377151966094971\n",
      "epoch 18 batch 99 loss: 5.284382343292236\n",
      "epoch 18 batch 100 loss: 5.264095783233643\n",
      "epoch 18 batch 101 loss: 5.28863525390625\n",
      "epoch 18 batch 102 loss: 5.271761894226074\n",
      "epoch 18 batch 103 loss: 5.292370319366455\n",
      "epoch 18 batch 104 loss: 5.251130104064941\n",
      "epoch 18 batch 105 loss: 5.27992582321167\n",
      "epoch 18 batch 106 loss: 5.2593255043029785\n",
      "epoch 18 batch 107 loss: 5.299562931060791\n",
      "epoch 18 batch 108 loss: 5.297656059265137\n",
      "epoch 18 batch 109 loss: 5.299901485443115\n",
      "epoch 18 batch 110 loss: 5.296308517456055\n",
      "epoch 18 batch 111 loss: 5.279720783233643\n",
      "epoch 18 batch 112 loss: 5.285290241241455\n",
      "epoch 18 batch 113 loss: 5.255271911621094\n",
      "epoch 18 batch 114 loss: 5.251602649688721\n",
      "epoch 18 batch 115 loss: 5.302553653717041\n",
      "epoch 18 batch 116 loss: 5.3063788414001465\n",
      "epoch 18 batch 117 loss: 5.337971210479736\n",
      "epoch 18 batch 118 loss: 5.280561447143555\n",
      "epoch 18 batch 119 loss: 5.266970157623291\n",
      "epoch 18 batch 120 loss: 5.271916389465332\n",
      "epoch 18 batch 121 loss: 5.281490325927734\n",
      "epoch 18 batch 122 loss: 5.278182029724121\n",
      "epoch 18 batch 123 loss: 5.277245998382568\n",
      "epoch 18 batch 124 loss: 5.2494120597839355\n",
      "epoch loss: 5.285839042663574\n",
      "epoch 19 batch 0 loss: 5.291111469268799\n",
      "epoch 19 batch 1 loss: 5.318875789642334\n",
      "epoch 19 batch 2 loss: 5.250192642211914\n",
      "epoch 19 batch 3 loss: 5.246464252471924\n",
      "epoch 19 batch 4 loss: 5.264686584472656\n",
      "epoch 19 batch 5 loss: 5.283750057220459\n",
      "epoch 19 batch 6 loss: 5.26894998550415\n",
      "epoch 19 batch 7 loss: 5.260559558868408\n",
      "epoch 19 batch 8 loss: 5.305672645568848\n",
      "epoch 19 batch 9 loss: 5.2943243980407715\n",
      "epoch 19 batch 10 loss: 5.272347450256348\n",
      "epoch 19 batch 11 loss: 5.270201683044434\n",
      "epoch 19 batch 12 loss: 5.266790866851807\n",
      "epoch 19 batch 13 loss: 5.263078689575195\n",
      "epoch 19 batch 14 loss: 5.296865940093994\n",
      "epoch 19 batch 15 loss: 5.289875507354736\n",
      "epoch 19 batch 16 loss: 5.304646015167236\n",
      "epoch 19 batch 17 loss: 5.284432411193848\n",
      "epoch 19 batch 18 loss: 5.329265594482422\n",
      "epoch 19 batch 19 loss: 5.272834777832031\n",
      "epoch 19 batch 20 loss: 5.289095878601074\n",
      "epoch 19 batch 21 loss: 5.30787467956543\n",
      "epoch 19 batch 22 loss: 5.280847549438477\n",
      "epoch 19 batch 23 loss: 5.282588005065918\n",
      "epoch 19 batch 24 loss: 5.266571998596191\n",
      "epoch 19 batch 25 loss: 5.253006458282471\n",
      "epoch 19 batch 26 loss: 5.327327251434326\n",
      "epoch 19 batch 27 loss: 5.273305416107178\n",
      "epoch 19 batch 28 loss: 5.298562049865723\n",
      "epoch 19 batch 29 loss: 5.267756938934326\n",
      "epoch 19 batch 30 loss: 5.306097507476807\n",
      "epoch 19 batch 31 loss: 5.262228488922119\n",
      "epoch 19 batch 32 loss: 5.292403221130371\n",
      "epoch 19 batch 33 loss: 5.256308078765869\n",
      "epoch 19 batch 34 loss: 5.285453796386719\n",
      "epoch 19 batch 35 loss: 5.285524845123291\n",
      "epoch 19 batch 36 loss: 5.2870965003967285\n",
      "epoch 19 batch 37 loss: 5.267243385314941\n",
      "epoch 19 batch 38 loss: 5.218279838562012\n",
      "epoch 19 batch 39 loss: 5.289215087890625\n",
      "epoch 19 batch 40 loss: 5.303591728210449\n",
      "epoch 19 batch 41 loss: 5.349722862243652\n",
      "epoch 19 batch 42 loss: 5.280671119689941\n",
      "epoch 19 batch 43 loss: 5.299924373626709\n",
      "epoch 19 batch 44 loss: 5.2957658767700195\n",
      "epoch 19 batch 45 loss: 5.28713846206665\n",
      "epoch 19 batch 46 loss: 5.28324556350708\n",
      "epoch 19 batch 47 loss: 5.279226779937744\n",
      "epoch 19 batch 48 loss: 5.282108783721924\n",
      "epoch 19 batch 49 loss: 5.268959045410156\n",
      "epoch 19 batch 50 loss: 5.279653072357178\n",
      "epoch 19 batch 51 loss: 5.264543533325195\n",
      "epoch 19 batch 52 loss: 5.266672134399414\n",
      "epoch 19 batch 53 loss: 5.273697853088379\n",
      "epoch 19 batch 54 loss: 5.2807793617248535\n",
      "epoch 19 batch 55 loss: 5.318881511688232\n",
      "epoch 19 batch 56 loss: 5.318171501159668\n",
      "epoch 19 batch 57 loss: 5.272933006286621\n",
      "epoch 19 batch 58 loss: 5.244199275970459\n",
      "epoch 19 batch 59 loss: 5.269312858581543\n",
      "epoch 19 batch 60 loss: 5.277350902557373\n",
      "epoch 19 batch 61 loss: 5.237993240356445\n",
      "epoch 19 batch 62 loss: 5.271270751953125\n",
      "epoch 19 batch 63 loss: 5.281303405761719\n",
      "epoch 19 batch 64 loss: 5.284424781799316\n",
      "epoch 19 batch 65 loss: 5.269701957702637\n",
      "epoch 19 batch 66 loss: 5.286183834075928\n",
      "epoch 19 batch 67 loss: 5.263493061065674\n",
      "epoch 19 batch 68 loss: 5.279840469360352\n",
      "epoch 19 batch 69 loss: 5.296176433563232\n",
      "epoch 19 batch 70 loss: 5.272491455078125\n",
      "epoch 19 batch 71 loss: 5.261402606964111\n",
      "epoch 19 batch 72 loss: 5.239035606384277\n",
      "epoch 19 batch 73 loss: 5.287300109863281\n",
      "epoch 19 batch 74 loss: 5.331695556640625\n",
      "epoch 19 batch 75 loss: 5.2422404289245605\n",
      "epoch 19 batch 76 loss: 5.264283180236816\n",
      "epoch 19 batch 77 loss: 5.280665874481201\n",
      "epoch 19 batch 78 loss: 5.264671325683594\n",
      "epoch 19 batch 79 loss: 5.2939300537109375\n",
      "epoch 19 batch 80 loss: 5.324469089508057\n",
      "epoch 19 batch 81 loss: 5.274011135101318\n",
      "epoch 19 batch 82 loss: 5.262199401855469\n",
      "epoch 19 batch 83 loss: 5.342275142669678\n",
      "epoch 19 batch 84 loss: 5.282075881958008\n",
      "epoch 19 batch 85 loss: 5.273849964141846\n",
      "epoch 19 batch 86 loss: 5.278733253479004\n",
      "epoch 19 batch 87 loss: 5.3799662590026855\n",
      "epoch 19 batch 88 loss: 5.283441543579102\n",
      "epoch 19 batch 89 loss: 5.3136091232299805\n",
      "epoch 19 batch 90 loss: 5.275713920593262\n",
      "epoch 19 batch 91 loss: 5.246963977813721\n",
      "epoch 19 batch 92 loss: 5.262404441833496\n",
      "epoch 19 batch 93 loss: 5.277437686920166\n",
      "epoch 19 batch 94 loss: 5.290399074554443\n",
      "epoch 19 batch 95 loss: 5.292812824249268\n",
      "epoch 19 batch 96 loss: 5.301515579223633\n",
      "epoch 19 batch 97 loss: 5.279314041137695\n",
      "epoch 19 batch 98 loss: 5.281553268432617\n",
      "epoch 19 batch 99 loss: 5.310675144195557\n",
      "epoch 19 batch 100 loss: 5.274024963378906\n",
      "epoch 19 batch 101 loss: 5.295907974243164\n",
      "epoch 19 batch 102 loss: 5.272453308105469\n",
      "epoch 19 batch 103 loss: 5.291635513305664\n",
      "epoch 19 batch 104 loss: 5.304426670074463\n",
      "epoch 19 batch 105 loss: 5.272583961486816\n",
      "epoch 19 batch 106 loss: 5.2695112228393555\n",
      "epoch 19 batch 107 loss: 5.272951602935791\n",
      "epoch 19 batch 108 loss: 5.274627685546875\n",
      "epoch 19 batch 109 loss: 5.277125358581543\n",
      "epoch 19 batch 110 loss: 5.263050556182861\n",
      "epoch 19 batch 111 loss: 5.2854905128479\n",
      "epoch 19 batch 112 loss: 5.302261829376221\n",
      "epoch 19 batch 113 loss: 5.286250591278076\n",
      "epoch 19 batch 114 loss: 5.280017375946045\n",
      "epoch 19 batch 115 loss: 5.260397911071777\n",
      "epoch 19 batch 116 loss: 5.2865891456604\n",
      "epoch 19 batch 117 loss: 5.2441086769104\n",
      "epoch 19 batch 118 loss: 5.2758469581604\n",
      "epoch 19 batch 119 loss: 5.311116695404053\n",
      "epoch 19 batch 120 loss: 5.275646686553955\n",
      "epoch 19 batch 121 loss: 5.309998512268066\n",
      "epoch 19 batch 122 loss: 5.305046081542969\n",
      "epoch 19 batch 123 loss: 5.286111831665039\n",
      "epoch 19 batch 124 loss: 5.270992755889893\n",
      "epoch loss: 5.282543697357178\n",
      "epoch 20 batch 0 loss: 5.262596130371094\n",
      "epoch 20 batch 1 loss: 5.302947044372559\n",
      "epoch 20 batch 2 loss: 5.246722221374512\n",
      "epoch 20 batch 3 loss: 5.263092994689941\n",
      "epoch 20 batch 4 loss: 5.256357192993164\n",
      "epoch 20 batch 5 loss: 5.300741672515869\n",
      "epoch 20 batch 6 loss: 5.314564228057861\n",
      "epoch 20 batch 7 loss: 5.318545818328857\n",
      "epoch 20 batch 8 loss: 5.268618583679199\n",
      "epoch 20 batch 9 loss: 5.24612283706665\n",
      "epoch 20 batch 10 loss: 5.278298377990723\n",
      "epoch 20 batch 11 loss: 5.270371437072754\n",
      "epoch 20 batch 12 loss: 5.287805080413818\n",
      "epoch 20 batch 13 loss: 5.264472007751465\n",
      "epoch 20 batch 14 loss: 5.287421703338623\n",
      "epoch 20 batch 15 loss: 5.261724948883057\n",
      "epoch 20 batch 16 loss: 5.308210372924805\n",
      "epoch 20 batch 17 loss: 5.3046088218688965\n",
      "epoch 20 batch 18 loss: 5.279890060424805\n",
      "epoch 20 batch 19 loss: 5.280156135559082\n",
      "epoch 20 batch 20 loss: 5.219437122344971\n",
      "epoch 20 batch 21 loss: 5.251452445983887\n",
      "epoch 20 batch 22 loss: 5.27065372467041\n",
      "epoch 20 batch 23 loss: 5.289221286773682\n",
      "epoch 20 batch 24 loss: 5.266149997711182\n",
      "epoch 20 batch 25 loss: 5.24479341506958\n",
      "epoch 20 batch 26 loss: 5.265042781829834\n",
      "epoch 20 batch 27 loss: 5.294610500335693\n",
      "epoch 20 batch 28 loss: 5.284601211547852\n",
      "epoch 20 batch 29 loss: 5.269135475158691\n",
      "epoch 20 batch 30 loss: 5.266528129577637\n",
      "epoch 20 batch 31 loss: 5.263628005981445\n",
      "epoch 20 batch 32 loss: 5.296437740325928\n",
      "epoch 20 batch 33 loss: 5.267138481140137\n",
      "epoch 20 batch 34 loss: 5.312568187713623\n",
      "epoch 20 batch 35 loss: 5.25726318359375\n",
      "epoch 20 batch 36 loss: 5.26871919631958\n",
      "epoch 20 batch 37 loss: 5.236422061920166\n",
      "epoch 20 batch 38 loss: 5.271576881408691\n",
      "epoch 20 batch 39 loss: 5.275668144226074\n",
      "epoch 20 batch 40 loss: 5.234177112579346\n",
      "epoch 20 batch 41 loss: 5.274926662445068\n",
      "epoch 20 batch 42 loss: 5.266857624053955\n",
      "epoch 20 batch 43 loss: 5.292917251586914\n",
      "epoch 20 batch 44 loss: 5.273918151855469\n",
      "epoch 20 batch 45 loss: 5.269383430480957\n",
      "epoch 20 batch 46 loss: 5.26166296005249\n",
      "epoch 20 batch 47 loss: 5.249476909637451\n",
      "epoch 20 batch 48 loss: 5.256073951721191\n",
      "epoch 20 batch 49 loss: 5.274104595184326\n",
      "epoch 20 batch 50 loss: 5.252649307250977\n",
      "epoch 20 batch 51 loss: 5.2891035079956055\n",
      "epoch 20 batch 52 loss: 5.277764320373535\n",
      "epoch 20 batch 53 loss: 5.252270221710205\n",
      "epoch 20 batch 54 loss: 5.291667938232422\n",
      "epoch 20 batch 55 loss: 5.282797813415527\n",
      "epoch 20 batch 56 loss: 5.285738945007324\n",
      "epoch 20 batch 57 loss: 5.282074451446533\n",
      "epoch 20 batch 58 loss: 5.258553504943848\n",
      "epoch 20 batch 59 loss: 5.296810150146484\n",
      "epoch 20 batch 60 loss: 5.246806621551514\n",
      "epoch 20 batch 61 loss: 5.288093090057373\n",
      "epoch 20 batch 62 loss: 5.305491924285889\n",
      "epoch 20 batch 63 loss: 5.289787292480469\n",
      "epoch 20 batch 64 loss: 5.222246170043945\n",
      "epoch 20 batch 65 loss: 5.261826038360596\n",
      "epoch 20 batch 66 loss: 5.261025428771973\n",
      "epoch 20 batch 67 loss: 5.236203670501709\n",
      "epoch 20 batch 68 loss: 5.269776344299316\n",
      "epoch 20 batch 69 loss: 5.238739013671875\n",
      "epoch 20 batch 70 loss: 5.280954360961914\n",
      "epoch 20 batch 71 loss: 5.267261981964111\n",
      "epoch 20 batch 72 loss: 5.253449440002441\n",
      "epoch 20 batch 73 loss: 5.281639099121094\n",
      "epoch 20 batch 74 loss: 5.242534160614014\n",
      "epoch 20 batch 75 loss: 5.298738956451416\n",
      "epoch 20 batch 76 loss: 5.253618240356445\n",
      "epoch 20 batch 77 loss: 5.274401664733887\n",
      "epoch 20 batch 78 loss: 5.227171421051025\n",
      "epoch 20 batch 79 loss: 5.254255771636963\n",
      "epoch 20 batch 80 loss: 5.293930530548096\n",
      "epoch 20 batch 81 loss: 5.283780574798584\n",
      "epoch 20 batch 82 loss: 5.304934024810791\n",
      "epoch 20 batch 83 loss: 5.281467914581299\n",
      "epoch 20 batch 84 loss: 5.256699085235596\n",
      "epoch 20 batch 85 loss: 5.27265739440918\n",
      "epoch 20 batch 86 loss: 5.2888689041137695\n",
      "epoch 20 batch 87 loss: 5.274139404296875\n",
      "epoch 20 batch 88 loss: 5.237669467926025\n",
      "epoch 20 batch 89 loss: 5.302371025085449\n",
      "epoch 20 batch 90 loss: 5.236640930175781\n",
      "epoch 20 batch 91 loss: 5.290414810180664\n",
      "epoch 20 batch 92 loss: 5.294008255004883\n",
      "epoch 20 batch 93 loss: 5.264873027801514\n",
      "epoch 20 batch 94 loss: 5.2729010581970215\n",
      "epoch 20 batch 95 loss: 5.264718532562256\n",
      "epoch 20 batch 96 loss: 5.2759623527526855\n",
      "epoch 20 batch 97 loss: 5.298789978027344\n",
      "epoch 20 batch 98 loss: 5.2139105796813965\n",
      "epoch 20 batch 99 loss: 5.277547359466553\n",
      "epoch 20 batch 100 loss: 5.287573337554932\n",
      "epoch 20 batch 101 loss: 5.279111862182617\n",
      "epoch 20 batch 102 loss: 5.316403865814209\n",
      "epoch 20 batch 103 loss: 5.258176326751709\n",
      "epoch 20 batch 104 loss: 5.254446983337402\n",
      "epoch 20 batch 105 loss: 5.24256706237793\n",
      "epoch 20 batch 106 loss: 5.251063346862793\n",
      "epoch 20 batch 107 loss: 5.221675872802734\n",
      "epoch 20 batch 108 loss: 5.260148048400879\n",
      "epoch 20 batch 109 loss: 5.301727294921875\n",
      "epoch 20 batch 110 loss: 5.268416404724121\n",
      "epoch 20 batch 111 loss: 5.2421159744262695\n",
      "epoch 20 batch 112 loss: 5.305521011352539\n",
      "epoch 20 batch 113 loss: 5.2350029945373535\n",
      "epoch 20 batch 114 loss: 5.265520095825195\n",
      "epoch 20 batch 115 loss: 5.312057971954346\n",
      "epoch 20 batch 116 loss: 5.23618745803833\n",
      "epoch 20 batch 117 loss: 5.315985679626465\n",
      "epoch 20 batch 118 loss: 5.2947282791137695\n",
      "epoch 20 batch 119 loss: 5.287408828735352\n",
      "epoch 20 batch 120 loss: 5.28050422668457\n",
      "epoch 20 batch 121 loss: 5.257469177246094\n",
      "epoch 20 batch 122 loss: 5.265424728393555\n",
      "epoch 20 batch 123 loss: 5.292937278747559\n",
      "epoch 20 batch 124 loss: 5.250584602355957\n",
      "epoch loss: 5.27136247253418\n",
      "epoch 21 batch 0 loss: 5.284633636474609\n",
      "epoch 21 batch 1 loss: 5.240860462188721\n",
      "epoch 21 batch 2 loss: 5.265748023986816\n",
      "epoch 21 batch 3 loss: 5.259602069854736\n",
      "epoch 21 batch 4 loss: 5.262900352478027\n",
      "epoch 21 batch 5 loss: 5.282052993774414\n",
      "epoch 21 batch 6 loss: 5.282438278198242\n",
      "epoch 21 batch 7 loss: 5.254004955291748\n",
      "epoch 21 batch 8 loss: 5.257540225982666\n",
      "epoch 21 batch 9 loss: 5.2661027908325195\n",
      "epoch 21 batch 10 loss: 5.277482032775879\n",
      "epoch 21 batch 11 loss: 5.264794826507568\n",
      "epoch 21 batch 12 loss: 5.260382175445557\n",
      "epoch 21 batch 13 loss: 5.256304740905762\n",
      "epoch 21 batch 14 loss: 5.299519062042236\n",
      "epoch 21 batch 15 loss: 5.282233238220215\n",
      "epoch 21 batch 16 loss: 5.308341026306152\n",
      "epoch 21 batch 17 loss: 5.276132583618164\n",
      "epoch 21 batch 18 loss: 5.265303134918213\n",
      "epoch 21 batch 19 loss: 5.2786078453063965\n",
      "epoch 21 batch 20 loss: 5.310056209564209\n",
      "epoch 21 batch 21 loss: 5.223705768585205\n",
      "epoch 21 batch 22 loss: 5.319362640380859\n",
      "epoch 21 batch 23 loss: 5.272193908691406\n",
      "epoch 21 batch 24 loss: 5.26856803894043\n",
      "epoch 21 batch 25 loss: 5.263175964355469\n",
      "epoch 21 batch 26 loss: 5.232636451721191\n",
      "epoch 21 batch 27 loss: 5.295199871063232\n",
      "epoch 21 batch 28 loss: 5.264290809631348\n",
      "epoch 21 batch 29 loss: 5.241382122039795\n",
      "epoch 21 batch 30 loss: 5.260206699371338\n",
      "epoch 21 batch 31 loss: 5.258876800537109\n",
      "epoch 21 batch 32 loss: 5.225134372711182\n",
      "epoch 21 batch 33 loss: 5.287805080413818\n",
      "epoch 21 batch 34 loss: 5.25122594833374\n",
      "epoch 21 batch 35 loss: 5.300947666168213\n",
      "epoch 21 batch 36 loss: 5.298715591430664\n",
      "epoch 21 batch 37 loss: 5.25520133972168\n",
      "epoch 21 batch 38 loss: 5.264644622802734\n",
      "epoch 21 batch 39 loss: 5.258612155914307\n",
      "epoch 21 batch 40 loss: 5.250604152679443\n",
      "epoch 21 batch 41 loss: 5.296597957611084\n",
      "epoch 21 batch 42 loss: 5.303017616271973\n",
      "epoch 21 batch 43 loss: 5.234256267547607\n",
      "epoch 21 batch 44 loss: 5.268550395965576\n",
      "epoch 21 batch 45 loss: 5.25408935546875\n",
      "epoch 21 batch 46 loss: 5.237354755401611\n",
      "epoch 21 batch 47 loss: 5.261958122253418\n",
      "epoch 21 batch 48 loss: 5.234657287597656\n",
      "epoch 21 batch 49 loss: 5.2949347496032715\n",
      "epoch 21 batch 50 loss: 5.269514083862305\n",
      "epoch 21 batch 51 loss: 5.264269828796387\n",
      "epoch 21 batch 52 loss: 5.261679649353027\n",
      "epoch 21 batch 53 loss: 5.252636432647705\n",
      "epoch 21 batch 54 loss: 5.284801006317139\n",
      "epoch 21 batch 55 loss: 5.232330322265625\n",
      "epoch 21 batch 56 loss: 5.277797698974609\n",
      "epoch 21 batch 57 loss: 5.234610557556152\n",
      "epoch 21 batch 58 loss: 5.264049053192139\n",
      "epoch 21 batch 59 loss: 5.277575969696045\n",
      "epoch 21 batch 60 loss: 5.236598014831543\n",
      "epoch 21 batch 61 loss: 5.2384033203125\n",
      "epoch 21 batch 62 loss: 5.275406360626221\n",
      "epoch 21 batch 63 loss: 5.245568752288818\n",
      "epoch 21 batch 64 loss: 5.235691547393799\n",
      "epoch 21 batch 65 loss: 5.279964447021484\n",
      "epoch 21 batch 66 loss: 5.23600435256958\n",
      "epoch 21 batch 67 loss: 5.311318397521973\n",
      "epoch 21 batch 68 loss: 5.259136199951172\n",
      "epoch 21 batch 69 loss: 5.264464378356934\n",
      "epoch 21 batch 70 loss: 5.25369930267334\n",
      "epoch 21 batch 71 loss: 5.2522172927856445\n",
      "epoch 21 batch 72 loss: 5.274364948272705\n",
      "epoch 21 batch 73 loss: 5.250439167022705\n",
      "epoch 21 batch 74 loss: 5.303536415100098\n",
      "epoch 21 batch 75 loss: 5.277280807495117\n",
      "epoch 21 batch 76 loss: 5.301591873168945\n",
      "epoch 21 batch 77 loss: 5.290961742401123\n",
      "epoch 21 batch 78 loss: 5.290029048919678\n",
      "epoch 21 batch 79 loss: 5.251386642456055\n",
      "epoch 21 batch 80 loss: 5.279820919036865\n",
      "epoch 21 batch 81 loss: 5.284109592437744\n",
      "epoch 21 batch 82 loss: 5.284602165222168\n",
      "epoch 21 batch 83 loss: 5.225348949432373\n",
      "epoch 21 batch 84 loss: 5.27957820892334\n",
      "epoch 21 batch 85 loss: 5.285500526428223\n",
      "epoch 21 batch 86 loss: 5.300809383392334\n",
      "epoch 21 batch 87 loss: 5.26328182220459\n",
      "epoch 21 batch 88 loss: 5.264153480529785\n",
      "epoch 21 batch 89 loss: 5.288887977600098\n",
      "epoch 21 batch 90 loss: 5.263636589050293\n",
      "epoch 21 batch 91 loss: 5.288374423980713\n",
      "epoch 21 batch 92 loss: 5.237372875213623\n",
      "epoch 21 batch 93 loss: 5.237934589385986\n",
      "epoch 21 batch 94 loss: 5.266837120056152\n",
      "epoch 21 batch 95 loss: 5.257486343383789\n",
      "epoch 21 batch 96 loss: 5.244616508483887\n",
      "epoch 21 batch 97 loss: 5.273110389709473\n",
      "epoch 21 batch 98 loss: 5.269913196563721\n",
      "epoch 21 batch 99 loss: 5.279757022857666\n",
      "epoch 21 batch 100 loss: 5.277651786804199\n",
      "epoch 21 batch 101 loss: 5.246038913726807\n",
      "epoch 21 batch 102 loss: 5.257739067077637\n",
      "epoch 21 batch 103 loss: 5.259147644042969\n",
      "epoch 21 batch 104 loss: 5.251214504241943\n",
      "epoch 21 batch 105 loss: 5.283422946929932\n",
      "epoch 21 batch 106 loss: 5.27345085144043\n",
      "epoch 21 batch 107 loss: 5.285302639007568\n",
      "epoch 21 batch 108 loss: 5.268736362457275\n",
      "epoch 21 batch 109 loss: 5.248201847076416\n",
      "epoch 21 batch 110 loss: 5.296628475189209\n",
      "epoch 21 batch 111 loss: 5.262699604034424\n",
      "epoch 21 batch 112 loss: 5.285488128662109\n",
      "epoch 21 batch 113 loss: 5.251608371734619\n",
      "epoch 21 batch 114 loss: 5.2683939933776855\n",
      "epoch 21 batch 115 loss: 5.303236484527588\n",
      "epoch 21 batch 116 loss: 5.276858806610107\n",
      "epoch 21 batch 117 loss: 5.256304740905762\n",
      "epoch 21 batch 118 loss: 5.250993728637695\n",
      "epoch 21 batch 119 loss: 5.264420032501221\n",
      "epoch 21 batch 120 loss: 5.238113880157471\n",
      "epoch 21 batch 121 loss: 5.272411823272705\n",
      "epoch 21 batch 122 loss: 5.287294864654541\n",
      "epoch 21 batch 123 loss: 5.226316928863525\n",
      "epoch 21 batch 124 loss: 5.290689468383789\n",
      "epoch loss: 5.267342182159424\n",
      "epoch 22 batch 0 loss: 5.263754844665527\n",
      "epoch 22 batch 1 loss: 5.271045207977295\n",
      "epoch 22 batch 2 loss: 5.276460647583008\n",
      "epoch 22 batch 3 loss: 5.294953346252441\n",
      "epoch 22 batch 4 loss: 5.280023097991943\n",
      "epoch 22 batch 5 loss: 5.255921363830566\n",
      "epoch 22 batch 6 loss: 5.235257625579834\n",
      "epoch 22 batch 7 loss: 5.214937686920166\n",
      "epoch 22 batch 8 loss: 5.231982231140137\n",
      "epoch 22 batch 9 loss: 5.263291358947754\n",
      "epoch 22 batch 10 loss: 5.275376319885254\n",
      "epoch 22 batch 11 loss: 5.279805660247803\n",
      "epoch 22 batch 12 loss: 5.2718095779418945\n",
      "epoch 22 batch 13 loss: 5.248202323913574\n",
      "epoch 22 batch 14 loss: 5.243017673492432\n",
      "epoch 22 batch 15 loss: 5.282745838165283\n",
      "epoch 22 batch 16 loss: 5.273141860961914\n",
      "epoch 22 batch 17 loss: 5.289601802825928\n",
      "epoch 22 batch 18 loss: 5.315781593322754\n",
      "epoch 22 batch 19 loss: 5.24497127532959\n",
      "epoch 22 batch 20 loss: 5.292073726654053\n",
      "epoch 22 batch 21 loss: 5.279841423034668\n",
      "epoch 22 batch 22 loss: 5.283434867858887\n",
      "epoch 22 batch 23 loss: 5.263172626495361\n",
      "epoch 22 batch 24 loss: 5.263332366943359\n",
      "epoch 22 batch 25 loss: 5.302033424377441\n",
      "epoch 22 batch 26 loss: 5.273605823516846\n",
      "epoch 22 batch 27 loss: 5.267463684082031\n",
      "epoch 22 batch 28 loss: 5.265945911407471\n",
      "epoch 22 batch 29 loss: 5.280875205993652\n",
      "epoch 22 batch 30 loss: 5.243359088897705\n",
      "epoch 22 batch 31 loss: 5.233396053314209\n",
      "epoch 22 batch 32 loss: 5.259685516357422\n",
      "epoch 22 batch 33 loss: 5.27987003326416\n",
      "epoch 22 batch 34 loss: 5.265499591827393\n",
      "epoch 22 batch 35 loss: 5.269665241241455\n",
      "epoch 22 batch 36 loss: 5.262300491333008\n",
      "epoch 22 batch 37 loss: 5.28720760345459\n",
      "epoch 22 batch 38 loss: 5.287750244140625\n",
      "epoch 22 batch 39 loss: 5.253242015838623\n",
      "epoch 22 batch 40 loss: 5.263911247253418\n",
      "epoch 22 batch 41 loss: 5.294336318969727\n",
      "epoch 22 batch 42 loss: 5.227334499359131\n",
      "epoch 22 batch 43 loss: 5.243550777435303\n",
      "epoch 22 batch 44 loss: 5.285079002380371\n",
      "epoch 22 batch 45 loss: 5.273522853851318\n",
      "epoch 22 batch 46 loss: 5.265064716339111\n",
      "epoch 22 batch 47 loss: 5.241347312927246\n",
      "epoch 22 batch 48 loss: 5.239373207092285\n",
      "epoch 22 batch 49 loss: 5.286715030670166\n",
      "epoch 22 batch 50 loss: 5.270567893981934\n",
      "epoch 22 batch 51 loss: 5.261913299560547\n",
      "epoch 22 batch 52 loss: 5.251193523406982\n",
      "epoch 22 batch 53 loss: 5.260102272033691\n",
      "epoch 22 batch 54 loss: 5.276155948638916\n",
      "epoch 22 batch 55 loss: 5.247931003570557\n",
      "epoch 22 batch 56 loss: 5.242111682891846\n",
      "epoch 22 batch 57 loss: 5.247220516204834\n",
      "epoch 22 batch 58 loss: 5.2519755363464355\n",
      "epoch 22 batch 59 loss: 5.292037010192871\n",
      "epoch 22 batch 60 loss: 5.284830093383789\n",
      "epoch 22 batch 61 loss: 5.275755405426025\n",
      "epoch 22 batch 62 loss: 5.265104293823242\n",
      "epoch 22 batch 63 loss: 5.227055549621582\n",
      "epoch 22 batch 64 loss: 5.251163005828857\n",
      "epoch 22 batch 65 loss: 5.2422966957092285\n",
      "epoch 22 batch 66 loss: 5.262561321258545\n",
      "epoch 22 batch 67 loss: 5.2493510246276855\n",
      "epoch 22 batch 68 loss: 5.240461349487305\n",
      "epoch 22 batch 69 loss: 5.262183666229248\n",
      "epoch 22 batch 70 loss: 5.291970252990723\n",
      "epoch 22 batch 71 loss: 5.287114143371582\n",
      "epoch 22 batch 72 loss: 5.234255790710449\n",
      "epoch 22 batch 73 loss: 5.2433061599731445\n",
      "epoch 22 batch 74 loss: 5.269097805023193\n",
      "epoch 22 batch 75 loss: 5.269227981567383\n",
      "epoch 22 batch 76 loss: 5.309141159057617\n",
      "epoch 22 batch 77 loss: 5.26902961730957\n",
      "epoch 22 batch 78 loss: 5.271820068359375\n",
      "epoch 22 batch 79 loss: 5.2411699295043945\n",
      "epoch 22 batch 80 loss: 5.270829200744629\n",
      "epoch 22 batch 81 loss: 5.250962257385254\n",
      "epoch 22 batch 82 loss: 5.276039123535156\n",
      "epoch 22 batch 83 loss: 5.267009258270264\n",
      "epoch 22 batch 84 loss: 5.24393892288208\n",
      "epoch 22 batch 85 loss: 5.253551006317139\n",
      "epoch 22 batch 86 loss: 5.245795726776123\n",
      "epoch 22 batch 87 loss: 5.283434867858887\n",
      "epoch 22 batch 88 loss: 5.273113250732422\n",
      "epoch 22 batch 89 loss: 5.276704788208008\n",
      "epoch 22 batch 90 loss: 5.282736778259277\n",
      "epoch 22 batch 91 loss: 5.228370189666748\n",
      "epoch 22 batch 92 loss: 5.255951881408691\n",
      "epoch 22 batch 93 loss: 5.279033184051514\n",
      "epoch 22 batch 94 loss: 5.267735481262207\n",
      "epoch 22 batch 95 loss: 5.233625411987305\n",
      "epoch 22 batch 96 loss: 5.2607011795043945\n",
      "epoch 22 batch 97 loss: 5.227427959442139\n",
      "epoch 22 batch 98 loss: 5.26099967956543\n",
      "epoch 22 batch 99 loss: 5.2645673751831055\n",
      "epoch 22 batch 100 loss: 5.2452287673950195\n",
      "epoch 22 batch 101 loss: 5.229108810424805\n",
      "epoch 22 batch 102 loss: 5.285934925079346\n",
      "epoch 22 batch 103 loss: 5.265683650970459\n",
      "epoch 22 batch 104 loss: 5.267024517059326\n",
      "epoch 22 batch 105 loss: 5.234268665313721\n",
      "epoch 22 batch 106 loss: 5.25760555267334\n",
      "epoch 22 batch 107 loss: 5.2747483253479\n",
      "epoch 22 batch 108 loss: 5.259552001953125\n",
      "epoch 22 batch 109 loss: 5.266700267791748\n",
      "epoch 22 batch 110 loss: 5.296899318695068\n",
      "epoch 22 batch 111 loss: 5.2672247886657715\n",
      "epoch 22 batch 112 loss: 5.275326251983643\n",
      "epoch 22 batch 113 loss: 5.305350303649902\n",
      "epoch 22 batch 114 loss: 5.253372669219971\n",
      "epoch 22 batch 115 loss: 5.27971076965332\n",
      "epoch 22 batch 116 loss: 5.274325847625732\n",
      "epoch 22 batch 117 loss: 5.279499530792236\n",
      "epoch 22 batch 118 loss: 5.250421047210693\n",
      "epoch 22 batch 119 loss: 5.267297744750977\n",
      "epoch 22 batch 120 loss: 5.246006965637207\n",
      "epoch 22 batch 121 loss: 5.26908016204834\n",
      "epoch 22 batch 122 loss: 5.260139465332031\n",
      "epoch 22 batch 123 loss: 5.23297119140625\n",
      "epoch 22 batch 124 loss: 5.230948448181152\n",
      "epoch loss: 5.263769062042236\n",
      "epoch 23 batch 0 loss: 5.242554664611816\n",
      "epoch 23 batch 1 loss: 5.281045913696289\n",
      "epoch 23 batch 2 loss: 5.241708755493164\n",
      "epoch 23 batch 3 loss: 5.300041198730469\n",
      "epoch 23 batch 4 loss: 5.276396751403809\n",
      "epoch 23 batch 5 loss: 5.270920276641846\n",
      "epoch 23 batch 6 loss: 5.2618021965026855\n",
      "epoch 23 batch 7 loss: 5.248755931854248\n",
      "epoch 23 batch 8 loss: 5.260955810546875\n",
      "epoch 23 batch 9 loss: 5.283696174621582\n",
      "epoch 23 batch 10 loss: 5.219153881072998\n",
      "epoch 23 batch 11 loss: 5.255258560180664\n",
      "epoch 23 batch 12 loss: 5.254883766174316\n",
      "epoch 23 batch 13 loss: 5.264967918395996\n",
      "epoch 23 batch 14 loss: 5.28502082824707\n",
      "epoch 23 batch 15 loss: 5.264848709106445\n",
      "epoch 23 batch 16 loss: 5.262963771820068\n",
      "epoch 23 batch 17 loss: 5.256246089935303\n",
      "epoch 23 batch 18 loss: 5.226513385772705\n",
      "epoch 23 batch 19 loss: 5.266777038574219\n",
      "epoch 23 batch 20 loss: 5.262166500091553\n",
      "epoch 23 batch 21 loss: 5.270874977111816\n",
      "epoch 23 batch 22 loss: 5.317863464355469\n",
      "epoch 23 batch 23 loss: 5.241348743438721\n",
      "epoch 23 batch 24 loss: 5.258325099945068\n",
      "epoch 23 batch 25 loss: 5.276930809020996\n",
      "epoch 23 batch 26 loss: 5.254489421844482\n",
      "epoch 23 batch 27 loss: 5.267352104187012\n",
      "epoch 23 batch 28 loss: 5.267112731933594\n",
      "epoch 23 batch 29 loss: 5.323300838470459\n",
      "epoch 23 batch 30 loss: 5.240778923034668\n",
      "epoch 23 batch 31 loss: 5.281946182250977\n",
      "epoch 23 batch 32 loss: 5.263502597808838\n",
      "epoch 23 batch 33 loss: 5.256538391113281\n",
      "epoch 23 batch 34 loss: 5.258028984069824\n",
      "epoch 23 batch 35 loss: 5.229387283325195\n",
      "epoch 23 batch 36 loss: 5.30497932434082\n",
      "epoch 23 batch 37 loss: 5.242538928985596\n",
      "epoch 23 batch 38 loss: 5.248921871185303\n",
      "epoch 23 batch 39 loss: 5.248141765594482\n",
      "epoch 23 batch 40 loss: 5.262631893157959\n",
      "epoch 23 batch 41 loss: 5.336688041687012\n",
      "epoch 23 batch 42 loss: 5.26435661315918\n",
      "epoch 23 batch 43 loss: 5.2528815269470215\n",
      "epoch 23 batch 44 loss: 5.234227180480957\n",
      "epoch 23 batch 45 loss: 5.203930377960205\n",
      "epoch 23 batch 46 loss: 5.268616676330566\n",
      "epoch 23 batch 47 loss: 5.253480911254883\n",
      "epoch 23 batch 48 loss: 5.252658367156982\n",
      "epoch 23 batch 49 loss: 5.229450702667236\n",
      "epoch 23 batch 50 loss: 5.265261173248291\n",
      "epoch 23 batch 51 loss: 5.242425441741943\n",
      "epoch 23 batch 52 loss: 5.25895357131958\n",
      "epoch 23 batch 53 loss: 5.258941650390625\n",
      "epoch 23 batch 54 loss: 5.2755351066589355\n",
      "epoch 23 batch 55 loss: 5.266387939453125\n",
      "epoch 23 batch 56 loss: 5.294776916503906\n",
      "epoch 23 batch 57 loss: 5.262546062469482\n",
      "epoch 23 batch 58 loss: 5.278361797332764\n",
      "epoch 23 batch 59 loss: 5.256956577301025\n",
      "epoch 23 batch 60 loss: 5.228520393371582\n",
      "epoch 23 batch 61 loss: 5.2613654136657715\n",
      "epoch 23 batch 62 loss: 5.257917881011963\n",
      "epoch 23 batch 63 loss: 5.257997989654541\n",
      "epoch 23 batch 64 loss: 5.256972312927246\n",
      "epoch 23 batch 65 loss: 5.2771172523498535\n",
      "epoch 23 batch 66 loss: 5.253612995147705\n",
      "epoch 23 batch 67 loss: 5.206305980682373\n",
      "epoch 23 batch 68 loss: 5.27126932144165\n",
      "epoch 23 batch 69 loss: 5.268026828765869\n",
      "epoch 23 batch 70 loss: 5.272531509399414\n",
      "epoch 23 batch 71 loss: 5.256627082824707\n",
      "epoch 23 batch 72 loss: 5.222492694854736\n",
      "epoch 23 batch 73 loss: 5.263060092926025\n",
      "epoch 23 batch 74 loss: 5.258316993713379\n",
      "epoch 23 batch 75 loss: 5.251702308654785\n",
      "epoch 23 batch 76 loss: 5.227504730224609\n",
      "epoch 23 batch 77 loss: 5.243858337402344\n",
      "epoch 23 batch 78 loss: 5.283663272857666\n",
      "epoch 23 batch 79 loss: 5.251491069793701\n",
      "epoch 23 batch 80 loss: 5.2764763832092285\n",
      "epoch 23 batch 81 loss: 5.2425537109375\n",
      "epoch 23 batch 82 loss: 5.272075653076172\n",
      "epoch 23 batch 83 loss: 5.276910781860352\n",
      "epoch 23 batch 84 loss: 5.272944450378418\n",
      "epoch 23 batch 85 loss: 5.233435153961182\n",
      "epoch 23 batch 86 loss: 5.265017986297607\n",
      "epoch 23 batch 87 loss: 5.257415771484375\n",
      "epoch 23 batch 88 loss: 5.227242469787598\n",
      "epoch 23 batch 89 loss: 5.221074104309082\n",
      "epoch 23 batch 90 loss: 5.245438575744629\n",
      "epoch 23 batch 91 loss: 5.253190040588379\n",
      "epoch 23 batch 92 loss: 5.314076900482178\n",
      "epoch 23 batch 93 loss: 5.275844573974609\n",
      "epoch 23 batch 94 loss: 5.275386810302734\n",
      "epoch 23 batch 95 loss: 5.249392509460449\n",
      "epoch 23 batch 96 loss: 5.272028923034668\n",
      "epoch 23 batch 97 loss: 5.242036819458008\n",
      "epoch 23 batch 98 loss: 5.296912670135498\n",
      "epoch 23 batch 99 loss: 5.26862907409668\n",
      "epoch 23 batch 100 loss: 5.26698112487793\n",
      "epoch 23 batch 101 loss: 5.2278289794921875\n",
      "epoch 23 batch 102 loss: 5.25129508972168\n",
      "epoch 23 batch 103 loss: 5.259918212890625\n",
      "epoch 23 batch 104 loss: 5.24380350112915\n",
      "epoch 23 batch 105 loss: 5.267539978027344\n",
      "epoch 23 batch 106 loss: 5.287587642669678\n",
      "epoch 23 batch 107 loss: 5.222975730895996\n",
      "epoch 23 batch 108 loss: 5.25132942199707\n",
      "epoch 23 batch 109 loss: 5.228984832763672\n",
      "epoch 23 batch 110 loss: 5.254331588745117\n",
      "epoch 23 batch 111 loss: 5.249953269958496\n",
      "epoch 23 batch 112 loss: 5.286845684051514\n",
      "epoch 23 batch 113 loss: 5.278986930847168\n",
      "epoch 23 batch 114 loss: 5.268368244171143\n",
      "epoch 23 batch 115 loss: 5.25102424621582\n",
      "epoch 23 batch 116 loss: 5.255029201507568\n",
      "epoch 23 batch 117 loss: 5.24014949798584\n",
      "epoch 23 batch 118 loss: 5.268940448760986\n",
      "epoch 23 batch 119 loss: 5.238316535949707\n",
      "epoch 23 batch 120 loss: 5.264565467834473\n",
      "epoch 23 batch 121 loss: 5.206364631652832\n",
      "epoch 23 batch 122 loss: 5.27421760559082\n",
      "epoch 23 batch 123 loss: 5.289017200469971\n",
      "epoch 23 batch 124 loss: 5.283791542053223\n",
      "epoch loss: 5.259877548217774\n",
      "epoch 24 batch 0 loss: 5.19575309753418\n",
      "epoch 24 batch 1 loss: 5.228472709655762\n",
      "epoch 24 batch 2 loss: 5.277771949768066\n",
      "epoch 24 batch 3 loss: 5.251750946044922\n",
      "epoch 24 batch 4 loss: 5.256980895996094\n",
      "epoch 24 batch 5 loss: 5.295709133148193\n",
      "epoch 24 batch 6 loss: 5.253983020782471\n",
      "epoch 24 batch 7 loss: 5.276987075805664\n",
      "epoch 24 batch 8 loss: 5.2229533195495605\n",
      "epoch 24 batch 9 loss: 5.261782169342041\n",
      "epoch 24 batch 10 loss: 5.268465042114258\n",
      "epoch 24 batch 11 loss: 5.3147172927856445\n",
      "epoch 24 batch 12 loss: 5.230612277984619\n",
      "epoch 24 batch 13 loss: 5.280640602111816\n",
      "epoch 24 batch 14 loss: 5.258734703063965\n",
      "epoch 24 batch 15 loss: 5.286731243133545\n",
      "epoch 24 batch 16 loss: 5.268520832061768\n",
      "epoch 24 batch 17 loss: 5.269227981567383\n",
      "epoch 24 batch 18 loss: 5.263425350189209\n",
      "epoch 24 batch 19 loss: 5.267548561096191\n",
      "epoch 24 batch 20 loss: 5.215487957000732\n",
      "epoch 24 batch 21 loss: 5.2774786949157715\n",
      "epoch 24 batch 22 loss: 5.250797748565674\n",
      "epoch 24 batch 23 loss: 5.247337341308594\n",
      "epoch 24 batch 24 loss: 5.227200031280518\n",
      "epoch 24 batch 25 loss: 5.288491249084473\n",
      "epoch 24 batch 26 loss: 5.250617504119873\n",
      "epoch 24 batch 27 loss: 5.260339736938477\n",
      "epoch 24 batch 28 loss: 5.29926872253418\n",
      "epoch 24 batch 29 loss: 5.279360771179199\n",
      "epoch 24 batch 30 loss: 5.230564594268799\n",
      "epoch 24 batch 31 loss: 5.246150970458984\n",
      "epoch 24 batch 32 loss: 5.258122444152832\n",
      "epoch 24 batch 33 loss: 5.24180793762207\n",
      "epoch 24 batch 34 loss: 5.270960807800293\n",
      "epoch 24 batch 35 loss: 5.267752170562744\n",
      "epoch 24 batch 36 loss: 5.214841365814209\n",
      "epoch 24 batch 37 loss: 5.2243194580078125\n",
      "epoch 24 batch 38 loss: 5.229176998138428\n",
      "epoch 24 batch 39 loss: 5.243967056274414\n",
      "epoch 24 batch 40 loss: 5.252266883850098\n",
      "epoch 24 batch 41 loss: 5.229874610900879\n",
      "epoch 24 batch 42 loss: 5.249789714813232\n",
      "epoch 24 batch 43 loss: 5.247969150543213\n",
      "epoch 24 batch 44 loss: 5.266131401062012\n",
      "epoch 24 batch 45 loss: 5.242769718170166\n",
      "epoch 24 batch 46 loss: 5.259970664978027\n",
      "epoch 24 batch 47 loss: 5.2207841873168945\n",
      "epoch 24 batch 48 loss: 5.282558917999268\n",
      "epoch 24 batch 49 loss: 5.271159648895264\n",
      "epoch 24 batch 50 loss: 5.291798114776611\n",
      "epoch 24 batch 51 loss: 5.2872314453125\n",
      "epoch 24 batch 52 loss: 5.274286270141602\n",
      "epoch 24 batch 53 loss: 5.259681701660156\n",
      "epoch 24 batch 54 loss: 5.238504409790039\n",
      "epoch 24 batch 55 loss: 5.2718119621276855\n",
      "epoch 24 batch 56 loss: 5.2928690910339355\n",
      "epoch 24 batch 57 loss: 5.237998962402344\n",
      "epoch 24 batch 58 loss: 5.316890716552734\n",
      "epoch 24 batch 59 loss: 5.247785568237305\n",
      "epoch 24 batch 60 loss: 5.244809627532959\n",
      "epoch 24 batch 61 loss: 5.25689697265625\n",
      "epoch 24 batch 62 loss: 5.261754035949707\n",
      "epoch 24 batch 63 loss: 5.248626708984375\n",
      "epoch 24 batch 64 loss: 5.282431125640869\n",
      "epoch 24 batch 65 loss: 5.257833957672119\n",
      "epoch 24 batch 66 loss: 5.258306980133057\n",
      "epoch 24 batch 67 loss: 5.252172946929932\n",
      "epoch 24 batch 68 loss: 5.291196346282959\n",
      "epoch 24 batch 69 loss: 5.2784905433654785\n",
      "epoch 24 batch 70 loss: 5.247946262359619\n",
      "epoch 24 batch 71 loss: 5.2500433921813965\n",
      "epoch 24 batch 72 loss: 5.280101776123047\n",
      "epoch 24 batch 73 loss: 5.248858451843262\n",
      "epoch 24 batch 74 loss: 5.279379844665527\n",
      "epoch 24 batch 75 loss: 5.283271312713623\n",
      "epoch 24 batch 76 loss: 5.269932746887207\n",
      "epoch 24 batch 77 loss: 5.279490947723389\n",
      "epoch 24 batch 78 loss: 5.223377704620361\n",
      "epoch 24 batch 79 loss: 5.242767333984375\n",
      "epoch 24 batch 80 loss: 5.304770469665527\n",
      "epoch 24 batch 81 loss: 5.280685901641846\n",
      "epoch 24 batch 82 loss: 5.228249549865723\n",
      "epoch 24 batch 83 loss: 5.275842189788818\n",
      "epoch 24 batch 84 loss: 5.2679643630981445\n",
      "epoch 24 batch 85 loss: 5.257625102996826\n",
      "epoch 24 batch 86 loss: 5.218683242797852\n",
      "epoch 24 batch 87 loss: 5.27347469329834\n",
      "epoch 24 batch 88 loss: 5.237109184265137\n",
      "epoch 24 batch 89 loss: 5.248010158538818\n",
      "epoch 24 batch 90 loss: 5.2634711265563965\n",
      "epoch 24 batch 91 loss: 5.234122276306152\n",
      "epoch 24 batch 92 loss: 5.250458717346191\n",
      "epoch 24 batch 93 loss: 5.250575542449951\n",
      "epoch 24 batch 94 loss: 5.219729900360107\n",
      "epoch 24 batch 95 loss: 5.190662860870361\n",
      "epoch 24 batch 96 loss: 5.237940788269043\n",
      "epoch 24 batch 97 loss: 5.235782146453857\n",
      "epoch 24 batch 98 loss: 5.244653701782227\n",
      "epoch 24 batch 99 loss: 5.28402042388916\n",
      "epoch 24 batch 100 loss: 5.229465007781982\n",
      "epoch 24 batch 101 loss: 5.272839069366455\n",
      "epoch 24 batch 102 loss: 5.222930908203125\n",
      "epoch 24 batch 103 loss: 5.290588855743408\n",
      "epoch 24 batch 104 loss: 5.282596111297607\n",
      "epoch 24 batch 105 loss: 5.294809341430664\n",
      "epoch 24 batch 106 loss: 5.266141414642334\n",
      "epoch 24 batch 107 loss: 5.234972953796387\n",
      "epoch 24 batch 108 loss: 5.218515396118164\n",
      "epoch 24 batch 109 loss: 5.240222454071045\n",
      "epoch 24 batch 110 loss: 5.24118709564209\n",
      "epoch 24 batch 111 loss: 5.2717461585998535\n",
      "epoch 24 batch 112 loss: 5.22561502456665\n",
      "epoch 24 batch 113 loss: 5.306629657745361\n",
      "epoch 24 batch 114 loss: 5.273447036743164\n",
      "epoch 24 batch 115 loss: 5.279646873474121\n",
      "epoch 24 batch 116 loss: 5.258122444152832\n",
      "epoch 24 batch 117 loss: 5.250844478607178\n",
      "epoch 24 batch 118 loss: 5.2993574142456055\n",
      "epoch 24 batch 119 loss: 5.251302242279053\n",
      "epoch 24 batch 120 loss: 5.218532085418701\n",
      "epoch 24 batch 121 loss: 5.234528541564941\n",
      "epoch 24 batch 122 loss: 5.251875877380371\n",
      "epoch 24 batch 123 loss: 5.222880840301514\n",
      "epoch 24 batch 124 loss: 5.267039775848389\n",
      "epoch loss: 5.257154426574707\n",
      "epoch 25 batch 0 loss: 5.2703166007995605\n",
      "epoch 25 batch 1 loss: 5.266484260559082\n",
      "epoch 25 batch 2 loss: 5.246169090270996\n",
      "epoch 25 batch 3 loss: 5.239400386810303\n",
      "epoch 25 batch 4 loss: 5.237843990325928\n",
      "epoch 25 batch 5 loss: 5.298957824707031\n",
      "epoch 25 batch 6 loss: 5.270384788513184\n",
      "epoch 25 batch 7 loss: 5.216105937957764\n",
      "epoch 25 batch 8 loss: 5.287662982940674\n",
      "epoch 25 batch 9 loss: 5.279964447021484\n",
      "epoch 25 batch 10 loss: 5.296139240264893\n",
      "epoch 25 batch 11 loss: 5.275632381439209\n",
      "epoch 25 batch 12 loss: 5.224079608917236\n",
      "epoch 25 batch 13 loss: 5.277958869934082\n",
      "epoch 25 batch 14 loss: 5.2274932861328125\n",
      "epoch 25 batch 15 loss: 5.251125335693359\n",
      "epoch 25 batch 16 loss: 5.2469635009765625\n",
      "epoch 25 batch 17 loss: 5.255430698394775\n",
      "epoch 25 batch 18 loss: 5.255626201629639\n",
      "epoch 25 batch 19 loss: 5.275916576385498\n",
      "epoch 25 batch 20 loss: 5.290759086608887\n",
      "epoch 25 batch 21 loss: 5.257298469543457\n",
      "epoch 25 batch 22 loss: 5.290158748626709\n",
      "epoch 25 batch 23 loss: 5.248145580291748\n",
      "epoch 25 batch 24 loss: 5.236568450927734\n",
      "epoch 25 batch 25 loss: 5.225146770477295\n",
      "epoch 25 batch 26 loss: 5.209547519683838\n",
      "epoch 25 batch 27 loss: 5.2035040855407715\n",
      "epoch 25 batch 28 loss: 5.251733779907227\n",
      "epoch 25 batch 29 loss: 5.24806022644043\n",
      "epoch 25 batch 30 loss: 5.287469863891602\n",
      "epoch 25 batch 31 loss: 5.245895862579346\n",
      "epoch 25 batch 32 loss: 5.20545768737793\n",
      "epoch 25 batch 33 loss: 5.234209060668945\n",
      "epoch 25 batch 34 loss: 5.2086358070373535\n",
      "epoch 25 batch 35 loss: 5.236112594604492\n",
      "epoch 25 batch 36 loss: 5.241921424865723\n",
      "epoch 25 batch 37 loss: 5.204178333282471\n",
      "epoch 25 batch 38 loss: 5.217475891113281\n",
      "epoch 25 batch 39 loss: 5.267216682434082\n",
      "epoch 25 batch 40 loss: 5.268336772918701\n",
      "epoch 25 batch 41 loss: 5.2038421630859375\n",
      "epoch 25 batch 42 loss: 5.255258083343506\n",
      "epoch 25 batch 43 loss: 5.247855186462402\n",
      "epoch 25 batch 44 loss: 5.232489109039307\n",
      "epoch 25 batch 45 loss: 5.263406753540039\n",
      "epoch 25 batch 46 loss: 5.222743988037109\n",
      "epoch 25 batch 47 loss: 5.217324256896973\n",
      "epoch 25 batch 48 loss: 5.2615647315979\n",
      "epoch 25 batch 49 loss: 5.270354270935059\n",
      "epoch 25 batch 50 loss: 5.265758991241455\n",
      "epoch 25 batch 51 loss: 5.26055908203125\n",
      "epoch 25 batch 52 loss: 5.256241321563721\n",
      "epoch 25 batch 53 loss: 5.269660949707031\n",
      "epoch 25 batch 54 loss: 5.250948429107666\n",
      "epoch 25 batch 55 loss: 5.288557529449463\n",
      "epoch 25 batch 56 loss: 5.218048572540283\n",
      "epoch 25 batch 57 loss: 5.248258113861084\n",
      "epoch 25 batch 58 loss: 5.218449592590332\n",
      "epoch 25 batch 59 loss: 5.26380729675293\n",
      "epoch 25 batch 60 loss: 5.2465643882751465\n",
      "epoch 25 batch 61 loss: 5.2072319984436035\n",
      "epoch 25 batch 62 loss: 5.216273307800293\n",
      "epoch 25 batch 63 loss: 5.2364020347595215\n",
      "epoch 25 batch 64 loss: 5.260902404785156\n",
      "epoch 25 batch 65 loss: 5.299375057220459\n",
      "epoch 25 batch 66 loss: 5.267536640167236\n",
      "epoch 25 batch 67 loss: 5.2719244956970215\n",
      "epoch 25 batch 68 loss: 5.275231838226318\n",
      "epoch 25 batch 69 loss: 5.241938591003418\n",
      "epoch 25 batch 70 loss: 5.24530553817749\n",
      "epoch 25 batch 71 loss: 5.236008167266846\n",
      "epoch 25 batch 72 loss: 5.258102893829346\n",
      "epoch 25 batch 73 loss: 5.23713493347168\n",
      "epoch 25 batch 74 loss: 5.255993366241455\n",
      "epoch 25 batch 75 loss: 5.245084285736084\n",
      "epoch 25 batch 76 loss: 5.219557762145996\n",
      "epoch 25 batch 77 loss: 5.236849784851074\n",
      "epoch 25 batch 78 loss: 5.25417947769165\n",
      "epoch 25 batch 79 loss: 5.254631042480469\n",
      "epoch 25 batch 80 loss: 5.243175029754639\n",
      "epoch 25 batch 81 loss: 5.30500602722168\n",
      "epoch 25 batch 82 loss: 5.252053737640381\n",
      "epoch 25 batch 83 loss: 5.254617691040039\n",
      "epoch 25 batch 84 loss: 5.230079174041748\n",
      "epoch 25 batch 85 loss: 5.233189582824707\n",
      "epoch 25 batch 86 loss: 5.278375148773193\n",
      "epoch 25 batch 87 loss: 5.26667594909668\n",
      "epoch 25 batch 88 loss: 5.22175407409668\n",
      "epoch 25 batch 89 loss: 5.236040115356445\n",
      "epoch 25 batch 90 loss: 5.247373104095459\n",
      "epoch 25 batch 91 loss: 5.226449012756348\n",
      "epoch 25 batch 92 loss: 5.205611228942871\n",
      "epoch 25 batch 93 loss: 5.275589466094971\n",
      "epoch 25 batch 94 loss: 5.21338415145874\n",
      "epoch 25 batch 95 loss: 5.3033127784729\n",
      "epoch 25 batch 96 loss: 5.221739768981934\n",
      "epoch 25 batch 97 loss: 5.216280460357666\n",
      "epoch 25 batch 98 loss: 5.286311626434326\n",
      "epoch 25 batch 99 loss: 5.25727653503418\n",
      "epoch 25 batch 100 loss: 5.269100666046143\n",
      "epoch 25 batch 101 loss: 5.240668773651123\n",
      "epoch 25 batch 102 loss: 5.268426418304443\n",
      "epoch 25 batch 103 loss: 5.229345798492432\n",
      "epoch 25 batch 104 loss: 5.229998588562012\n",
      "epoch 25 batch 105 loss: 5.245967864990234\n",
      "epoch 25 batch 106 loss: 5.214128494262695\n",
      "epoch 25 batch 107 loss: 5.228177547454834\n",
      "epoch 25 batch 108 loss: 5.232260227203369\n",
      "epoch 25 batch 109 loss: 5.2697954177856445\n",
      "epoch 25 batch 110 loss: 5.227847099304199\n",
      "epoch 25 batch 111 loss: 5.200232028961182\n",
      "epoch 25 batch 112 loss: 5.217056751251221\n",
      "epoch 25 batch 113 loss: 5.228513240814209\n",
      "epoch 25 batch 114 loss: 5.21235990524292\n",
      "epoch 25 batch 115 loss: 5.26887321472168\n",
      "epoch 25 batch 116 loss: 5.242717266082764\n",
      "epoch 25 batch 117 loss: 5.248054504394531\n",
      "epoch 25 batch 118 loss: 5.259502410888672\n",
      "epoch 25 batch 119 loss: 5.255325794219971\n",
      "epoch 25 batch 120 loss: 5.233588218688965\n",
      "epoch 25 batch 121 loss: 5.265833854675293\n",
      "epoch 25 batch 122 loss: 5.274189472198486\n",
      "epoch 25 batch 123 loss: 5.296720027923584\n",
      "epoch 25 batch 124 loss: 5.251738548278809\n",
      "epoch loss: 5.248348495483398\n",
      "epoch 26 batch 0 loss: 5.237757682800293\n",
      "epoch 26 batch 1 loss: 5.245749473571777\n",
      "epoch 26 batch 2 loss: 5.266861438751221\n",
      "epoch 26 batch 3 loss: 5.27336311340332\n",
      "epoch 26 batch 4 loss: 5.227190971374512\n",
      "epoch 26 batch 5 loss: 5.245589256286621\n",
      "epoch 26 batch 6 loss: 5.231114387512207\n",
      "epoch 26 batch 7 loss: 5.231844425201416\n",
      "epoch 26 batch 8 loss: 5.256923675537109\n",
      "epoch 26 batch 9 loss: 5.225800514221191\n",
      "epoch 26 batch 10 loss: 5.222769260406494\n",
      "epoch 26 batch 11 loss: 5.243153095245361\n",
      "epoch 26 batch 12 loss: 5.207391262054443\n",
      "epoch 26 batch 13 loss: 5.243948936462402\n",
      "epoch 26 batch 14 loss: 5.248435020446777\n",
      "epoch 26 batch 15 loss: 5.233453273773193\n",
      "epoch 26 batch 16 loss: 5.2411041259765625\n",
      "epoch 26 batch 17 loss: 5.242558479309082\n",
      "epoch 26 batch 18 loss: 5.246493816375732\n",
      "epoch 26 batch 19 loss: 5.22037410736084\n",
      "epoch 26 batch 20 loss: 5.245510578155518\n",
      "epoch 26 batch 21 loss: 5.193325042724609\n",
      "epoch 26 batch 22 loss: 5.253050327301025\n",
      "epoch 26 batch 23 loss: 5.252048015594482\n",
      "epoch 26 batch 24 loss: 5.266317844390869\n",
      "epoch 26 batch 25 loss: 5.26658821105957\n",
      "epoch 26 batch 26 loss: 5.234029293060303\n",
      "epoch 26 batch 27 loss: 5.272360324859619\n",
      "epoch 26 batch 28 loss: 5.244851112365723\n",
      "epoch 26 batch 29 loss: 5.276461124420166\n",
      "epoch 26 batch 30 loss: 5.243574619293213\n",
      "epoch 26 batch 31 loss: 5.230475425720215\n",
      "epoch 26 batch 32 loss: 5.215460300445557\n",
      "epoch 26 batch 33 loss: 5.228791236877441\n",
      "epoch 26 batch 34 loss: 5.243776798248291\n",
      "epoch 26 batch 35 loss: 5.2705817222595215\n",
      "epoch 26 batch 36 loss: 5.280284404754639\n",
      "epoch 26 batch 37 loss: 5.238525390625\n",
      "epoch 26 batch 38 loss: 5.31385612487793\n",
      "epoch 26 batch 39 loss: 5.257081985473633\n",
      "epoch 26 batch 40 loss: 5.245732307434082\n",
      "epoch 26 batch 41 loss: 5.226153373718262\n",
      "epoch 26 batch 42 loss: 5.253375053405762\n",
      "epoch 26 batch 43 loss: 5.231053352355957\n",
      "epoch 26 batch 44 loss: 5.238224983215332\n",
      "epoch 26 batch 45 loss: 5.272702693939209\n",
      "epoch 26 batch 46 loss: 5.250977516174316\n",
      "epoch 26 batch 47 loss: 5.260134220123291\n",
      "epoch 26 batch 48 loss: 5.277181148529053\n",
      "epoch 26 batch 49 loss: 5.181911468505859\n",
      "epoch 26 batch 50 loss: 5.258228778839111\n",
      "epoch 26 batch 51 loss: 5.245706081390381\n",
      "epoch 26 batch 52 loss: 5.269850730895996\n",
      "epoch 26 batch 53 loss: 5.245507717132568\n",
      "epoch 26 batch 54 loss: 5.233282566070557\n",
      "epoch 26 batch 55 loss: 5.251994609832764\n",
      "epoch 26 batch 56 loss: 5.274697303771973\n",
      "epoch 26 batch 57 loss: 5.226384162902832\n",
      "epoch 26 batch 58 loss: 5.250746250152588\n",
      "epoch 26 batch 59 loss: 5.210419654846191\n",
      "epoch 26 batch 60 loss: 5.239587306976318\n",
      "epoch 26 batch 61 loss: 5.2676191329956055\n",
      "epoch 26 batch 62 loss: 5.250239849090576\n",
      "epoch 26 batch 63 loss: 5.267897605895996\n",
      "epoch 26 batch 64 loss: 5.225154876708984\n",
      "epoch 26 batch 65 loss: 5.2384161949157715\n",
      "epoch 26 batch 66 loss: 5.24980354309082\n",
      "epoch 26 batch 67 loss: 5.220142841339111\n",
      "epoch 26 batch 68 loss: 5.266900539398193\n",
      "epoch 26 batch 69 loss: 5.237685203552246\n",
      "epoch 26 batch 70 loss: 5.2284064292907715\n",
      "epoch 26 batch 71 loss: 5.194113254547119\n",
      "epoch 26 batch 72 loss: 5.260215759277344\n",
      "epoch 26 batch 73 loss: 5.249412536621094\n",
      "epoch 26 batch 74 loss: 5.243947505950928\n",
      "epoch 26 batch 75 loss: 5.261938571929932\n",
      "epoch 26 batch 76 loss: 5.239205837249756\n",
      "epoch 26 batch 77 loss: 5.264252662658691\n",
      "epoch 26 batch 78 loss: 5.245027542114258\n",
      "epoch 26 batch 79 loss: 5.24357795715332\n",
      "epoch 26 batch 80 loss: 5.228832244873047\n",
      "epoch 26 batch 81 loss: 5.248050212860107\n",
      "epoch 26 batch 82 loss: 5.24605655670166\n",
      "epoch 26 batch 83 loss: 5.244251728057861\n",
      "epoch 26 batch 84 loss: 5.215511322021484\n",
      "epoch 26 batch 85 loss: 5.254997253417969\n",
      "epoch 26 batch 86 loss: 5.217000484466553\n",
      "epoch 26 batch 87 loss: 5.276403903961182\n",
      "epoch 26 batch 88 loss: 5.242192268371582\n",
      "epoch 26 batch 89 loss: 5.240370273590088\n",
      "epoch 26 batch 90 loss: 5.243309020996094\n",
      "epoch 26 batch 91 loss: 5.2282395362854\n",
      "epoch 26 batch 92 loss: 5.2013092041015625\n",
      "epoch 26 batch 93 loss: 5.224226951599121\n",
      "epoch 26 batch 94 loss: 5.233068943023682\n",
      "epoch 26 batch 95 loss: 5.244542121887207\n",
      "epoch 26 batch 96 loss: 5.233501434326172\n",
      "epoch 26 batch 97 loss: 5.260079383850098\n",
      "epoch 26 batch 98 loss: 5.252799034118652\n",
      "epoch 26 batch 99 loss: 5.243081092834473\n",
      "epoch 26 batch 100 loss: 5.243427753448486\n",
      "epoch 26 batch 101 loss: 5.296833515167236\n",
      "epoch 26 batch 102 loss: 5.232515811920166\n",
      "epoch 26 batch 103 loss: 5.230281829833984\n",
      "epoch 26 batch 104 loss: 5.231834411621094\n",
      "epoch 26 batch 105 loss: 5.251502990722656\n",
      "epoch 26 batch 106 loss: 5.230110168457031\n",
      "epoch 26 batch 107 loss: 5.250839710235596\n",
      "epoch 26 batch 108 loss: 5.282094478607178\n",
      "epoch 26 batch 109 loss: 5.258645534515381\n",
      "epoch 26 batch 110 loss: 5.238461017608643\n",
      "epoch 26 batch 111 loss: 5.274550437927246\n",
      "epoch 26 batch 112 loss: 5.227297306060791\n",
      "epoch 26 batch 113 loss: 5.259022235870361\n",
      "epoch 26 batch 114 loss: 5.258728504180908\n",
      "epoch 26 batch 115 loss: 5.255835056304932\n",
      "epoch 26 batch 116 loss: 5.244185447692871\n",
      "epoch 26 batch 117 loss: 5.231864929199219\n",
      "epoch 26 batch 118 loss: 5.281917572021484\n",
      "epoch 26 batch 119 loss: 5.250364780426025\n",
      "epoch 26 batch 120 loss: 5.239930152893066\n",
      "epoch 26 batch 121 loss: 5.277228832244873\n",
      "epoch 26 batch 122 loss: 5.2777910232543945\n",
      "epoch 26 batch 123 loss: 5.280609130859375\n",
      "epoch 26 batch 124 loss: 5.220391750335693\n",
      "epoch loss: 5.2457657814025875\n",
      "epoch 27 batch 0 loss: 5.219090461730957\n",
      "epoch 27 batch 1 loss: 5.246854782104492\n",
      "epoch 27 batch 2 loss: 5.227076530456543\n",
      "epoch 27 batch 3 loss: 5.247216701507568\n",
      "epoch 27 batch 4 loss: 5.225680351257324\n",
      "epoch 27 batch 5 loss: 5.2467780113220215\n",
      "epoch 27 batch 6 loss: 5.272764682769775\n",
      "epoch 27 batch 7 loss: 5.254017353057861\n",
      "epoch 27 batch 8 loss: 5.238577842712402\n",
      "epoch 27 batch 9 loss: 5.227872371673584\n",
      "epoch 27 batch 10 loss: 5.257918357849121\n",
      "epoch 27 batch 11 loss: 5.248085975646973\n",
      "epoch 27 batch 12 loss: 5.233249187469482\n",
      "epoch 27 batch 13 loss: 5.251784324645996\n",
      "epoch 27 batch 14 loss: 5.275990009307861\n",
      "epoch 27 batch 15 loss: 5.229820251464844\n",
      "epoch 27 batch 16 loss: 5.253852367401123\n",
      "epoch 27 batch 17 loss: 5.311142444610596\n",
      "epoch 27 batch 18 loss: 5.258793354034424\n",
      "epoch 27 batch 19 loss: 5.281425476074219\n",
      "epoch 27 batch 20 loss: 5.2649993896484375\n",
      "epoch 27 batch 21 loss: 5.244039058685303\n",
      "epoch 27 batch 22 loss: 5.236689567565918\n",
      "epoch 27 batch 23 loss: 5.232615947723389\n",
      "epoch 27 batch 24 loss: 5.238276481628418\n",
      "epoch 27 batch 25 loss: 5.20652437210083\n",
      "epoch 27 batch 26 loss: 5.259758949279785\n",
      "epoch 27 batch 27 loss: 5.282506465911865\n",
      "epoch 27 batch 28 loss: 5.259452819824219\n",
      "epoch 27 batch 29 loss: 5.232542514801025\n",
      "epoch 27 batch 30 loss: 5.230961322784424\n",
      "epoch 27 batch 31 loss: 5.224594593048096\n",
      "epoch 27 batch 32 loss: 5.2563347816467285\n",
      "epoch 27 batch 33 loss: 5.252116680145264\n",
      "epoch 27 batch 34 loss: 5.24494743347168\n",
      "epoch 27 batch 35 loss: 5.2507524490356445\n",
      "epoch 27 batch 36 loss: 5.246091365814209\n",
      "epoch 27 batch 37 loss: 5.212283134460449\n",
      "epoch 27 batch 38 loss: 5.234989643096924\n",
      "epoch 27 batch 39 loss: 5.231513977050781\n",
      "epoch 27 batch 40 loss: 5.302119255065918\n",
      "epoch 27 batch 41 loss: 5.242977619171143\n",
      "epoch 27 batch 42 loss: 5.209179401397705\n",
      "epoch 27 batch 43 loss: 5.271779537200928\n",
      "epoch 27 batch 44 loss: 5.229939937591553\n",
      "epoch 27 batch 45 loss: 5.26304292678833\n",
      "epoch 27 batch 46 loss: 5.267962455749512\n",
      "epoch 27 batch 47 loss: 5.236919403076172\n",
      "epoch 27 batch 48 loss: 5.232048034667969\n",
      "epoch 27 batch 49 loss: 5.265381813049316\n",
      "epoch 27 batch 50 loss: 5.225833415985107\n",
      "epoch 27 batch 51 loss: 5.226985454559326\n",
      "epoch 27 batch 52 loss: 5.205618858337402\n",
      "epoch 27 batch 53 loss: 5.2410888671875\n",
      "epoch 27 batch 54 loss: 5.244718074798584\n",
      "epoch 27 batch 55 loss: 5.221177101135254\n",
      "epoch 27 batch 56 loss: 5.216939449310303\n",
      "epoch 27 batch 57 loss: 5.251863956451416\n",
      "epoch 27 batch 58 loss: 5.252893447875977\n",
      "epoch 27 batch 59 loss: 5.248030662536621\n",
      "epoch 27 batch 60 loss: 5.239990711212158\n",
      "epoch 27 batch 61 loss: 5.243278503417969\n",
      "epoch 27 batch 62 loss: 5.239200115203857\n",
      "epoch 27 batch 63 loss: 5.246840476989746\n",
      "epoch 27 batch 64 loss: 5.208563327789307\n",
      "epoch 27 batch 65 loss: 5.245940208435059\n",
      "epoch 27 batch 66 loss: 5.233705997467041\n",
      "epoch 27 batch 67 loss: 5.218318939208984\n",
      "epoch 27 batch 68 loss: 5.261422634124756\n",
      "epoch 27 batch 69 loss: 5.229665279388428\n",
      "epoch 27 batch 70 loss: 5.236033916473389\n",
      "epoch 27 batch 71 loss: 5.223287105560303\n",
      "epoch 27 batch 72 loss: 5.2269287109375\n",
      "epoch 27 batch 73 loss: 5.233515739440918\n",
      "epoch 27 batch 74 loss: 5.210669994354248\n",
      "epoch 27 batch 75 loss: 5.235292911529541\n",
      "epoch 27 batch 76 loss: 5.234790325164795\n",
      "epoch 27 batch 77 loss: 5.23919677734375\n",
      "epoch 27 batch 78 loss: 5.253538608551025\n",
      "epoch 27 batch 79 loss: 5.236396312713623\n",
      "epoch 27 batch 80 loss: 5.224330425262451\n",
      "epoch 27 batch 81 loss: 5.307754993438721\n",
      "epoch 27 batch 82 loss: 5.207836151123047\n",
      "epoch 27 batch 83 loss: 5.208006858825684\n",
      "epoch 27 batch 84 loss: 5.226438522338867\n",
      "epoch 27 batch 85 loss: 5.251557350158691\n",
      "epoch 27 batch 86 loss: 5.278420448303223\n",
      "epoch 27 batch 87 loss: 5.251290321350098\n",
      "epoch 27 batch 88 loss: 5.261682510375977\n",
      "epoch 27 batch 89 loss: 5.224075794219971\n",
      "epoch 27 batch 90 loss: 5.214561462402344\n",
      "epoch 27 batch 91 loss: 5.237124919891357\n",
      "epoch 27 batch 92 loss: 5.259347915649414\n",
      "epoch 27 batch 93 loss: 5.257747173309326\n",
      "epoch 27 batch 94 loss: 5.242506980895996\n",
      "epoch 27 batch 95 loss: 5.263646125793457\n",
      "epoch 27 batch 96 loss: 5.274166107177734\n",
      "epoch 27 batch 97 loss: 5.198594093322754\n",
      "epoch 27 batch 98 loss: 5.250516891479492\n",
      "epoch 27 batch 99 loss: 5.243745803833008\n",
      "epoch 27 batch 100 loss: 5.229439735412598\n",
      "epoch 27 batch 101 loss: 5.244316101074219\n",
      "epoch 27 batch 102 loss: 5.2203850746154785\n",
      "epoch 27 batch 103 loss: 5.260707855224609\n",
      "epoch 27 batch 104 loss: 5.239893913269043\n",
      "epoch 27 batch 105 loss: 5.205502986907959\n",
      "epoch 27 batch 106 loss: 5.239275932312012\n",
      "epoch 27 batch 107 loss: 5.259024620056152\n",
      "epoch 27 batch 108 loss: 5.237681865692139\n",
      "epoch 27 batch 109 loss: 5.2504191398620605\n",
      "epoch 27 batch 110 loss: 5.262423515319824\n",
      "epoch 27 batch 111 loss: 5.2056565284729\n",
      "epoch 27 batch 112 loss: 5.255202293395996\n",
      "epoch 27 batch 113 loss: 5.239084243774414\n",
      "epoch 27 batch 114 loss: 5.253780841827393\n",
      "epoch 27 batch 115 loss: 5.21250057220459\n",
      "epoch 27 batch 116 loss: 5.282655715942383\n",
      "epoch 27 batch 117 loss: 5.248048782348633\n",
      "epoch 27 batch 118 loss: 5.219460487365723\n",
      "epoch 27 batch 119 loss: 5.214751243591309\n",
      "epoch 27 batch 120 loss: 5.220889568328857\n",
      "epoch 27 batch 121 loss: 5.235306739807129\n",
      "epoch 27 batch 122 loss: 5.250813007354736\n",
      "epoch 27 batch 123 loss: 5.223148822784424\n",
      "epoch 27 batch 124 loss: 5.245773792266846\n",
      "epoch loss: 5.241940418243408\n",
      "epoch 28 batch 0 loss: 5.297277927398682\n",
      "epoch 28 batch 1 loss: 5.232757568359375\n",
      "epoch 28 batch 2 loss: 5.266334056854248\n",
      "epoch 28 batch 3 loss: 5.28608512878418\n",
      "epoch 28 batch 4 loss: 5.250604152679443\n",
      "epoch 28 batch 5 loss: 5.2413330078125\n",
      "epoch 28 batch 6 loss: 5.224180698394775\n",
      "epoch 28 batch 7 loss: 5.2196784019470215\n",
      "epoch 28 batch 8 loss: 5.240848064422607\n",
      "epoch 28 batch 9 loss: 5.24626350402832\n",
      "epoch 28 batch 10 loss: 5.277104377746582\n",
      "epoch 28 batch 11 loss: 5.237740993499756\n",
      "epoch 28 batch 12 loss: 5.264524936676025\n",
      "epoch 28 batch 13 loss: 5.267770767211914\n",
      "epoch 28 batch 14 loss: 5.218916893005371\n",
      "epoch 28 batch 15 loss: 5.271892070770264\n",
      "epoch 28 batch 16 loss: 5.289278507232666\n",
      "epoch 28 batch 17 loss: 5.249229431152344\n",
      "epoch 28 batch 18 loss: 5.275913715362549\n",
      "epoch 28 batch 19 loss: 5.246124267578125\n",
      "epoch 28 batch 20 loss: 5.219201564788818\n",
      "epoch 28 batch 21 loss: 5.208914279937744\n",
      "epoch 28 batch 22 loss: 5.255622863769531\n",
      "epoch 28 batch 23 loss: 5.2390313148498535\n",
      "epoch 28 batch 24 loss: 5.241279125213623\n",
      "epoch 28 batch 25 loss: 5.246389865875244\n",
      "epoch 28 batch 26 loss: 5.222296714782715\n",
      "epoch 28 batch 27 loss: 5.2871246337890625\n",
      "epoch 28 batch 28 loss: 5.262783050537109\n",
      "epoch 28 batch 29 loss: 5.267726898193359\n",
      "epoch 28 batch 30 loss: 5.2772626876831055\n",
      "epoch 28 batch 31 loss: 5.240498065948486\n",
      "epoch 28 batch 32 loss: 5.243793964385986\n",
      "epoch 28 batch 33 loss: 5.26139497756958\n",
      "epoch 28 batch 34 loss: 5.2408881187438965\n",
      "epoch 28 batch 35 loss: 5.231280326843262\n",
      "epoch 28 batch 36 loss: 5.263185977935791\n",
      "epoch 28 batch 37 loss: 5.247800350189209\n",
      "epoch 28 batch 38 loss: 5.263886451721191\n",
      "epoch 28 batch 39 loss: 5.260988712310791\n",
      "epoch 28 batch 40 loss: 5.249248027801514\n",
      "epoch 28 batch 41 loss: 5.23116397857666\n",
      "epoch 28 batch 42 loss: 5.23457670211792\n",
      "epoch 28 batch 43 loss: 5.221697807312012\n",
      "epoch 28 batch 44 loss: 5.233246803283691\n",
      "epoch 28 batch 45 loss: 5.2252702713012695\n",
      "epoch 28 batch 46 loss: 5.244937419891357\n",
      "epoch 28 batch 47 loss: 5.245590686798096\n",
      "epoch 28 batch 48 loss: 5.278299331665039\n",
      "epoch 28 batch 49 loss: 5.242159843444824\n",
      "epoch 28 batch 50 loss: 5.251591682434082\n",
      "epoch 28 batch 51 loss: 5.230015754699707\n",
      "epoch 28 batch 52 loss: 5.251269340515137\n",
      "epoch 28 batch 53 loss: 5.214076042175293\n",
      "epoch 28 batch 54 loss: 5.253669738769531\n",
      "epoch 28 batch 55 loss: 5.245266437530518\n",
      "epoch 28 batch 56 loss: 5.267307758331299\n",
      "epoch 28 batch 57 loss: 5.238947868347168\n",
      "epoch 28 batch 58 loss: 5.215543746948242\n",
      "epoch 28 batch 59 loss: 5.2524261474609375\n",
      "epoch 28 batch 60 loss: 5.22180700302124\n",
      "epoch 28 batch 61 loss: 5.275022983551025\n",
      "epoch 28 batch 62 loss: 5.27729606628418\n",
      "epoch 28 batch 63 loss: 5.2405548095703125\n",
      "epoch 28 batch 64 loss: 5.244969367980957\n",
      "epoch 28 batch 65 loss: 5.2213134765625\n",
      "epoch 28 batch 66 loss: 5.245128154754639\n",
      "epoch 28 batch 67 loss: 5.250903129577637\n",
      "epoch 28 batch 68 loss: 5.266952991485596\n",
      "epoch 28 batch 69 loss: 5.251993179321289\n",
      "epoch 28 batch 70 loss: 5.249180316925049\n",
      "epoch 28 batch 71 loss: 5.247427940368652\n",
      "epoch 28 batch 72 loss: 5.245100021362305\n",
      "epoch 28 batch 73 loss: 5.244101524353027\n",
      "epoch 28 batch 74 loss: 5.244659423828125\n",
      "epoch 28 batch 75 loss: 5.243269920349121\n",
      "epoch 28 batch 76 loss: 5.239022254943848\n",
      "epoch 28 batch 77 loss: 5.223114013671875\n",
      "epoch 28 batch 78 loss: 5.211575984954834\n",
      "epoch 28 batch 79 loss: 5.237490653991699\n",
      "epoch 28 batch 80 loss: 5.262460231781006\n",
      "epoch 28 batch 81 loss: 5.221841335296631\n",
      "epoch 28 batch 82 loss: 5.251692295074463\n",
      "epoch 28 batch 83 loss: 5.232155799865723\n",
      "epoch 28 batch 84 loss: 5.253631591796875\n",
      "epoch 28 batch 85 loss: 5.2260541915893555\n",
      "epoch 28 batch 86 loss: 5.218266487121582\n",
      "epoch 28 batch 87 loss: 5.240041255950928\n",
      "epoch 28 batch 88 loss: 5.257960796356201\n",
      "epoch 28 batch 89 loss: 5.28422737121582\n",
      "epoch 28 batch 90 loss: 5.198231220245361\n",
      "epoch 28 batch 91 loss: 5.216519832611084\n",
      "epoch 28 batch 92 loss: 5.2540764808654785\n",
      "epoch 28 batch 93 loss: 5.258969783782959\n",
      "epoch 28 batch 94 loss: 5.238342761993408\n",
      "epoch 28 batch 95 loss: 5.2611870765686035\n",
      "epoch 28 batch 96 loss: 5.2566938400268555\n",
      "epoch 28 batch 97 loss: 5.272275924682617\n",
      "epoch 28 batch 98 loss: 5.2706451416015625\n",
      "epoch 28 batch 99 loss: 5.224039077758789\n",
      "epoch 28 batch 100 loss: 5.258913993835449\n",
      "epoch 28 batch 101 loss: 5.266211986541748\n",
      "epoch 28 batch 102 loss: 5.282225131988525\n",
      "epoch 28 batch 103 loss: 5.233339309692383\n",
      "epoch 28 batch 104 loss: 5.299614906311035\n",
      "epoch 28 batch 105 loss: 5.228505611419678\n",
      "epoch 28 batch 106 loss: 5.2563300132751465\n",
      "epoch 28 batch 107 loss: 5.24709939956665\n",
      "epoch 28 batch 108 loss: 5.254450798034668\n",
      "epoch 28 batch 109 loss: 5.260236740112305\n",
      "epoch 28 batch 110 loss: 5.24658203125\n",
      "epoch 28 batch 111 loss: 5.232355117797852\n",
      "epoch 28 batch 112 loss: 5.242665767669678\n",
      "epoch 28 batch 113 loss: 5.250885009765625\n",
      "epoch 28 batch 114 loss: 5.213671684265137\n",
      "epoch 28 batch 115 loss: 5.220953941345215\n",
      "epoch 28 batch 116 loss: 5.228575706481934\n",
      "epoch 28 batch 117 loss: 5.247277736663818\n",
      "epoch 28 batch 118 loss: 5.227531433105469\n",
      "epoch 28 batch 119 loss: 5.302761077880859\n",
      "epoch 28 batch 120 loss: 5.234511375427246\n",
      "epoch 28 batch 121 loss: 5.211256980895996\n",
      "epoch 28 batch 122 loss: 5.208034992218018\n",
      "epoch 28 batch 123 loss: 5.2929840087890625\n",
      "epoch 28 batch 124 loss: 5.2273125648498535\n",
      "epoch loss: 5.246927974700927\n",
      "epoch 29 batch 0 loss: 5.243842124938965\n",
      "epoch 29 batch 1 loss: 5.267176628112793\n",
      "epoch 29 batch 2 loss: 5.249815464019775\n",
      "epoch 29 batch 3 loss: 5.235016345977783\n",
      "epoch 29 batch 4 loss: 5.234147071838379\n",
      "epoch 29 batch 5 loss: 5.224034309387207\n",
      "epoch 29 batch 6 loss: 5.2136454582214355\n",
      "epoch 29 batch 7 loss: 5.254242897033691\n",
      "epoch 29 batch 8 loss: 5.232572078704834\n",
      "epoch 29 batch 9 loss: 5.253047943115234\n",
      "epoch 29 batch 10 loss: 5.245413780212402\n",
      "epoch 29 batch 11 loss: 5.244081974029541\n",
      "epoch 29 batch 12 loss: 5.222590923309326\n",
      "epoch 29 batch 13 loss: 5.214669227600098\n",
      "epoch 29 batch 14 loss: 5.264396667480469\n",
      "epoch 29 batch 15 loss: 5.236907958984375\n",
      "epoch 29 batch 16 loss: 5.227842807769775\n",
      "epoch 29 batch 17 loss: 5.22575569152832\n",
      "epoch 29 batch 18 loss: 5.243855953216553\n",
      "epoch 29 batch 19 loss: 5.244808197021484\n",
      "epoch 29 batch 20 loss: 5.223776817321777\n",
      "epoch 29 batch 21 loss: 5.224820613861084\n",
      "epoch 29 batch 22 loss: 5.252558708190918\n",
      "epoch 29 batch 23 loss: 5.300018310546875\n",
      "epoch 29 batch 24 loss: 5.2169189453125\n",
      "epoch 29 batch 25 loss: 5.240236759185791\n",
      "epoch 29 batch 26 loss: 5.2487711906433105\n",
      "epoch 29 batch 27 loss: 5.223206043243408\n",
      "epoch 29 batch 28 loss: 5.246777057647705\n",
      "epoch 29 batch 29 loss: 5.229819297790527\n",
      "epoch 29 batch 30 loss: 5.254035949707031\n",
      "epoch 29 batch 31 loss: 5.252740383148193\n",
      "epoch 29 batch 32 loss: 5.199635028839111\n",
      "epoch 29 batch 33 loss: 5.242412090301514\n",
      "epoch 29 batch 34 loss: 5.258580207824707\n",
      "epoch 29 batch 35 loss: 5.202737808227539\n",
      "epoch 29 batch 36 loss: 5.261198043823242\n",
      "epoch 29 batch 37 loss: 5.244112014770508\n",
      "epoch 29 batch 38 loss: 5.26231575012207\n",
      "epoch 29 batch 39 loss: 5.21788215637207\n",
      "epoch 29 batch 40 loss: 5.2210917472839355\n",
      "epoch 29 batch 41 loss: 5.209316730499268\n",
      "epoch 29 batch 42 loss: 5.23423433303833\n",
      "epoch 29 batch 43 loss: 5.184451103210449\n",
      "epoch 29 batch 44 loss: 5.247424602508545\n",
      "epoch 29 batch 45 loss: 5.241915702819824\n",
      "epoch 29 batch 46 loss: 5.249963760375977\n",
      "epoch 29 batch 47 loss: 5.26918888092041\n",
      "epoch 29 batch 48 loss: 5.237496852874756\n",
      "epoch 29 batch 49 loss: 5.210318565368652\n",
      "epoch 29 batch 50 loss: 5.228571891784668\n",
      "epoch 29 batch 51 loss: 5.241957664489746\n",
      "epoch 29 batch 52 loss: 5.27341890335083\n",
      "epoch 29 batch 53 loss: 5.233260154724121\n",
      "epoch 29 batch 54 loss: 5.256352424621582\n",
      "epoch 29 batch 55 loss: 5.223349094390869\n",
      "epoch 29 batch 56 loss: 5.222333908081055\n",
      "epoch 29 batch 57 loss: 5.240853786468506\n",
      "epoch 29 batch 58 loss: 5.242249488830566\n",
      "epoch 29 batch 59 loss: 5.213655948638916\n",
      "epoch 29 batch 60 loss: 5.257889747619629\n",
      "epoch 29 batch 61 loss: 5.205575466156006\n",
      "epoch 29 batch 62 loss: 5.242982864379883\n",
      "epoch 29 batch 63 loss: 5.269874572753906\n",
      "epoch 29 batch 64 loss: 5.241898059844971\n",
      "epoch 29 batch 65 loss: 5.215157508850098\n",
      "epoch 29 batch 66 loss: 5.269457817077637\n",
      "epoch 29 batch 67 loss: 5.252099514007568\n",
      "epoch 29 batch 68 loss: 5.236781120300293\n",
      "epoch 29 batch 69 loss: 5.234824180603027\n",
      "epoch 29 batch 70 loss: 5.23903751373291\n",
      "epoch 29 batch 71 loss: 5.2706804275512695\n",
      "epoch 29 batch 72 loss: 5.228651523590088\n",
      "epoch 29 batch 73 loss: 5.217718601226807\n",
      "epoch 29 batch 74 loss: 5.221548557281494\n",
      "epoch 29 batch 75 loss: 5.207883834838867\n",
      "epoch 29 batch 76 loss: 5.256596565246582\n",
      "epoch 29 batch 77 loss: 5.2439117431640625\n",
      "epoch 29 batch 78 loss: 5.221899509429932\n",
      "epoch 29 batch 79 loss: 5.214001178741455\n",
      "epoch 29 batch 80 loss: 5.249612808227539\n",
      "epoch 29 batch 81 loss: 5.27554178237915\n",
      "epoch 29 batch 82 loss: 5.242131233215332\n",
      "epoch 29 batch 83 loss: 5.244526386260986\n",
      "epoch 29 batch 84 loss: 5.25157356262207\n",
      "epoch 29 batch 85 loss: 5.21353006362915\n",
      "epoch 29 batch 86 loss: 5.244546413421631\n",
      "epoch 29 batch 87 loss: 5.246304988861084\n",
      "epoch 29 batch 88 loss: 5.257271766662598\n",
      "epoch 29 batch 89 loss: 5.246939182281494\n",
      "epoch 29 batch 90 loss: 5.249629020690918\n",
      "epoch 29 batch 91 loss: 5.249917030334473\n",
      "epoch 29 batch 92 loss: 5.227424144744873\n",
      "epoch 29 batch 93 loss: 5.247208118438721\n",
      "epoch 29 batch 94 loss: 5.234146595001221\n",
      "epoch 29 batch 95 loss: 5.216938495635986\n",
      "epoch 29 batch 96 loss: 5.255590915679932\n",
      "epoch 29 batch 97 loss: 5.210981369018555\n",
      "epoch 29 batch 98 loss: 5.224246501922607\n",
      "epoch 29 batch 99 loss: 5.258419036865234\n",
      "epoch 29 batch 100 loss: 5.208497047424316\n",
      "epoch 29 batch 101 loss: 5.258998870849609\n",
      "epoch 29 batch 102 loss: 5.236502647399902\n",
      "epoch 29 batch 103 loss: 5.239416599273682\n",
      "epoch 29 batch 104 loss: 5.22429084777832\n",
      "epoch 29 batch 105 loss: 5.2658281326293945\n",
      "epoch 29 batch 106 loss: 5.2256574630737305\n",
      "epoch 29 batch 107 loss: 5.169004917144775\n",
      "epoch 29 batch 108 loss: 5.249344825744629\n",
      "epoch 29 batch 109 loss: 5.2425127029418945\n",
      "epoch 29 batch 110 loss: 5.259280681610107\n",
      "epoch 29 batch 111 loss: 5.273990154266357\n",
      "epoch 29 batch 112 loss: 5.263579368591309\n",
      "epoch 29 batch 113 loss: 5.230000972747803\n",
      "epoch 29 batch 114 loss: 5.250321388244629\n",
      "epoch 29 batch 115 loss: 5.213167190551758\n",
      "epoch 29 batch 116 loss: 5.205128192901611\n",
      "epoch 29 batch 117 loss: 5.286265850067139\n",
      "epoch 29 batch 118 loss: 5.220797061920166\n",
      "epoch 29 batch 119 loss: 5.2504377365112305\n",
      "epoch 29 batch 120 loss: 5.254277229309082\n",
      "epoch 29 batch 121 loss: 5.253213882446289\n",
      "epoch 29 batch 122 loss: 5.248574256896973\n",
      "epoch 29 batch 123 loss: 5.219927787780762\n",
      "epoch 29 batch 124 loss: 5.2445478439331055\n",
      "epoch loss: 5.238802684783936\n",
      "epoch 30 batch 0 loss: 5.211474895477295\n",
      "epoch 30 batch 1 loss: 5.250641822814941\n",
      "epoch 30 batch 2 loss: 5.260109901428223\n",
      "epoch 30 batch 3 loss: 5.338024139404297\n",
      "epoch 30 batch 4 loss: 5.2557268142700195\n",
      "epoch 30 batch 5 loss: 5.239936351776123\n",
      "epoch 30 batch 6 loss: 5.236906051635742\n",
      "epoch 30 batch 7 loss: 5.253632545471191\n",
      "epoch 30 batch 8 loss: 5.26643705368042\n",
      "epoch 30 batch 9 loss: 5.240779876708984\n",
      "epoch 30 batch 10 loss: 5.2275848388671875\n",
      "epoch 30 batch 11 loss: 5.277651309967041\n",
      "epoch 30 batch 12 loss: 5.271787166595459\n",
      "epoch 30 batch 13 loss: 5.232200622558594\n",
      "epoch 30 batch 14 loss: 5.221505165100098\n",
      "epoch 30 batch 15 loss: 5.269347190856934\n",
      "epoch 30 batch 16 loss: 5.257474422454834\n",
      "epoch 30 batch 17 loss: 5.263017654418945\n",
      "epoch 30 batch 18 loss: 5.251649856567383\n",
      "epoch 30 batch 19 loss: 5.205446720123291\n",
      "epoch 30 batch 20 loss: 5.20003080368042\n",
      "epoch 30 batch 21 loss: 5.214970588684082\n",
      "epoch 30 batch 22 loss: 5.243907451629639\n",
      "epoch 30 batch 23 loss: 5.2359771728515625\n",
      "epoch 30 batch 24 loss: 5.222092151641846\n",
      "epoch 30 batch 25 loss: 5.275054931640625\n",
      "epoch 30 batch 26 loss: 5.289756774902344\n",
      "epoch 30 batch 27 loss: 5.248450756072998\n",
      "epoch 30 batch 28 loss: 5.231592178344727\n",
      "epoch 30 batch 29 loss: 5.266026496887207\n",
      "epoch 30 batch 30 loss: 5.279732704162598\n",
      "epoch 30 batch 31 loss: 5.217002868652344\n",
      "epoch 30 batch 32 loss: 5.23404598236084\n",
      "epoch 30 batch 33 loss: 5.247646331787109\n",
      "epoch 30 batch 34 loss: 5.273209571838379\n",
      "epoch 30 batch 35 loss: 5.236289024353027\n",
      "epoch 30 batch 36 loss: 5.236083984375\n",
      "epoch 30 batch 37 loss: 5.234322547912598\n",
      "epoch 30 batch 38 loss: 5.2558698654174805\n",
      "epoch 30 batch 39 loss: 5.261239051818848\n",
      "epoch 30 batch 40 loss: 5.241706371307373\n",
      "epoch 30 batch 41 loss: 5.239851951599121\n",
      "epoch 30 batch 42 loss: 5.217856407165527\n",
      "epoch 30 batch 43 loss: 5.245941162109375\n",
      "epoch 30 batch 44 loss: 5.23964786529541\n",
      "epoch 30 batch 45 loss: 5.222070217132568\n",
      "epoch 30 batch 46 loss: 5.215076923370361\n",
      "epoch 30 batch 47 loss: 5.230610370635986\n",
      "epoch 30 batch 48 loss: 5.234835147857666\n",
      "epoch 30 batch 49 loss: 5.209744930267334\n",
      "epoch 30 batch 50 loss: 5.251894474029541\n",
      "epoch 30 batch 51 loss: 5.247315406799316\n",
      "epoch 30 batch 52 loss: 5.288605690002441\n",
      "epoch 30 batch 53 loss: 5.220705032348633\n",
      "epoch 30 batch 54 loss: 5.253201961517334\n",
      "epoch 30 batch 55 loss: 5.224804401397705\n",
      "epoch 30 batch 56 loss: 5.254963397979736\n",
      "epoch 30 batch 57 loss: 5.231574535369873\n",
      "epoch 30 batch 58 loss: 5.228885173797607\n",
      "epoch 30 batch 59 loss: 5.223891258239746\n",
      "epoch 30 batch 60 loss: 5.2207560539245605\n",
      "epoch 30 batch 61 loss: 5.216458797454834\n",
      "epoch 30 batch 62 loss: 5.2360520362854\n",
      "epoch 30 batch 63 loss: 5.260873317718506\n",
      "epoch 30 batch 64 loss: 5.229773998260498\n",
      "epoch 30 batch 65 loss: 5.2199883460998535\n",
      "epoch 30 batch 66 loss: 5.23681640625\n",
      "epoch 30 batch 67 loss: 5.239352703094482\n",
      "epoch 30 batch 68 loss: 5.23439359664917\n",
      "epoch 30 batch 69 loss: 5.221257209777832\n",
      "epoch 30 batch 70 loss: 5.252662181854248\n",
      "epoch 30 batch 71 loss: 5.237010478973389\n",
      "epoch 30 batch 72 loss: 5.246222496032715\n",
      "epoch 30 batch 73 loss: 5.269474983215332\n",
      "epoch 30 batch 74 loss: 5.240649223327637\n",
      "epoch 30 batch 75 loss: 5.256772518157959\n",
      "epoch 30 batch 76 loss: 5.203010082244873\n",
      "epoch 30 batch 77 loss: 5.2328782081604\n",
      "epoch 30 batch 78 loss: 5.274373531341553\n",
      "epoch 30 batch 79 loss: 5.241170406341553\n",
      "epoch 30 batch 80 loss: 5.2532958984375\n",
      "epoch 30 batch 81 loss: 5.231038570404053\n",
      "epoch 30 batch 82 loss: 5.234713077545166\n",
      "epoch 30 batch 83 loss: 5.187387466430664\n",
      "epoch 30 batch 84 loss: 5.230082988739014\n",
      "epoch 30 batch 85 loss: 5.213634490966797\n",
      "epoch 30 batch 86 loss: 5.236596584320068\n",
      "epoch 30 batch 87 loss: 5.196445941925049\n",
      "epoch 30 batch 88 loss: 5.225414752960205\n",
      "epoch 30 batch 89 loss: 5.22304105758667\n",
      "epoch 30 batch 90 loss: 5.260254383087158\n",
      "epoch 30 batch 91 loss: 5.23411750793457\n",
      "epoch 30 batch 92 loss: 5.263147354125977\n",
      "epoch 30 batch 93 loss: 5.216165542602539\n",
      "epoch 30 batch 94 loss: 5.266238212585449\n",
      "epoch 30 batch 95 loss: 5.24439811706543\n",
      "epoch 30 batch 96 loss: 5.225519180297852\n",
      "epoch 30 batch 97 loss: 5.233808517456055\n",
      "epoch 30 batch 98 loss: 5.255321979522705\n",
      "epoch 30 batch 99 loss: 5.239167213439941\n",
      "epoch 30 batch 100 loss: 5.252359390258789\n",
      "epoch 30 batch 101 loss: 5.234868049621582\n",
      "epoch 30 batch 102 loss: 5.2112345695495605\n",
      "epoch 30 batch 103 loss: 5.240429401397705\n",
      "epoch 30 batch 104 loss: 5.247839450836182\n",
      "epoch 30 batch 105 loss: 5.27871561050415\n",
      "epoch 30 batch 106 loss: 5.203144550323486\n",
      "epoch 30 batch 107 loss: 5.252857685089111\n",
      "epoch 30 batch 108 loss: 5.270815372467041\n",
      "epoch 30 batch 109 loss: 5.215665340423584\n",
      "epoch 30 batch 110 loss: 5.204034805297852\n",
      "epoch 30 batch 111 loss: 5.264201641082764\n",
      "epoch 30 batch 112 loss: 5.259659767150879\n",
      "epoch 30 batch 113 loss: 5.218780517578125\n",
      "epoch 30 batch 114 loss: 5.22566032409668\n",
      "epoch 30 batch 115 loss: 5.214979648590088\n",
      "epoch 30 batch 116 loss: 5.194127082824707\n",
      "epoch 30 batch 117 loss: 5.224504470825195\n",
      "epoch 30 batch 118 loss: 5.2621049880981445\n",
      "epoch 30 batch 119 loss: 5.262077331542969\n",
      "epoch 30 batch 120 loss: 5.239480018615723\n",
      "epoch 30 batch 121 loss: 5.221970558166504\n",
      "epoch 30 batch 122 loss: 5.2293195724487305\n",
      "epoch 30 batch 123 loss: 5.241936683654785\n",
      "epoch 30 batch 124 loss: 5.190026760101318\n",
      "epoch loss: 5.239666778564453\n",
      "epoch 31 batch 0 loss: 5.198293209075928\n",
      "epoch 31 batch 1 loss: 5.22934627532959\n",
      "epoch 31 batch 2 loss: 5.223089694976807\n",
      "epoch 31 batch 3 loss: 5.256156921386719\n",
      "epoch 31 batch 4 loss: 5.221937656402588\n",
      "epoch 31 batch 5 loss: 5.2617998123168945\n",
      "epoch 31 batch 6 loss: 5.206120491027832\n",
      "epoch 31 batch 7 loss: 5.221320152282715\n",
      "epoch 31 batch 8 loss: 5.27581787109375\n",
      "epoch 31 batch 9 loss: 5.249320983886719\n",
      "epoch 31 batch 10 loss: 5.24375581741333\n",
      "epoch 31 batch 11 loss: 5.221365451812744\n",
      "epoch 31 batch 12 loss: 5.210039138793945\n",
      "epoch 31 batch 13 loss: 5.213376522064209\n",
      "epoch 31 batch 14 loss: 5.235613822937012\n",
      "epoch 31 batch 15 loss: 5.242878437042236\n",
      "epoch 31 batch 16 loss: 5.252114772796631\n",
      "epoch 31 batch 17 loss: 5.223740100860596\n",
      "epoch 31 batch 18 loss: 5.222405910491943\n",
      "epoch 31 batch 19 loss: 5.219940185546875\n",
      "epoch 31 batch 20 loss: 5.250874042510986\n",
      "epoch 31 batch 21 loss: 5.217193603515625\n",
      "epoch 31 batch 22 loss: 5.25991678237915\n",
      "epoch 31 batch 23 loss: 5.249950885772705\n",
      "epoch 31 batch 24 loss: 5.209859371185303\n",
      "epoch 31 batch 25 loss: 5.245524883270264\n",
      "epoch 31 batch 26 loss: 5.244772911071777\n",
      "epoch 31 batch 27 loss: 5.209671497344971\n",
      "epoch 31 batch 28 loss: 5.225305080413818\n",
      "epoch 31 batch 29 loss: 5.228969573974609\n",
      "epoch 31 batch 30 loss: 5.227345943450928\n",
      "epoch 31 batch 31 loss: 5.228753089904785\n",
      "epoch 31 batch 32 loss: 5.260750770568848\n",
      "epoch 31 batch 33 loss: 5.243494987487793\n",
      "epoch 31 batch 34 loss: 5.217811584472656\n",
      "epoch 31 batch 35 loss: 5.2399444580078125\n",
      "epoch 31 batch 36 loss: 5.248602390289307\n",
      "epoch 31 batch 37 loss: 5.217736721038818\n",
      "epoch 31 batch 38 loss: 5.238863945007324\n",
      "epoch 31 batch 39 loss: 5.248171806335449\n",
      "epoch 31 batch 40 loss: 5.246254920959473\n",
      "epoch 31 batch 41 loss: 5.261488437652588\n",
      "epoch 31 batch 42 loss: 5.234277248382568\n",
      "epoch 31 batch 43 loss: 5.244646072387695\n",
      "epoch 31 batch 44 loss: 5.217239856719971\n",
      "epoch 31 batch 45 loss: 5.227206230163574\n",
      "epoch 31 batch 46 loss: 5.20222282409668\n",
      "epoch 31 batch 47 loss: 5.217438697814941\n",
      "epoch 31 batch 48 loss: 5.272374153137207\n",
      "epoch 31 batch 49 loss: 5.2231597900390625\n",
      "epoch 31 batch 50 loss: 5.225438117980957\n",
      "epoch 31 batch 51 loss: 5.241474628448486\n",
      "epoch 31 batch 52 loss: 5.246283054351807\n",
      "epoch 31 batch 53 loss: 5.261335372924805\n",
      "epoch 31 batch 54 loss: 5.20172119140625\n",
      "epoch 31 batch 55 loss: 5.228598117828369\n",
      "epoch 31 batch 56 loss: 5.227395057678223\n",
      "epoch 31 batch 57 loss: 5.225812911987305\n",
      "epoch 31 batch 58 loss: 5.232508659362793\n",
      "epoch 31 batch 59 loss: 5.194251537322998\n",
      "epoch 31 batch 60 loss: 5.244049072265625\n",
      "epoch 31 batch 61 loss: 5.204636096954346\n",
      "epoch 31 batch 62 loss: 5.21552038192749\n",
      "epoch 31 batch 63 loss: 5.231141090393066\n",
      "epoch 31 batch 64 loss: 5.213519096374512\n",
      "epoch 31 batch 65 loss: 5.22760009765625\n",
      "epoch 31 batch 66 loss: 5.255425930023193\n",
      "epoch 31 batch 67 loss: 5.180414199829102\n",
      "epoch 31 batch 68 loss: 5.190382957458496\n",
      "epoch 31 batch 69 loss: 5.28369665145874\n",
      "epoch 31 batch 70 loss: 5.22804069519043\n",
      "epoch 31 batch 71 loss: 5.2244553565979\n",
      "epoch 31 batch 72 loss: 5.2800493240356445\n",
      "epoch 31 batch 73 loss: 5.215818881988525\n",
      "epoch 31 batch 74 loss: 5.261895656585693\n",
      "epoch 31 batch 75 loss: 5.211304187774658\n",
      "epoch 31 batch 76 loss: 5.165247917175293\n",
      "epoch 31 batch 77 loss: 5.247297286987305\n",
      "epoch 31 batch 78 loss: 5.239560604095459\n",
      "epoch 31 batch 79 loss: 5.243587017059326\n",
      "epoch 31 batch 80 loss: 5.24087381362915\n",
      "epoch 31 batch 81 loss: 5.246915817260742\n",
      "epoch 31 batch 82 loss: 5.210717678070068\n",
      "epoch 31 batch 83 loss: 5.252620220184326\n",
      "epoch 31 batch 84 loss: 5.205286026000977\n",
      "epoch 31 batch 85 loss: 5.220785617828369\n",
      "epoch 31 batch 86 loss: 5.219794273376465\n",
      "epoch 31 batch 87 loss: 5.185394763946533\n",
      "epoch 31 batch 88 loss: 5.220400810241699\n",
      "epoch 31 batch 89 loss: 5.231408596038818\n",
      "epoch 31 batch 90 loss: 5.239343166351318\n",
      "epoch 31 batch 91 loss: 5.2575860023498535\n",
      "epoch 31 batch 92 loss: 5.223918437957764\n",
      "epoch 31 batch 93 loss: 5.240710258483887\n",
      "epoch 31 batch 94 loss: 5.243486404418945\n",
      "epoch 31 batch 95 loss: 5.225810527801514\n",
      "epoch 31 batch 96 loss: 5.228221416473389\n",
      "epoch 31 batch 97 loss: 5.218809604644775\n",
      "epoch 31 batch 98 loss: 5.205078601837158\n",
      "epoch 31 batch 99 loss: 5.246558666229248\n",
      "epoch 31 batch 100 loss: 5.227510929107666\n",
      "epoch 31 batch 101 loss: 5.201475620269775\n",
      "epoch 31 batch 102 loss: 5.237331390380859\n",
      "epoch 31 batch 103 loss: 5.232908725738525\n",
      "epoch 31 batch 104 loss: 5.234991550445557\n",
      "epoch 31 batch 105 loss: 5.216590404510498\n",
      "epoch 31 batch 106 loss: 5.237527370452881\n",
      "epoch 31 batch 107 loss: 5.2596025466918945\n",
      "epoch 31 batch 108 loss: 5.218371391296387\n",
      "epoch 31 batch 109 loss: 5.211317539215088\n",
      "epoch 31 batch 110 loss: 5.242388725280762\n",
      "epoch 31 batch 111 loss: 5.229483127593994\n",
      "epoch 31 batch 112 loss: 5.242778301239014\n",
      "epoch 31 batch 113 loss: 5.221068382263184\n",
      "epoch 31 batch 114 loss: 5.2687225341796875\n",
      "epoch 31 batch 115 loss: 5.198787689208984\n",
      "epoch 31 batch 116 loss: 5.245492458343506\n",
      "epoch 31 batch 117 loss: 5.235646724700928\n",
      "epoch 31 batch 118 loss: 5.199056148529053\n",
      "epoch 31 batch 119 loss: 5.208864212036133\n",
      "epoch 31 batch 120 loss: 5.21696138381958\n",
      "epoch 31 batch 121 loss: 5.184993267059326\n",
      "epoch 31 batch 122 loss: 5.211422443389893\n",
      "epoch 31 batch 123 loss: 5.243247032165527\n",
      "epoch 31 batch 124 loss: 5.180697917938232\n",
      "epoch loss: 5.229661403656006\n",
      "epoch 32 batch 0 loss: 5.2494916915893555\n",
      "epoch 32 batch 1 loss: 5.2326555252075195\n",
      "epoch 32 batch 2 loss: 5.219732761383057\n",
      "epoch 32 batch 3 loss: 5.196413516998291\n",
      "epoch 32 batch 4 loss: 5.220561504364014\n",
      "epoch 32 batch 5 loss: 5.220849514007568\n",
      "epoch 32 batch 6 loss: 5.234796047210693\n",
      "epoch 32 batch 7 loss: 5.198199272155762\n",
      "epoch 32 batch 8 loss: 5.194734573364258\n",
      "epoch 32 batch 9 loss: 5.249971866607666\n",
      "epoch 32 batch 10 loss: 5.212399959564209\n",
      "epoch 32 batch 11 loss: 5.260127067565918\n",
      "epoch 32 batch 12 loss: 5.263484001159668\n",
      "epoch 32 batch 13 loss: 5.217565059661865\n",
      "epoch 32 batch 14 loss: 5.2307939529418945\n",
      "epoch 32 batch 15 loss: 5.211087703704834\n",
      "epoch 32 batch 16 loss: 5.249468803405762\n",
      "epoch 32 batch 17 loss: 5.24269962310791\n",
      "epoch 32 batch 18 loss: 5.222879409790039\n",
      "epoch 32 batch 19 loss: 5.188870906829834\n",
      "epoch 32 batch 20 loss: 5.2247514724731445\n",
      "epoch 32 batch 21 loss: 5.234927654266357\n",
      "epoch 32 batch 22 loss: 5.211119651794434\n",
      "epoch 32 batch 23 loss: 5.2474045753479\n",
      "epoch 32 batch 24 loss: 5.183018684387207\n",
      "epoch 32 batch 25 loss: 5.222743034362793\n",
      "epoch 32 batch 26 loss: 5.193681716918945\n",
      "epoch 32 batch 27 loss: 5.233401775360107\n",
      "epoch 32 batch 28 loss: 5.251869201660156\n",
      "epoch 32 batch 29 loss: 5.20298433303833\n",
      "epoch 32 batch 30 loss: 5.1699090003967285\n",
      "epoch 32 batch 31 loss: 5.2343549728393555\n",
      "epoch 32 batch 32 loss: 5.214816093444824\n",
      "epoch 32 batch 33 loss: 5.198836326599121\n",
      "epoch 32 batch 34 loss: 5.218847751617432\n",
      "epoch 32 batch 35 loss: 5.160294055938721\n",
      "epoch 32 batch 36 loss: 5.226706027984619\n",
      "epoch 32 batch 37 loss: 5.236536026000977\n",
      "epoch 32 batch 38 loss: 5.216498851776123\n",
      "epoch 32 batch 39 loss: 5.2063798904418945\n",
      "epoch 32 batch 40 loss: 5.20794677734375\n",
      "epoch 32 batch 41 loss: 5.2403740882873535\n",
      "epoch 32 batch 42 loss: 5.244010925292969\n",
      "epoch 32 batch 43 loss: 5.216114521026611\n",
      "epoch 32 batch 44 loss: 5.232960224151611\n",
      "epoch 32 batch 45 loss: 5.220263481140137\n",
      "epoch 32 batch 46 loss: 5.231239318847656\n",
      "epoch 32 batch 47 loss: 5.252359390258789\n",
      "epoch 32 batch 48 loss: 5.234881401062012\n",
      "epoch 32 batch 49 loss: 5.1789231300354\n",
      "epoch 32 batch 50 loss: 5.247305393218994\n",
      "epoch 32 batch 51 loss: 5.230780124664307\n",
      "epoch 32 batch 52 loss: 5.251185417175293\n",
      "epoch 32 batch 53 loss: 5.285271644592285\n",
      "epoch 32 batch 54 loss: 5.238057613372803\n",
      "epoch 32 batch 55 loss: 5.242130756378174\n",
      "epoch 32 batch 56 loss: 5.210621356964111\n",
      "epoch 32 batch 57 loss: 5.244869232177734\n",
      "epoch 32 batch 58 loss: 5.25642204284668\n",
      "epoch 32 batch 59 loss: 5.222125053405762\n",
      "epoch 32 batch 60 loss: 5.2687201499938965\n",
      "epoch 32 batch 61 loss: 5.200582027435303\n",
      "epoch 32 batch 62 loss: 5.249401569366455\n",
      "epoch 32 batch 63 loss: 5.206271648406982\n",
      "epoch 32 batch 64 loss: 5.265651702880859\n",
      "epoch 32 batch 65 loss: 5.224086284637451\n",
      "epoch 32 batch 66 loss: 5.234982967376709\n",
      "epoch 32 batch 67 loss: 5.252583026885986\n",
      "epoch 32 batch 68 loss: 5.219966888427734\n",
      "epoch 32 batch 69 loss: 5.177846431732178\n",
      "epoch 32 batch 70 loss: 5.219736099243164\n",
      "epoch 32 batch 71 loss: 5.231016159057617\n",
      "epoch 32 batch 72 loss: 5.2257795333862305\n",
      "epoch 32 batch 73 loss: 5.239375114440918\n",
      "epoch 32 batch 74 loss: 5.240936756134033\n",
      "epoch 32 batch 75 loss: 5.262667179107666\n",
      "epoch 32 batch 76 loss: 5.214861869812012\n",
      "epoch 32 batch 77 loss: 5.217139720916748\n",
      "epoch 32 batch 78 loss: 5.214405536651611\n",
      "epoch 32 batch 79 loss: 5.240566253662109\n",
      "epoch 32 batch 80 loss: 5.206225395202637\n",
      "epoch 32 batch 81 loss: 5.248867511749268\n",
      "epoch 32 batch 82 loss: 5.233913421630859\n",
      "epoch 32 batch 83 loss: 5.261712551116943\n",
      "epoch 32 batch 84 loss: 5.232409477233887\n",
      "epoch 32 batch 85 loss: 5.271426200866699\n",
      "epoch 32 batch 86 loss: 5.2155561447143555\n",
      "epoch 32 batch 87 loss: 5.23478889465332\n",
      "epoch 32 batch 88 loss: 5.2339019775390625\n",
      "epoch 32 batch 89 loss: 5.213782787322998\n",
      "epoch 32 batch 90 loss: 5.259731292724609\n",
      "epoch 32 batch 91 loss: 5.260635852813721\n",
      "epoch 32 batch 92 loss: 5.216675758361816\n",
      "epoch 32 batch 93 loss: 5.201210975646973\n",
      "epoch 32 batch 94 loss: 5.222850322723389\n",
      "epoch 32 batch 95 loss: 5.239192485809326\n",
      "epoch 32 batch 96 loss: 5.240592956542969\n",
      "epoch 32 batch 97 loss: 5.250932693481445\n",
      "epoch 32 batch 98 loss: 5.215260982513428\n",
      "epoch 32 batch 99 loss: 5.248126029968262\n",
      "epoch 32 batch 100 loss: 5.209396839141846\n",
      "epoch 32 batch 101 loss: 5.2095232009887695\n",
      "epoch 32 batch 102 loss: 5.2658371925354\n",
      "epoch 32 batch 103 loss: 5.249065399169922\n",
      "epoch 32 batch 104 loss: 5.230135917663574\n",
      "epoch 32 batch 105 loss: 5.228539943695068\n",
      "epoch 32 batch 106 loss: 5.222586631774902\n",
      "epoch 32 batch 107 loss: 5.206593990325928\n",
      "epoch 32 batch 108 loss: 5.226118087768555\n",
      "epoch 32 batch 109 loss: 5.225449562072754\n",
      "epoch 32 batch 110 loss: 5.231436252593994\n",
      "epoch 32 batch 111 loss: 5.229584693908691\n",
      "epoch 32 batch 112 loss: 5.207954406738281\n",
      "epoch 32 batch 113 loss: 5.213332176208496\n",
      "epoch 32 batch 114 loss: 5.233392238616943\n",
      "epoch 32 batch 115 loss: 5.25462532043457\n",
      "epoch 32 batch 116 loss: 5.212810516357422\n",
      "epoch 32 batch 117 loss: 5.235238075256348\n",
      "epoch 32 batch 118 loss: 5.221616268157959\n",
      "epoch 32 batch 119 loss: 5.283919334411621\n",
      "epoch 32 batch 120 loss: 5.2310895919799805\n",
      "epoch 32 batch 121 loss: 5.239044189453125\n",
      "epoch 32 batch 122 loss: 5.218935012817383\n",
      "epoch 32 batch 123 loss: 5.244245529174805\n",
      "epoch 32 batch 124 loss: 5.2171454429626465\n",
      "epoch loss: 5.228445537567138\n",
      "epoch 33 batch 0 loss: 5.22520112991333\n",
      "epoch 33 batch 1 loss: 5.200295448303223\n",
      "epoch 33 batch 2 loss: 5.246203422546387\n",
      "epoch 33 batch 3 loss: 5.227931976318359\n",
      "epoch 33 batch 4 loss: 5.256380081176758\n",
      "epoch 33 batch 5 loss: 5.219624996185303\n",
      "epoch 33 batch 6 loss: 5.196545124053955\n",
      "epoch 33 batch 7 loss: 5.220178127288818\n",
      "epoch 33 batch 8 loss: 5.21403694152832\n",
      "epoch 33 batch 9 loss: 5.196895599365234\n",
      "epoch 33 batch 10 loss: 5.250729084014893\n",
      "epoch 33 batch 11 loss: 5.236662864685059\n",
      "epoch 33 batch 12 loss: 5.247381687164307\n",
      "epoch 33 batch 13 loss: 5.234919548034668\n",
      "epoch 33 batch 14 loss: 5.215462684631348\n",
      "epoch 33 batch 15 loss: 5.2368998527526855\n",
      "epoch 33 batch 16 loss: 5.245113372802734\n",
      "epoch 33 batch 17 loss: 5.23008918762207\n",
      "epoch 33 batch 18 loss: 5.229516506195068\n",
      "epoch 33 batch 19 loss: 5.25577449798584\n",
      "epoch 33 batch 20 loss: 5.2947893142700195\n",
      "epoch 33 batch 21 loss: 5.238560199737549\n",
      "epoch 33 batch 22 loss: 5.224475383758545\n",
      "epoch 33 batch 23 loss: 5.234927654266357\n",
      "epoch 33 batch 24 loss: 5.244844913482666\n",
      "epoch 33 batch 25 loss: 5.215700149536133\n",
      "epoch 33 batch 26 loss: 5.209689140319824\n",
      "epoch 33 batch 27 loss: 5.278875827789307\n",
      "epoch 33 batch 28 loss: 5.22796630859375\n",
      "epoch 33 batch 29 loss: 5.230119705200195\n",
      "epoch 33 batch 30 loss: 5.226188659667969\n",
      "epoch 33 batch 31 loss: 5.246425628662109\n",
      "epoch 33 batch 32 loss: 5.19766902923584\n",
      "epoch 33 batch 33 loss: 5.230167865753174\n",
      "epoch 33 batch 34 loss: 5.224282264709473\n",
      "epoch 33 batch 35 loss: 5.203980445861816\n",
      "epoch 33 batch 36 loss: 5.23902702331543\n",
      "epoch 33 batch 37 loss: 5.233431339263916\n",
      "epoch 33 batch 38 loss: 5.206942081451416\n",
      "epoch 33 batch 39 loss: 5.263322830200195\n",
      "epoch 33 batch 40 loss: 5.239734172821045\n",
      "epoch 33 batch 41 loss: 5.205597877502441\n",
      "epoch 33 batch 42 loss: 5.205368518829346\n",
      "epoch 33 batch 43 loss: 5.22816801071167\n",
      "epoch 33 batch 44 loss: 5.196934700012207\n",
      "epoch 33 batch 45 loss: 5.2088422775268555\n",
      "epoch 33 batch 46 loss: 5.222485065460205\n",
      "epoch 33 batch 47 loss: 5.236076354980469\n",
      "epoch 33 batch 48 loss: 5.213043689727783\n",
      "epoch 33 batch 49 loss: 5.252345085144043\n",
      "epoch 33 batch 50 loss: 5.202006816864014\n",
      "epoch 33 batch 51 loss: 5.2219672203063965\n",
      "epoch 33 batch 52 loss: 5.261303424835205\n",
      "epoch 33 batch 53 loss: 5.228733539581299\n",
      "epoch 33 batch 54 loss: 5.200324058532715\n",
      "epoch 33 batch 55 loss: 5.243774890899658\n",
      "epoch 33 batch 56 loss: 5.231310844421387\n",
      "epoch 33 batch 57 loss: 5.241418361663818\n",
      "epoch 33 batch 58 loss: 5.217993259429932\n",
      "epoch 33 batch 59 loss: 5.190489292144775\n",
      "epoch 33 batch 60 loss: 5.230050086975098\n",
      "epoch 33 batch 61 loss: 5.227870464324951\n",
      "epoch 33 batch 62 loss: 5.24558162689209\n",
      "epoch 33 batch 63 loss: 5.269622802734375\n",
      "epoch 33 batch 64 loss: 5.200169563293457\n",
      "epoch 33 batch 65 loss: 5.198889255523682\n",
      "epoch 33 batch 66 loss: 5.205377101898193\n",
      "epoch 33 batch 67 loss: 5.240408420562744\n",
      "epoch 33 batch 68 loss: 5.200570106506348\n",
      "epoch 33 batch 69 loss: 5.255794048309326\n",
      "epoch 33 batch 70 loss: 5.183049201965332\n",
      "epoch 33 batch 71 loss: 5.216142654418945\n",
      "epoch 33 batch 72 loss: 5.247446060180664\n",
      "epoch 33 batch 73 loss: 5.204771995544434\n",
      "epoch 33 batch 74 loss: 5.220393180847168\n",
      "epoch 33 batch 75 loss: 5.18801736831665\n",
      "epoch 33 batch 76 loss: 5.202678203582764\n",
      "epoch 33 batch 77 loss: 5.239535808563232\n",
      "epoch 33 batch 78 loss: 5.263564109802246\n",
      "epoch 33 batch 79 loss: 5.199548244476318\n",
      "epoch 33 batch 80 loss: 5.197717189788818\n",
      "epoch 33 batch 81 loss: 5.245195388793945\n",
      "epoch 33 batch 82 loss: 5.2417168617248535\n",
      "epoch 33 batch 83 loss: 5.207773208618164\n",
      "epoch 33 batch 84 loss: 5.2060546875\n",
      "epoch 33 batch 85 loss: 5.235912799835205\n",
      "epoch 33 batch 86 loss: 5.166770935058594\n",
      "epoch 33 batch 87 loss: 5.198567867279053\n",
      "epoch 33 batch 88 loss: 5.219603061676025\n",
      "epoch 33 batch 89 loss: 5.242098808288574\n",
      "epoch 33 batch 90 loss: 5.219719886779785\n",
      "epoch 33 batch 91 loss: 5.2313971519470215\n",
      "epoch 33 batch 92 loss: 5.220703601837158\n",
      "epoch 33 batch 93 loss: 5.200791358947754\n",
      "epoch 33 batch 94 loss: 5.218735218048096\n",
      "epoch 33 batch 95 loss: 5.249948024749756\n",
      "epoch 33 batch 96 loss: 5.220711708068848\n",
      "epoch 33 batch 97 loss: 5.248891353607178\n",
      "epoch 33 batch 98 loss: 5.233602046966553\n",
      "epoch 33 batch 99 loss: 5.222839832305908\n",
      "epoch 33 batch 100 loss: 5.1950788497924805\n",
      "epoch 33 batch 101 loss: 5.249976634979248\n",
      "epoch 33 batch 102 loss: 5.222819805145264\n",
      "epoch 33 batch 103 loss: 5.2025299072265625\n",
      "epoch 33 batch 104 loss: 5.215909481048584\n",
      "epoch 33 batch 105 loss: 5.24395751953125\n",
      "epoch 33 batch 106 loss: 5.203204154968262\n",
      "epoch 33 batch 107 loss: 5.189101219177246\n",
      "epoch 33 batch 108 loss: 5.2012834548950195\n",
      "epoch 33 batch 109 loss: 5.227778911590576\n",
      "epoch 33 batch 110 loss: 5.234072685241699\n",
      "epoch 33 batch 111 loss: 5.204137325286865\n",
      "epoch 33 batch 112 loss: 5.243562698364258\n",
      "epoch 33 batch 113 loss: 5.230288982391357\n",
      "epoch 33 batch 114 loss: 5.285914421081543\n",
      "epoch 33 batch 115 loss: 5.229045867919922\n",
      "epoch 33 batch 116 loss: 5.193659782409668\n",
      "epoch 33 batch 117 loss: 5.217536449432373\n",
      "epoch 33 batch 118 loss: 5.239584922790527\n",
      "epoch 33 batch 119 loss: 5.201695442199707\n",
      "epoch 33 batch 120 loss: 5.196657657623291\n",
      "epoch 33 batch 121 loss: 5.23417329788208\n",
      "epoch 33 batch 122 loss: 5.187788009643555\n",
      "epoch 33 batch 123 loss: 5.213100910186768\n",
      "epoch 33 batch 124 loss: 5.242593765258789\n",
      "epoch loss: 5.224630630493164\n",
      "epoch 34 batch 0 loss: 5.229991436004639\n",
      "epoch 34 batch 1 loss: 5.242319107055664\n",
      "epoch 34 batch 2 loss: 5.252573013305664\n",
      "epoch 34 batch 3 loss: 5.252231121063232\n",
      "epoch 34 batch 4 loss: 5.227787971496582\n",
      "epoch 34 batch 5 loss: 5.222651481628418\n",
      "epoch 34 batch 6 loss: 5.211703777313232\n",
      "epoch 34 batch 7 loss: 5.198540687561035\n",
      "epoch 34 batch 8 loss: 5.2449951171875\n",
      "epoch 34 batch 9 loss: 5.229305267333984\n",
      "epoch 34 batch 10 loss: 5.205573081970215\n",
      "epoch 34 batch 11 loss: 5.207389831542969\n",
      "epoch 34 batch 12 loss: 5.202148914337158\n",
      "epoch 34 batch 13 loss: 5.234316349029541\n",
      "epoch 34 batch 14 loss: 5.200171947479248\n",
      "epoch 34 batch 15 loss: 5.167116641998291\n",
      "epoch 34 batch 16 loss: 5.203207969665527\n",
      "epoch 34 batch 17 loss: 5.228241443634033\n",
      "epoch 34 batch 18 loss: 5.201178073883057\n",
      "epoch 34 batch 19 loss: 5.203051567077637\n",
      "epoch 34 batch 20 loss: 5.2204203605651855\n",
      "epoch 34 batch 21 loss: 5.186152935028076\n",
      "epoch 34 batch 22 loss: 5.223007678985596\n",
      "epoch 34 batch 23 loss: 5.235665321350098\n",
      "epoch 34 batch 24 loss: 5.250024318695068\n",
      "epoch 34 batch 25 loss: 5.259848594665527\n",
      "epoch 34 batch 26 loss: 5.219433307647705\n",
      "epoch 34 batch 27 loss: 5.209492206573486\n",
      "epoch 34 batch 28 loss: 5.251813888549805\n",
      "epoch 34 batch 29 loss: 5.214369773864746\n",
      "epoch 34 batch 30 loss: 5.202857494354248\n",
      "epoch 34 batch 31 loss: 5.21177864074707\n",
      "epoch 34 batch 32 loss: 5.193290710449219\n",
      "epoch 34 batch 33 loss: 5.226691246032715\n",
      "epoch 34 batch 34 loss: 5.211306095123291\n",
      "epoch 34 batch 35 loss: 5.254940032958984\n",
      "epoch 34 batch 36 loss: 5.206087589263916\n",
      "epoch 34 batch 37 loss: 5.233800411224365\n",
      "epoch 34 batch 38 loss: 5.210804462432861\n",
      "epoch 34 batch 39 loss: 5.223849773406982\n",
      "epoch 34 batch 40 loss: 5.222855567932129\n",
      "epoch 34 batch 41 loss: 5.2161455154418945\n",
      "epoch 34 batch 42 loss: 5.230885982513428\n",
      "epoch 34 batch 43 loss: 5.221498966217041\n",
      "epoch 34 batch 44 loss: 5.230606555938721\n",
      "epoch 34 batch 45 loss: 5.208389759063721\n",
      "epoch 34 batch 46 loss: 5.221653938293457\n",
      "epoch 34 batch 47 loss: 5.200456619262695\n",
      "epoch 34 batch 48 loss: 5.218988418579102\n",
      "epoch 34 batch 49 loss: 5.218057632446289\n",
      "epoch 34 batch 50 loss: 5.203373432159424\n",
      "epoch 34 batch 51 loss: 5.185379981994629\n",
      "epoch 34 batch 52 loss: 5.212911128997803\n",
      "epoch 34 batch 53 loss: 5.190480709075928\n",
      "epoch 34 batch 54 loss: 5.202427387237549\n",
      "epoch 34 batch 55 loss: 5.261293411254883\n",
      "epoch 34 batch 56 loss: 5.211469650268555\n",
      "epoch 34 batch 57 loss: 5.161265850067139\n",
      "epoch 34 batch 58 loss: 5.239406585693359\n",
      "epoch 34 batch 59 loss: 5.191073894500732\n",
      "epoch 34 batch 60 loss: 5.228931903839111\n",
      "epoch 34 batch 61 loss: 5.257767200469971\n",
      "epoch 34 batch 62 loss: 5.229115962982178\n",
      "epoch 34 batch 63 loss: 5.235365867614746\n",
      "epoch 34 batch 64 loss: 5.212626934051514\n",
      "epoch 34 batch 65 loss: 5.250115871429443\n",
      "epoch 34 batch 66 loss: 5.222680568695068\n",
      "epoch 34 batch 67 loss: 5.234769344329834\n",
      "epoch 34 batch 68 loss: 5.2302751541137695\n",
      "epoch 34 batch 69 loss: 5.206225395202637\n",
      "epoch 34 batch 70 loss: 5.2676568031311035\n",
      "epoch 34 batch 71 loss: 5.273393630981445\n",
      "epoch 34 batch 72 loss: 5.216386795043945\n",
      "epoch 34 batch 73 loss: 5.214658737182617\n",
      "epoch 34 batch 74 loss: 5.2265753746032715\n",
      "epoch 34 batch 75 loss: 5.225420951843262\n",
      "epoch 34 batch 76 loss: 5.226628303527832\n",
      "epoch 34 batch 77 loss: 5.2181396484375\n",
      "epoch 34 batch 78 loss: 5.210289001464844\n",
      "epoch 34 batch 79 loss: 5.204432487487793\n",
      "epoch 34 batch 80 loss: 5.213076591491699\n",
      "epoch 34 batch 81 loss: 5.212700843811035\n",
      "epoch 34 batch 82 loss: 5.184235572814941\n",
      "epoch 34 batch 83 loss: 5.234097957611084\n",
      "epoch 34 batch 84 loss: 5.213013172149658\n",
      "epoch 34 batch 85 loss: 5.2144670486450195\n",
      "epoch 34 batch 86 loss: 5.220435619354248\n",
      "epoch 34 batch 87 loss: 5.198526382446289\n",
      "epoch 34 batch 88 loss: 5.273278713226318\n",
      "epoch 34 batch 89 loss: 5.231457233428955\n",
      "epoch 34 batch 90 loss: 5.219283103942871\n",
      "epoch 34 batch 91 loss: 5.228433609008789\n",
      "epoch 34 batch 92 loss: 5.207716464996338\n",
      "epoch 34 batch 93 loss: 5.237329006195068\n",
      "epoch 34 batch 94 loss: 5.249653339385986\n",
      "epoch 34 batch 95 loss: 5.24171257019043\n",
      "epoch 34 batch 96 loss: 5.213318824768066\n",
      "epoch 34 batch 97 loss: 5.25950813293457\n",
      "epoch 34 batch 98 loss: 5.181346893310547\n",
      "epoch 34 batch 99 loss: 5.197575569152832\n",
      "epoch 34 batch 100 loss: 5.229527473449707\n",
      "epoch 34 batch 101 loss: 5.199168682098389\n",
      "epoch 34 batch 102 loss: 5.21913480758667\n",
      "epoch 34 batch 103 loss: 5.216679573059082\n",
      "epoch 34 batch 104 loss: 5.232101917266846\n",
      "epoch 34 batch 105 loss: 5.223857402801514\n",
      "epoch 34 batch 106 loss: 5.229738712310791\n",
      "epoch 34 batch 107 loss: 5.207142353057861\n",
      "epoch 34 batch 108 loss: 5.234634876251221\n",
      "epoch 34 batch 109 loss: 5.208847522735596\n",
      "epoch 34 batch 110 loss: 5.244729518890381\n",
      "epoch 34 batch 111 loss: 5.189115047454834\n",
      "epoch 34 batch 112 loss: 5.239840507507324\n",
      "epoch 34 batch 113 loss: 5.22899055480957\n",
      "epoch 34 batch 114 loss: 5.233105182647705\n",
      "epoch 34 batch 115 loss: 5.222409725189209\n",
      "epoch 34 batch 116 loss: 5.234571933746338\n",
      "epoch 34 batch 117 loss: 5.229913234710693\n",
      "epoch 34 batch 118 loss: 5.194127082824707\n",
      "epoch 34 batch 119 loss: 5.250218391418457\n",
      "epoch 34 batch 120 loss: 5.203887939453125\n",
      "epoch 34 batch 121 loss: 5.239020824432373\n",
      "epoch 34 batch 122 loss: 5.2025346755981445\n",
      "epoch 34 batch 123 loss: 5.208622932434082\n",
      "epoch 34 batch 124 loss: 5.243526935577393\n",
      "epoch loss: 5.22117456817627\n",
      "epoch 35 batch 0 loss: 5.173346519470215\n",
      "epoch 35 batch 1 loss: 5.196635723114014\n",
      "epoch 35 batch 2 loss: 5.234152793884277\n",
      "epoch 35 batch 3 loss: 5.277313709259033\n",
      "epoch 35 batch 4 loss: 5.277431488037109\n",
      "epoch 35 batch 5 loss: 5.211349964141846\n",
      "epoch 35 batch 6 loss: 5.231846332550049\n",
      "epoch 35 batch 7 loss: 5.228348731994629\n",
      "epoch 35 batch 8 loss: 5.267155170440674\n",
      "epoch 35 batch 9 loss: 5.227811813354492\n",
      "epoch 35 batch 10 loss: 5.226731300354004\n",
      "epoch 35 batch 11 loss: 5.193506240844727\n",
      "epoch 35 batch 12 loss: 5.224359035491943\n",
      "epoch 35 batch 13 loss: 5.196076393127441\n",
      "epoch 35 batch 14 loss: 5.232415676116943\n",
      "epoch 35 batch 15 loss: 5.23408842086792\n",
      "epoch 35 batch 16 loss: 5.252917289733887\n",
      "epoch 35 batch 17 loss: 5.2191162109375\n",
      "epoch 35 batch 18 loss: 5.2521891593933105\n",
      "epoch 35 batch 19 loss: 5.27542781829834\n",
      "epoch 35 batch 20 loss: 5.178582191467285\n",
      "epoch 35 batch 21 loss: 5.235095024108887\n",
      "epoch 35 batch 22 loss: 5.197382926940918\n",
      "epoch 35 batch 23 loss: 5.196377277374268\n",
      "epoch 35 batch 24 loss: 5.259480953216553\n",
      "epoch 35 batch 25 loss: 5.18284797668457\n",
      "epoch 35 batch 26 loss: 5.212117671966553\n",
      "epoch 35 batch 27 loss: 5.226057052612305\n",
      "epoch 35 batch 28 loss: 5.2420783042907715\n",
      "epoch 35 batch 29 loss: 5.192671298980713\n",
      "epoch 35 batch 30 loss: 5.199819087982178\n",
      "epoch 35 batch 31 loss: 5.201882362365723\n",
      "epoch 35 batch 32 loss: 5.236328601837158\n",
      "epoch 35 batch 33 loss: 5.2013726234436035\n",
      "epoch 35 batch 34 loss: 5.226166248321533\n",
      "epoch 35 batch 35 loss: 5.221320152282715\n",
      "epoch 35 batch 36 loss: 5.213442325592041\n",
      "epoch 35 batch 37 loss: 5.246728420257568\n",
      "epoch 35 batch 38 loss: 5.264884948730469\n",
      "epoch 35 batch 39 loss: 5.23842191696167\n",
      "epoch 35 batch 40 loss: 5.251247882843018\n",
      "epoch 35 batch 41 loss: 5.200477123260498\n",
      "epoch 35 batch 42 loss: 5.231140613555908\n",
      "epoch 35 batch 43 loss: 5.212372779846191\n",
      "epoch 35 batch 44 loss: 5.192609786987305\n",
      "epoch 35 batch 45 loss: 5.24329137802124\n",
      "epoch 35 batch 46 loss: 5.20053768157959\n",
      "epoch 35 batch 47 loss: 5.206606864929199\n",
      "epoch 35 batch 48 loss: 5.223625183105469\n",
      "epoch 35 batch 49 loss: 5.236645221710205\n",
      "epoch 35 batch 50 loss: 5.222511291503906\n",
      "epoch 35 batch 51 loss: 5.193896293640137\n",
      "epoch 35 batch 52 loss: 5.215986728668213\n",
      "epoch 35 batch 53 loss: 5.211881160736084\n",
      "epoch 35 batch 54 loss: 5.232498645782471\n",
      "epoch 35 batch 55 loss: 5.219230651855469\n",
      "epoch 35 batch 56 loss: 5.25079870223999\n",
      "epoch 35 batch 57 loss: 5.213832378387451\n",
      "epoch 35 batch 58 loss: 5.202772617340088\n",
      "epoch 35 batch 59 loss: 5.228524208068848\n",
      "epoch 35 batch 60 loss: 5.224626541137695\n",
      "epoch 35 batch 61 loss: 5.214150905609131\n",
      "epoch 35 batch 62 loss: 5.245842933654785\n",
      "epoch 35 batch 63 loss: 5.234630584716797\n",
      "epoch 35 batch 64 loss: 5.247366428375244\n",
      "epoch 35 batch 65 loss: 5.223555564880371\n",
      "epoch 35 batch 66 loss: 5.212588787078857\n",
      "epoch 35 batch 67 loss: 5.236405372619629\n",
      "epoch 35 batch 68 loss: 5.211700439453125\n",
      "epoch 35 batch 69 loss: 5.207162857055664\n",
      "epoch 35 batch 70 loss: 5.2523040771484375\n",
      "epoch 35 batch 71 loss: 5.2069621086120605\n",
      "epoch 35 batch 72 loss: 5.224463939666748\n",
      "epoch 35 batch 73 loss: 5.244853496551514\n",
      "epoch 35 batch 74 loss: 5.233584880828857\n",
      "epoch 35 batch 75 loss: 5.2290544509887695\n",
      "epoch 35 batch 76 loss: 5.199914455413818\n",
      "epoch 35 batch 77 loss: 5.194879055023193\n",
      "epoch 35 batch 78 loss: 5.174153804779053\n",
      "epoch 35 batch 79 loss: 5.231729507446289\n",
      "epoch 35 batch 80 loss: 5.204967021942139\n",
      "epoch 35 batch 81 loss: 5.224974155426025\n",
      "epoch 35 batch 82 loss: 5.199591159820557\n",
      "epoch 35 batch 83 loss: 5.276989459991455\n",
      "epoch 35 batch 84 loss: 5.257059097290039\n",
      "epoch 35 batch 85 loss: 5.233893871307373\n",
      "epoch 35 batch 86 loss: 5.2393646240234375\n",
      "epoch 35 batch 87 loss: 5.213222503662109\n",
      "epoch 35 batch 88 loss: 5.192509651184082\n",
      "epoch 35 batch 89 loss: 5.19041633605957\n",
      "epoch 35 batch 90 loss: 5.224812984466553\n",
      "epoch 35 batch 91 loss: 5.181353569030762\n",
      "epoch 35 batch 92 loss: 5.215245246887207\n",
      "epoch 35 batch 93 loss: 5.185152530670166\n",
      "epoch 35 batch 94 loss: 5.21353816986084\n",
      "epoch 35 batch 95 loss: 5.239554405212402\n",
      "epoch 35 batch 96 loss: 5.204936981201172\n",
      "epoch 35 batch 97 loss: 5.204936981201172\n",
      "epoch 35 batch 98 loss: 5.269699573516846\n",
      "epoch 35 batch 99 loss: 5.245306968688965\n",
      "epoch 35 batch 100 loss: 5.211565017700195\n",
      "epoch 35 batch 101 loss: 5.198382377624512\n",
      "epoch 35 batch 102 loss: 5.211849689483643\n",
      "epoch 35 batch 103 loss: 5.2178473472595215\n",
      "epoch 35 batch 104 loss: 5.186371326446533\n",
      "epoch 35 batch 105 loss: 5.228421688079834\n",
      "epoch 35 batch 106 loss: 5.232723236083984\n",
      "epoch 35 batch 107 loss: 5.223920822143555\n",
      "epoch 35 batch 108 loss: 5.205270290374756\n",
      "epoch 35 batch 109 loss: 5.236901760101318\n",
      "epoch 35 batch 110 loss: 5.175487995147705\n",
      "epoch 35 batch 111 loss: 5.249136924743652\n",
      "epoch 35 batch 112 loss: 5.199265480041504\n",
      "epoch 35 batch 113 loss: 5.262434959411621\n",
      "epoch 35 batch 114 loss: 5.191324234008789\n",
      "epoch 35 batch 115 loss: 5.239413738250732\n",
      "epoch 35 batch 116 loss: 5.184069633483887\n",
      "epoch 35 batch 117 loss: 5.201269626617432\n",
      "epoch 35 batch 118 loss: 5.253762245178223\n",
      "epoch 35 batch 119 loss: 5.212839126586914\n",
      "epoch 35 batch 120 loss: 5.2148871421813965\n",
      "epoch 35 batch 121 loss: 5.235100746154785\n",
      "epoch 35 batch 122 loss: 5.250655174255371\n",
      "epoch 35 batch 123 loss: 5.172845363616943\n",
      "epoch 35 batch 124 loss: 5.184643268585205\n",
      "epoch loss: 5.221148872375489\n",
      "epoch 36 batch 0 loss: 5.182390213012695\n",
      "epoch 36 batch 1 loss: 5.224434852600098\n",
      "epoch 36 batch 2 loss: 5.202690124511719\n",
      "epoch 36 batch 3 loss: 5.232690811157227\n",
      "epoch 36 batch 4 loss: 5.234338760375977\n",
      "epoch 36 batch 5 loss: 5.233869552612305\n",
      "epoch 36 batch 6 loss: 5.176948070526123\n",
      "epoch 36 batch 7 loss: 5.2212114334106445\n",
      "epoch 36 batch 8 loss: 5.19007682800293\n",
      "epoch 36 batch 9 loss: 5.245088577270508\n",
      "epoch 36 batch 10 loss: 5.220948219299316\n",
      "epoch 36 batch 11 loss: 5.256091117858887\n",
      "epoch 36 batch 12 loss: 5.244954586029053\n",
      "epoch 36 batch 13 loss: 5.191096305847168\n",
      "epoch 36 batch 14 loss: 5.206138610839844\n",
      "epoch 36 batch 15 loss: 5.229506492614746\n",
      "epoch 36 batch 16 loss: 5.215847015380859\n",
      "epoch 36 batch 17 loss: 5.170042991638184\n",
      "epoch 36 batch 18 loss: 5.209134578704834\n",
      "epoch 36 batch 19 loss: 5.233090877532959\n",
      "epoch 36 batch 20 loss: 5.221459865570068\n",
      "epoch 36 batch 21 loss: 5.228993892669678\n",
      "epoch 36 batch 22 loss: 5.255155563354492\n",
      "epoch 36 batch 23 loss: 5.206961631774902\n",
      "epoch 36 batch 24 loss: 5.2399725914001465\n",
      "epoch 36 batch 25 loss: 5.246933460235596\n",
      "epoch 36 batch 26 loss: 5.224644660949707\n",
      "epoch 36 batch 27 loss: 5.233912467956543\n",
      "epoch 36 batch 28 loss: 5.186757564544678\n",
      "epoch 36 batch 29 loss: 5.256552696228027\n",
      "epoch 36 batch 30 loss: 5.249017238616943\n",
      "epoch 36 batch 31 loss: 5.2353973388671875\n",
      "epoch 36 batch 32 loss: 5.219884395599365\n",
      "epoch 36 batch 33 loss: 5.198004722595215\n",
      "epoch 36 batch 34 loss: 5.190305709838867\n",
      "epoch 36 batch 35 loss: 5.186789512634277\n",
      "epoch 36 batch 36 loss: 5.210999011993408\n",
      "epoch 36 batch 37 loss: 5.214393138885498\n",
      "epoch 36 batch 38 loss: 5.221156120300293\n",
      "epoch 36 batch 39 loss: 5.2281494140625\n",
      "epoch 36 batch 40 loss: 5.227388858795166\n",
      "epoch 36 batch 41 loss: 5.2253546714782715\n",
      "epoch 36 batch 42 loss: 5.196913719177246\n",
      "epoch 36 batch 43 loss: 5.195217132568359\n",
      "epoch 36 batch 44 loss: 5.223138332366943\n",
      "epoch 36 batch 45 loss: 5.218851089477539\n",
      "epoch 36 batch 46 loss: 5.211246967315674\n",
      "epoch 36 batch 47 loss: 5.1741156578063965\n",
      "epoch 36 batch 48 loss: 5.212879657745361\n",
      "epoch 36 batch 49 loss: 5.2049431800842285\n",
      "epoch 36 batch 50 loss: 5.243886470794678\n",
      "epoch 36 batch 51 loss: 5.203093528747559\n",
      "epoch 36 batch 52 loss: 5.220048904418945\n",
      "epoch 36 batch 53 loss: 5.226524353027344\n",
      "epoch 36 batch 54 loss: 5.2487897872924805\n",
      "epoch 36 batch 55 loss: 5.225111484527588\n",
      "epoch 36 batch 56 loss: 5.236274242401123\n",
      "epoch 36 batch 57 loss: 5.20286226272583\n",
      "epoch 36 batch 58 loss: 5.222285270690918\n",
      "epoch 36 batch 59 loss: 5.230744361877441\n",
      "epoch 36 batch 60 loss: 5.233727931976318\n",
      "epoch 36 batch 61 loss: 5.234423637390137\n",
      "epoch 36 batch 62 loss: 5.208757400512695\n",
      "epoch 36 batch 63 loss: 5.216973304748535\n",
      "epoch 36 batch 64 loss: 5.184964656829834\n",
      "epoch 36 batch 65 loss: 5.198737144470215\n",
      "epoch 36 batch 66 loss: 5.1913743019104\n",
      "epoch 36 batch 67 loss: 5.229465007781982\n",
      "epoch 36 batch 68 loss: 5.245810508728027\n",
      "epoch 36 batch 69 loss: 5.207027435302734\n",
      "epoch 36 batch 70 loss: 5.21575927734375\n",
      "epoch 36 batch 71 loss: 5.223312854766846\n",
      "epoch 36 batch 72 loss: 5.267256259918213\n",
      "epoch 36 batch 73 loss: 5.210654258728027\n",
      "epoch 36 batch 74 loss: 5.227614402770996\n",
      "epoch 36 batch 75 loss: 5.234724521636963\n",
      "epoch 36 batch 76 loss: 5.213428974151611\n",
      "epoch 36 batch 77 loss: 5.200933933258057\n",
      "epoch 36 batch 78 loss: 5.241630554199219\n",
      "epoch 36 batch 79 loss: 5.249424457550049\n",
      "epoch 36 batch 80 loss: 5.255364894866943\n",
      "epoch 36 batch 81 loss: 5.222955703735352\n",
      "epoch 36 batch 82 loss: 5.239079475402832\n",
      "epoch 36 batch 83 loss: 5.199297904968262\n",
      "epoch 36 batch 84 loss: 5.232470512390137\n",
      "epoch 36 batch 85 loss: 5.215937614440918\n",
      "epoch 36 batch 86 loss: 5.24278450012207\n",
      "epoch 36 batch 87 loss: 5.19385290145874\n",
      "epoch 36 batch 88 loss: 5.218527793884277\n",
      "epoch 36 batch 89 loss: 5.2368292808532715\n",
      "epoch 36 batch 90 loss: 5.19974946975708\n",
      "epoch 36 batch 91 loss: 5.198967933654785\n",
      "epoch 36 batch 92 loss: 5.175042629241943\n",
      "epoch 36 batch 93 loss: 5.249710559844971\n",
      "epoch 36 batch 94 loss: 5.237189292907715\n",
      "epoch 36 batch 95 loss: 5.232561111450195\n",
      "epoch 36 batch 96 loss: 5.194998264312744\n",
      "epoch 36 batch 97 loss: 5.211864948272705\n",
      "epoch 36 batch 98 loss: 5.194142818450928\n",
      "epoch 36 batch 99 loss: 5.21329927444458\n",
      "epoch 36 batch 100 loss: 5.190773963928223\n",
      "epoch 36 batch 101 loss: 5.191965103149414\n",
      "epoch 36 batch 102 loss: 5.203800678253174\n",
      "epoch 36 batch 103 loss: 5.221887111663818\n",
      "epoch 36 batch 104 loss: 5.198030948638916\n",
      "epoch 36 batch 105 loss: 5.210928916931152\n",
      "epoch 36 batch 106 loss: 5.222692966461182\n",
      "epoch 36 batch 107 loss: 5.235360622406006\n",
      "epoch 36 batch 108 loss: 5.164257526397705\n",
      "epoch 36 batch 109 loss: 5.219329833984375\n",
      "epoch 36 batch 110 loss: 5.179629802703857\n",
      "epoch 36 batch 111 loss: 5.211048603057861\n",
      "epoch 36 batch 112 loss: 5.227998733520508\n",
      "epoch 36 batch 113 loss: 5.234228610992432\n",
      "epoch 36 batch 114 loss: 5.232577323913574\n",
      "epoch 36 batch 115 loss: 5.220488548278809\n",
      "epoch 36 batch 116 loss: 5.2105817794799805\n",
      "epoch 36 batch 117 loss: 5.203434944152832\n",
      "epoch 36 batch 118 loss: 5.198768138885498\n",
      "epoch 36 batch 119 loss: 5.244345188140869\n",
      "epoch 36 batch 120 loss: 5.212773323059082\n",
      "epoch 36 batch 121 loss: 5.2361273765563965\n",
      "epoch 36 batch 122 loss: 5.228311538696289\n",
      "epoch 36 batch 123 loss: 5.251728534698486\n",
      "epoch 36 batch 124 loss: 5.203240871429443\n",
      "epoch loss: 5.2182451171875\n",
      "epoch 37 batch 0 loss: 5.2147722244262695\n",
      "epoch 37 batch 1 loss: 5.224402904510498\n",
      "epoch 37 batch 2 loss: 5.212730407714844\n",
      "epoch 37 batch 3 loss: 5.276166915893555\n",
      "epoch 37 batch 4 loss: 5.201513767242432\n",
      "epoch 37 batch 5 loss: 5.246037483215332\n",
      "epoch 37 batch 6 loss: 5.230719089508057\n",
      "epoch 37 batch 7 loss: 5.249989032745361\n",
      "epoch 37 batch 8 loss: 5.228348731994629\n",
      "epoch 37 batch 9 loss: 5.216088771820068\n",
      "epoch 37 batch 10 loss: 5.202061653137207\n",
      "epoch 37 batch 11 loss: 5.210928916931152\n",
      "epoch 37 batch 12 loss: 5.21461820602417\n",
      "epoch 37 batch 13 loss: 5.217011451721191\n",
      "epoch 37 batch 14 loss: 5.251406192779541\n",
      "epoch 37 batch 15 loss: 5.228055477142334\n",
      "epoch 37 batch 16 loss: 5.218504428863525\n",
      "epoch 37 batch 17 loss: 5.215582847595215\n",
      "epoch 37 batch 18 loss: 5.195090293884277\n",
      "epoch 37 batch 19 loss: 5.199424743652344\n",
      "epoch 37 batch 20 loss: 5.198294162750244\n",
      "epoch 37 batch 21 loss: 5.203666687011719\n",
      "epoch 37 batch 22 loss: 5.232141017913818\n",
      "epoch 37 batch 23 loss: 5.2635416984558105\n",
      "epoch 37 batch 24 loss: 5.2248148918151855\n",
      "epoch 37 batch 25 loss: 5.203432083129883\n",
      "epoch 37 batch 26 loss: 5.239048480987549\n",
      "epoch 37 batch 27 loss: 5.264474391937256\n",
      "epoch 37 batch 28 loss: 5.218814373016357\n",
      "epoch 37 batch 29 loss: 5.219268798828125\n",
      "epoch 37 batch 30 loss: 5.206470966339111\n",
      "epoch 37 batch 31 loss: 5.228638648986816\n",
      "epoch 37 batch 32 loss: 5.195629596710205\n",
      "epoch 37 batch 33 loss: 5.205258846282959\n",
      "epoch 37 batch 34 loss: 5.227657318115234\n",
      "epoch 37 batch 35 loss: 5.190378189086914\n",
      "epoch 37 batch 36 loss: 5.245928764343262\n",
      "epoch 37 batch 37 loss: 5.243740558624268\n",
      "epoch 37 batch 38 loss: 5.197084903717041\n",
      "epoch 37 batch 39 loss: 5.206016540527344\n",
      "epoch 37 batch 40 loss: 5.225710391998291\n",
      "epoch 37 batch 41 loss: 5.196094036102295\n",
      "epoch 37 batch 42 loss: 5.2293620109558105\n",
      "epoch 37 batch 43 loss: 5.2037353515625\n",
      "epoch 37 batch 44 loss: 5.213898181915283\n",
      "epoch 37 batch 45 loss: 5.208934783935547\n",
      "epoch 37 batch 46 loss: 5.19666051864624\n",
      "epoch 37 batch 47 loss: 5.256645202636719\n",
      "epoch 37 batch 48 loss: 5.223186492919922\n",
      "epoch 37 batch 49 loss: 5.226571559906006\n",
      "epoch 37 batch 50 loss: 5.250518798828125\n",
      "epoch 37 batch 51 loss: 5.2035746574401855\n",
      "epoch 37 batch 52 loss: 5.222143650054932\n",
      "epoch 37 batch 53 loss: 5.213115692138672\n",
      "epoch 37 batch 54 loss: 5.196794509887695\n",
      "epoch 37 batch 55 loss: 5.160747528076172\n",
      "epoch 37 batch 56 loss: 5.215628147125244\n",
      "epoch 37 batch 57 loss: 5.199709415435791\n",
      "epoch 37 batch 58 loss: 5.258063316345215\n",
      "epoch 37 batch 59 loss: 5.241373062133789\n",
      "epoch 37 batch 60 loss: 5.198429584503174\n",
      "epoch 37 batch 61 loss: 5.169321060180664\n",
      "epoch 37 batch 62 loss: 5.253215789794922\n",
      "epoch 37 batch 63 loss: 5.231685638427734\n",
      "epoch 37 batch 64 loss: 5.17236852645874\n",
      "epoch 37 batch 65 loss: 5.236526966094971\n",
      "epoch 37 batch 66 loss: 5.208977699279785\n",
      "epoch 37 batch 67 loss: 5.229665279388428\n",
      "epoch 37 batch 68 loss: 5.198726654052734\n",
      "epoch 37 batch 69 loss: 5.2282209396362305\n",
      "epoch 37 batch 70 loss: 5.200735569000244\n",
      "epoch 37 batch 71 loss: 5.248705863952637\n",
      "epoch 37 batch 72 loss: 5.212993621826172\n",
      "epoch 37 batch 73 loss: 5.196628093719482\n",
      "epoch 37 batch 74 loss: 5.240666389465332\n",
      "epoch 37 batch 75 loss: 5.217024803161621\n",
      "epoch 37 batch 76 loss: 5.239405632019043\n",
      "epoch 37 batch 77 loss: 5.177299499511719\n",
      "epoch 37 batch 78 loss: 5.230297088623047\n",
      "epoch 37 batch 79 loss: 5.212531566619873\n",
      "epoch 37 batch 80 loss: 5.220037937164307\n",
      "epoch 37 batch 81 loss: 5.223465442657471\n",
      "epoch 37 batch 82 loss: 5.224749565124512\n",
      "epoch 37 batch 83 loss: 5.22451114654541\n",
      "epoch 37 batch 84 loss: 5.2234697341918945\n",
      "epoch 37 batch 85 loss: 5.206259727478027\n",
      "epoch 37 batch 86 loss: 5.211234092712402\n",
      "epoch 37 batch 87 loss: 5.184884548187256\n",
      "epoch 37 batch 88 loss: 5.237401008605957\n",
      "epoch 37 batch 89 loss: 5.256742000579834\n",
      "epoch 37 batch 90 loss: 5.178030967712402\n",
      "epoch 37 batch 91 loss: 5.221652030944824\n",
      "epoch 37 batch 92 loss: 5.257483959197998\n",
      "epoch 37 batch 93 loss: 5.205012321472168\n",
      "epoch 37 batch 94 loss: 5.222448825836182\n",
      "epoch 37 batch 95 loss: 5.216989040374756\n",
      "epoch 37 batch 96 loss: 5.208644866943359\n",
      "epoch 37 batch 97 loss: 5.226814270019531\n",
      "epoch 37 batch 98 loss: 5.24949836730957\n",
      "epoch 37 batch 99 loss: 5.182745456695557\n",
      "epoch 37 batch 100 loss: 5.197458267211914\n",
      "epoch 37 batch 101 loss: 5.238026142120361\n",
      "epoch 37 batch 102 loss: 5.230381965637207\n",
      "epoch 37 batch 103 loss: 5.241816520690918\n",
      "epoch 37 batch 104 loss: 5.233551979064941\n",
      "epoch 37 batch 105 loss: 5.2023138999938965\n",
      "epoch 37 batch 106 loss: 5.210704326629639\n",
      "epoch 37 batch 107 loss: 5.181250095367432\n",
      "epoch 37 batch 108 loss: 5.231867790222168\n",
      "epoch 37 batch 109 loss: 5.218942642211914\n",
      "epoch 37 batch 110 loss: 5.200032711029053\n",
      "epoch 37 batch 111 loss: 5.218260288238525\n",
      "epoch 37 batch 112 loss: 5.218660831451416\n",
      "epoch 37 batch 113 loss: 5.174571514129639\n",
      "epoch 37 batch 114 loss: 5.246546745300293\n",
      "epoch 37 batch 115 loss: 5.216581344604492\n",
      "epoch 37 batch 116 loss: 5.225065231323242\n",
      "epoch 37 batch 117 loss: 5.185375690460205\n",
      "epoch 37 batch 118 loss: 5.174150466918945\n",
      "epoch 37 batch 119 loss: 5.22199821472168\n",
      "epoch 37 batch 120 loss: 5.225786209106445\n",
      "epoch 37 batch 121 loss: 5.237085342407227\n",
      "epoch 37 batch 122 loss: 5.225514888763428\n",
      "epoch 37 batch 123 loss: 5.260196208953857\n",
      "epoch 37 batch 124 loss: 5.213178634643555\n",
      "epoch loss: 5.218600330352783\n",
      "epoch 38 batch 0 loss: 5.223772048950195\n",
      "epoch 38 batch 1 loss: 5.2203474044799805\n",
      "epoch 38 batch 2 loss: 5.254203796386719\n",
      "epoch 38 batch 3 loss: 5.222960472106934\n",
      "epoch 38 batch 4 loss: 5.181884765625\n",
      "epoch 38 batch 5 loss: 5.202375411987305\n",
      "epoch 38 batch 6 loss: 5.214042663574219\n",
      "epoch 38 batch 7 loss: 5.224686145782471\n",
      "epoch 38 batch 8 loss: 5.243103981018066\n",
      "epoch 38 batch 9 loss: 5.225329875946045\n",
      "epoch 38 batch 10 loss: 5.189065933227539\n",
      "epoch 38 batch 11 loss: 5.183660984039307\n",
      "epoch 38 batch 12 loss: 5.212166786193848\n",
      "epoch 38 batch 13 loss: 5.210902690887451\n",
      "epoch 38 batch 14 loss: 5.209475517272949\n",
      "epoch 38 batch 15 loss: 5.252374649047852\n",
      "epoch 38 batch 16 loss: 5.216997146606445\n",
      "epoch 38 batch 17 loss: 5.207515716552734\n",
      "epoch 38 batch 18 loss: 5.22659969329834\n",
      "epoch 38 batch 19 loss: 5.1923627853393555\n",
      "epoch 38 batch 20 loss: 5.2031073570251465\n",
      "epoch 38 batch 21 loss: 5.200875282287598\n",
      "epoch 38 batch 22 loss: 5.199321269989014\n",
      "epoch 38 batch 23 loss: 5.252558708190918\n",
      "epoch 38 batch 24 loss: 5.228369235992432\n",
      "epoch 38 batch 25 loss: 5.196810245513916\n",
      "epoch 38 batch 26 loss: 5.180075645446777\n",
      "epoch 38 batch 27 loss: 5.211954116821289\n",
      "epoch 38 batch 28 loss: 5.223190784454346\n",
      "epoch 38 batch 29 loss: 5.184987545013428\n",
      "epoch 38 batch 30 loss: 5.21488094329834\n",
      "epoch 38 batch 31 loss: 5.243160724639893\n",
      "epoch 38 batch 32 loss: 5.230311393737793\n",
      "epoch 38 batch 33 loss: 5.221810340881348\n",
      "epoch 38 batch 34 loss: 5.209102630615234\n",
      "epoch 38 batch 35 loss: 5.221211910247803\n",
      "epoch 38 batch 36 loss: 5.184968948364258\n",
      "epoch 38 batch 37 loss: 5.218118190765381\n",
      "epoch 38 batch 38 loss: 5.218479633331299\n",
      "epoch 38 batch 39 loss: 5.180740833282471\n",
      "epoch 38 batch 40 loss: 5.257265567779541\n",
      "epoch 38 batch 41 loss: 5.21832799911499\n",
      "epoch 38 batch 42 loss: 5.213386058807373\n",
      "epoch 38 batch 43 loss: 5.251742839813232\n",
      "epoch 38 batch 44 loss: 5.208518028259277\n",
      "epoch 38 batch 45 loss: 5.184387683868408\n",
      "epoch 38 batch 46 loss: 5.197336196899414\n",
      "epoch 38 batch 47 loss: 5.191086292266846\n",
      "epoch 38 batch 48 loss: 5.20684814453125\n",
      "epoch 38 batch 49 loss: 5.202615261077881\n",
      "epoch 38 batch 50 loss: 5.224946022033691\n",
      "epoch 38 batch 51 loss: 5.184663772583008\n",
      "epoch 38 batch 52 loss: 5.249858379364014\n",
      "epoch 38 batch 53 loss: 5.232321739196777\n",
      "epoch 38 batch 54 loss: 5.185018062591553\n",
      "epoch 38 batch 55 loss: 5.195136547088623\n",
      "epoch 38 batch 56 loss: 5.244930744171143\n",
      "epoch 38 batch 57 loss: 5.17756986618042\n",
      "epoch 38 batch 58 loss: 5.177283763885498\n",
      "epoch 38 batch 59 loss: 5.212790489196777\n",
      "epoch 38 batch 60 loss: 5.217667579650879\n",
      "epoch 38 batch 61 loss: 5.1818461418151855\n",
      "epoch 38 batch 62 loss: 5.2240214347839355\n",
      "epoch 38 batch 63 loss: 5.236749172210693\n",
      "epoch 38 batch 64 loss: 5.208123207092285\n",
      "epoch 38 batch 65 loss: 5.217593669891357\n",
      "epoch 38 batch 66 loss: 5.194821834564209\n",
      "epoch 38 batch 67 loss: 5.204960346221924\n",
      "epoch 38 batch 68 loss: 5.210385799407959\n",
      "epoch 38 batch 69 loss: 5.216136932373047\n",
      "epoch 38 batch 70 loss: 5.209475040435791\n",
      "epoch 38 batch 71 loss: 5.256964206695557\n",
      "epoch 38 batch 72 loss: 5.248610019683838\n",
      "epoch 38 batch 73 loss: 5.222238540649414\n",
      "epoch 38 batch 74 loss: 5.195310115814209\n",
      "epoch 38 batch 75 loss: 5.2035956382751465\n",
      "epoch 38 batch 76 loss: 5.2118024826049805\n",
      "epoch 38 batch 77 loss: 5.228292942047119\n",
      "epoch 38 batch 78 loss: 5.228774070739746\n",
      "epoch 38 batch 79 loss: 5.229489803314209\n",
      "epoch 38 batch 80 loss: 5.224506855010986\n",
      "epoch 38 batch 81 loss: 5.22669792175293\n",
      "epoch 38 batch 82 loss: 5.249129295349121\n",
      "epoch 38 batch 83 loss: 5.194876194000244\n",
      "epoch 38 batch 84 loss: 5.21973991394043\n",
      "epoch 38 batch 85 loss: 5.240782260894775\n",
      "epoch 38 batch 86 loss: 5.186640739440918\n",
      "epoch 38 batch 87 loss: 5.239521503448486\n",
      "epoch 38 batch 88 loss: 5.238930702209473\n",
      "epoch 38 batch 89 loss: 5.182973384857178\n",
      "epoch 38 batch 90 loss: 5.246219158172607\n",
      "epoch 38 batch 91 loss: 5.205766677856445\n",
      "epoch 38 batch 92 loss: 5.213736534118652\n",
      "epoch 38 batch 93 loss: 5.189432144165039\n",
      "epoch 38 batch 94 loss: 5.189237117767334\n",
      "epoch 38 batch 95 loss: 5.209191799163818\n",
      "epoch 38 batch 96 loss: 5.226196765899658\n",
      "epoch 38 batch 97 loss: 5.217220783233643\n",
      "epoch 38 batch 98 loss: 5.206413269042969\n",
      "epoch 38 batch 99 loss: 5.207825660705566\n",
      "epoch 38 batch 100 loss: 5.230971336364746\n",
      "epoch 38 batch 101 loss: 5.190950870513916\n",
      "epoch 38 batch 102 loss: 5.205222129821777\n",
      "epoch 38 batch 103 loss: 5.153565406799316\n",
      "epoch 38 batch 104 loss: 5.227269172668457\n",
      "epoch 38 batch 105 loss: 5.232656002044678\n",
      "epoch 38 batch 106 loss: 5.207755088806152\n",
      "epoch 38 batch 107 loss: 5.213937282562256\n",
      "epoch 38 batch 108 loss: 5.210076808929443\n",
      "epoch 38 batch 109 loss: 5.191216945648193\n",
      "epoch 38 batch 110 loss: 5.185338020324707\n",
      "epoch 38 batch 111 loss: 5.249959468841553\n",
      "epoch 38 batch 112 loss: 5.227774143218994\n",
      "epoch 38 batch 113 loss: 5.248452186584473\n",
      "epoch 38 batch 114 loss: 5.229824066162109\n",
      "epoch 38 batch 115 loss: 5.196761131286621\n",
      "epoch 38 batch 116 loss: 5.209786891937256\n",
      "epoch 38 batch 117 loss: 5.280798435211182\n",
      "epoch 38 batch 118 loss: 5.19729471206665\n",
      "epoch 38 batch 119 loss: 5.215335369110107\n",
      "epoch 38 batch 120 loss: 5.221138000488281\n",
      "epoch 38 batch 121 loss: 5.2173752784729\n",
      "epoch 38 batch 122 loss: 5.216165542602539\n",
      "epoch 38 batch 123 loss: 5.235260009765625\n",
      "epoch 38 batch 124 loss: 5.2171125411987305\n",
      "epoch loss: 5.214622337341308\n",
      "epoch 39 batch 0 loss: 5.213512420654297\n",
      "epoch 39 batch 1 loss: 5.240354061126709\n",
      "epoch 39 batch 2 loss: 5.209529876708984\n",
      "epoch 39 batch 3 loss: 5.185507774353027\n",
      "epoch 39 batch 4 loss: 5.223082542419434\n",
      "epoch 39 batch 5 loss: 5.228684425354004\n",
      "epoch 39 batch 6 loss: 5.207547664642334\n",
      "epoch 39 batch 7 loss: 5.194606781005859\n",
      "epoch 39 batch 8 loss: 5.209775924682617\n",
      "epoch 39 batch 9 loss: 5.257845878601074\n",
      "epoch 39 batch 10 loss: 5.215425491333008\n",
      "epoch 39 batch 11 loss: 5.188993453979492\n",
      "epoch 39 batch 12 loss: 5.231390476226807\n",
      "epoch 39 batch 13 loss: 5.184498310089111\n",
      "epoch 39 batch 14 loss: 5.202905178070068\n",
      "epoch 39 batch 15 loss: 5.237529754638672\n",
      "epoch 39 batch 16 loss: 5.222103595733643\n",
      "epoch 39 batch 17 loss: 5.225319862365723\n",
      "epoch 39 batch 18 loss: 5.24212646484375\n",
      "epoch 39 batch 19 loss: 5.235440731048584\n",
      "epoch 39 batch 20 loss: 5.200080871582031\n",
      "epoch 39 batch 21 loss: 5.178492546081543\n",
      "epoch 39 batch 22 loss: 5.227277755737305\n",
      "epoch 39 batch 23 loss: 5.230691432952881\n",
      "epoch 39 batch 24 loss: 5.197647571563721\n",
      "epoch 39 batch 25 loss: 5.235423564910889\n",
      "epoch 39 batch 26 loss: 5.213213920593262\n",
      "epoch 39 batch 27 loss: 5.218782424926758\n",
      "epoch 39 batch 28 loss: 5.21219539642334\n",
      "epoch 39 batch 29 loss: 5.215220928192139\n",
      "epoch 39 batch 30 loss: 5.193391799926758\n",
      "epoch 39 batch 31 loss: 5.208968162536621\n",
      "epoch 39 batch 32 loss: 5.229686737060547\n",
      "epoch 39 batch 33 loss: 5.206267356872559\n",
      "epoch 39 batch 34 loss: 5.2277326583862305\n",
      "epoch 39 batch 35 loss: 5.215724945068359\n",
      "epoch 39 batch 36 loss: 5.190700531005859\n",
      "epoch 39 batch 37 loss: 5.229534149169922\n",
      "epoch 39 batch 38 loss: 5.2401604652404785\n",
      "epoch 39 batch 39 loss: 5.228640556335449\n",
      "epoch 39 batch 40 loss: 5.206478118896484\n",
      "epoch 39 batch 41 loss: 5.184807777404785\n",
      "epoch 39 batch 42 loss: 5.210353851318359\n",
      "epoch 39 batch 43 loss: 5.200994968414307\n",
      "epoch 39 batch 44 loss: 5.221651554107666\n",
      "epoch 39 batch 45 loss: 5.204213619232178\n",
      "epoch 39 batch 46 loss: 5.237724304199219\n",
      "epoch 39 batch 47 loss: 5.191882133483887\n",
      "epoch 39 batch 48 loss: 5.194149017333984\n",
      "epoch 39 batch 49 loss: 5.211058139801025\n",
      "epoch 39 batch 50 loss: 5.214776039123535\n",
      "epoch 39 batch 51 loss: 5.237806797027588\n",
      "epoch 39 batch 52 loss: 5.195888519287109\n",
      "epoch 39 batch 53 loss: 5.2425537109375\n",
      "epoch 39 batch 54 loss: 5.223658561706543\n",
      "epoch 39 batch 55 loss: 5.208712100982666\n",
      "epoch 39 batch 56 loss: 5.208163738250732\n",
      "epoch 39 batch 57 loss: 5.2259745597839355\n",
      "epoch 39 batch 58 loss: 5.174484729766846\n",
      "epoch 39 batch 59 loss: 5.204535484313965\n",
      "epoch 39 batch 60 loss: 5.186562538146973\n",
      "epoch 39 batch 61 loss: 5.188206672668457\n",
      "epoch 39 batch 62 loss: 5.21489143371582\n",
      "epoch 39 batch 63 loss: 5.196189880371094\n",
      "epoch 39 batch 64 loss: 5.187399864196777\n",
      "epoch 39 batch 65 loss: 5.206564426422119\n",
      "epoch 39 batch 66 loss: 5.22661018371582\n",
      "epoch 39 batch 67 loss: 5.194146633148193\n",
      "epoch 39 batch 68 loss: 5.18496036529541\n",
      "epoch 39 batch 69 loss: 5.240542411804199\n",
      "epoch 39 batch 70 loss: 5.212594509124756\n",
      "epoch 39 batch 71 loss: 5.229794025421143\n",
      "epoch 39 batch 72 loss: 5.210022449493408\n",
      "epoch 39 batch 73 loss: 5.23377799987793\n",
      "epoch 39 batch 74 loss: 5.22575044631958\n",
      "epoch 39 batch 75 loss: 5.216080665588379\n",
      "epoch 39 batch 76 loss: 5.218634128570557\n",
      "epoch 39 batch 77 loss: 5.192860126495361\n",
      "epoch 39 batch 78 loss: 5.2132248878479\n",
      "epoch 39 batch 79 loss: 5.2054877281188965\n",
      "epoch 39 batch 80 loss: 5.25165319442749\n",
      "epoch 39 batch 81 loss: 5.196664810180664\n",
      "epoch 39 batch 82 loss: 5.24053955078125\n",
      "epoch 39 batch 83 loss: 5.216484069824219\n",
      "epoch 39 batch 84 loss: 5.220136642456055\n",
      "epoch 39 batch 85 loss: 5.225316047668457\n",
      "epoch 39 batch 86 loss: 5.17544412612915\n",
      "epoch 39 batch 87 loss: 5.196331024169922\n",
      "epoch 39 batch 88 loss: 5.2186408042907715\n",
      "epoch 39 batch 89 loss: 5.197263240814209\n",
      "epoch 39 batch 90 loss: 5.1911468505859375\n",
      "epoch 39 batch 91 loss: 5.221109390258789\n",
      "epoch 39 batch 92 loss: 5.213126182556152\n",
      "epoch 39 batch 93 loss: 5.2082061767578125\n",
      "epoch 39 batch 94 loss: 5.208306789398193\n",
      "epoch 39 batch 95 loss: 5.253392219543457\n",
      "epoch 39 batch 96 loss: 5.205461502075195\n",
      "epoch 39 batch 97 loss: 5.2123637199401855\n",
      "epoch 39 batch 98 loss: 5.17988920211792\n",
      "epoch 39 batch 99 loss: 5.244494438171387\n",
      "epoch 39 batch 100 loss: 5.173100471496582\n",
      "epoch 39 batch 101 loss: 5.241130352020264\n",
      "epoch 39 batch 102 loss: 5.202325344085693\n",
      "epoch 39 batch 103 loss: 5.21936559677124\n",
      "epoch 39 batch 104 loss: 5.2164788246154785\n",
      "epoch 39 batch 105 loss: 5.213355541229248\n",
      "epoch 39 batch 106 loss: 5.219186305999756\n",
      "epoch 39 batch 107 loss: 5.182402610778809\n",
      "epoch 39 batch 108 loss: 5.216097354888916\n",
      "epoch 39 batch 109 loss: 5.218451976776123\n",
      "epoch 39 batch 110 loss: 5.203352928161621\n",
      "epoch 39 batch 111 loss: 5.210517883300781\n",
      "epoch 39 batch 112 loss: 5.236796855926514\n",
      "epoch 39 batch 113 loss: 5.197327613830566\n",
      "epoch 39 batch 114 loss: 5.215510368347168\n",
      "epoch 39 batch 115 loss: 5.214792251586914\n",
      "epoch 39 batch 116 loss: 5.213409900665283\n",
      "epoch 39 batch 117 loss: 5.19821834564209\n",
      "epoch 39 batch 118 loss: 5.19334602355957\n",
      "epoch 39 batch 119 loss: 5.250472068786621\n",
      "epoch 39 batch 120 loss: 5.219363212585449\n",
      "epoch 39 batch 121 loss: 5.215634822845459\n",
      "epoch 39 batch 122 loss: 5.1980438232421875\n",
      "epoch 39 batch 123 loss: 5.205279350280762\n",
      "epoch 39 batch 124 loss: 5.199288845062256\n",
      "epoch loss: 5.212715217590333\n",
      "epoch 40 batch 0 loss: 5.217421531677246\n",
      "epoch 40 batch 1 loss: 5.232855796813965\n",
      "epoch 40 batch 2 loss: 5.225193977355957\n",
      "epoch 40 batch 3 loss: 5.2195000648498535\n",
      "epoch 40 batch 4 loss: 5.19398307800293\n",
      "epoch 40 batch 5 loss: 5.228714942932129\n",
      "epoch 40 batch 6 loss: 5.180700778961182\n",
      "epoch 40 batch 7 loss: 5.2111382484436035\n",
      "epoch 40 batch 8 loss: 5.219047546386719\n",
      "epoch 40 batch 9 loss: 5.185990333557129\n",
      "epoch 40 batch 10 loss: 5.195606708526611\n",
      "epoch 40 batch 11 loss: 5.197770118713379\n",
      "epoch 40 batch 12 loss: 5.225470542907715\n",
      "epoch 40 batch 13 loss: 5.168339729309082\n",
      "epoch 40 batch 14 loss: 5.208273410797119\n",
      "epoch 40 batch 15 loss: 5.195484638214111\n",
      "epoch 40 batch 16 loss: 5.2354254722595215\n",
      "epoch 40 batch 17 loss: 5.199009895324707\n",
      "epoch 40 batch 18 loss: 5.209660053253174\n",
      "epoch 40 batch 19 loss: 5.202544212341309\n",
      "epoch 40 batch 20 loss: 5.2386932373046875\n",
      "epoch 40 batch 21 loss: 5.197366714477539\n",
      "epoch 40 batch 22 loss: 5.206107139587402\n",
      "epoch 40 batch 23 loss: 5.19470739364624\n",
      "epoch 40 batch 24 loss: 5.207141399383545\n",
      "epoch 40 batch 25 loss: 5.219757556915283\n",
      "epoch 40 batch 26 loss: 5.20387077331543\n",
      "epoch 40 batch 27 loss: 5.234595775604248\n",
      "epoch 40 batch 28 loss: 5.200553894042969\n",
      "epoch 40 batch 29 loss: 5.220378398895264\n",
      "epoch 40 batch 30 loss: 5.161446571350098\n",
      "epoch 40 batch 31 loss: 5.204699516296387\n",
      "epoch 40 batch 32 loss: 5.244932174682617\n",
      "epoch 40 batch 33 loss: 5.207579135894775\n",
      "epoch 40 batch 34 loss: 5.225807189941406\n",
      "epoch 40 batch 35 loss: 5.200826168060303\n",
      "epoch 40 batch 36 loss: 5.215168476104736\n",
      "epoch 40 batch 37 loss: 5.203835964202881\n",
      "epoch 40 batch 38 loss: 5.211047172546387\n",
      "epoch 40 batch 39 loss: 5.238771915435791\n",
      "epoch 40 batch 40 loss: 5.200176239013672\n",
      "epoch 40 batch 41 loss: 5.215113639831543\n",
      "epoch 40 batch 42 loss: 5.203376293182373\n",
      "epoch 40 batch 43 loss: 5.236330509185791\n",
      "epoch 40 batch 44 loss: 5.210875034332275\n",
      "epoch 40 batch 45 loss: 5.2173991203308105\n",
      "epoch 40 batch 46 loss: 5.195760250091553\n",
      "epoch 40 batch 47 loss: 5.19620418548584\n",
      "epoch 40 batch 48 loss: 5.2233405113220215\n",
      "epoch 40 batch 49 loss: 5.227212905883789\n",
      "epoch 40 batch 50 loss: 5.186629295349121\n",
      "epoch 40 batch 51 loss: 5.203052043914795\n",
      "epoch 40 batch 52 loss: 5.228472709655762\n",
      "epoch 40 batch 53 loss: 5.202035427093506\n",
      "epoch 40 batch 54 loss: 5.231033802032471\n",
      "epoch 40 batch 55 loss: 5.206092834472656\n",
      "epoch 40 batch 56 loss: 5.197242259979248\n",
      "epoch 40 batch 57 loss: 5.221250057220459\n",
      "epoch 40 batch 58 loss: 5.240146160125732\n",
      "epoch 40 batch 59 loss: 5.202284336090088\n",
      "epoch 40 batch 60 loss: 5.237505912780762\n",
      "epoch 40 batch 61 loss: 5.237756729125977\n",
      "epoch 40 batch 62 loss: 5.199516296386719\n",
      "epoch 40 batch 63 loss: 5.213367938995361\n",
      "epoch 40 batch 64 loss: 5.210166931152344\n",
      "epoch 40 batch 65 loss: 5.198785305023193\n",
      "epoch 40 batch 66 loss: 5.214026927947998\n",
      "epoch 40 batch 67 loss: 5.233336448669434\n",
      "epoch 40 batch 68 loss: 5.217215538024902\n",
      "epoch 40 batch 69 loss: 5.1482439041137695\n",
      "epoch 40 batch 70 loss: 5.253260612487793\n",
      "epoch 40 batch 71 loss: 5.21230936050415\n",
      "epoch 40 batch 72 loss: 5.194775104522705\n",
      "epoch 40 batch 73 loss: 5.247803688049316\n",
      "epoch 40 batch 74 loss: 5.232315540313721\n",
      "epoch 40 batch 75 loss: 5.208293914794922\n",
      "epoch 40 batch 76 loss: 5.218026161193848\n",
      "epoch 40 batch 77 loss: 5.166988372802734\n",
      "epoch 40 batch 78 loss: 5.20469331741333\n",
      "epoch 40 batch 79 loss: 5.202507972717285\n",
      "epoch 40 batch 80 loss: 5.196254730224609\n",
      "epoch 40 batch 81 loss: 5.1756157875061035\n",
      "epoch 40 batch 82 loss: 5.20517635345459\n",
      "epoch 40 batch 83 loss: 5.218157768249512\n",
      "epoch 40 batch 84 loss: 5.188937664031982\n",
      "epoch 40 batch 85 loss: 5.191298007965088\n",
      "epoch 40 batch 86 loss: 5.2079572677612305\n",
      "epoch 40 batch 87 loss: 5.199549674987793\n",
      "epoch 40 batch 88 loss: 5.18870210647583\n",
      "epoch 40 batch 89 loss: 5.217818260192871\n",
      "epoch 40 batch 90 loss: 5.207701206207275\n",
      "epoch 40 batch 91 loss: 5.227447509765625\n",
      "epoch 40 batch 92 loss: 5.221804141998291\n",
      "epoch 40 batch 93 loss: 5.212255001068115\n",
      "epoch 40 batch 94 loss: 5.199146747589111\n",
      "epoch 40 batch 95 loss: 5.227766513824463\n",
      "epoch 40 batch 96 loss: 5.2333903312683105\n",
      "epoch 40 batch 97 loss: 5.201698303222656\n",
      "epoch 40 batch 98 loss: 5.201513767242432\n",
      "epoch 40 batch 99 loss: 5.20166015625\n",
      "epoch 40 batch 100 loss: 5.212522983551025\n",
      "epoch 40 batch 101 loss: 5.176150798797607\n",
      "epoch 40 batch 102 loss: 5.194135665893555\n",
      "epoch 40 batch 103 loss: 5.218659400939941\n",
      "epoch 40 batch 104 loss: 5.17381477355957\n",
      "epoch 40 batch 105 loss: 5.231695652008057\n",
      "epoch 40 batch 106 loss: 5.194588661193848\n",
      "epoch 40 batch 107 loss: 5.217942237854004\n",
      "epoch 40 batch 108 loss: 5.200368404388428\n",
      "epoch 40 batch 109 loss: 5.214168548583984\n",
      "epoch 40 batch 110 loss: 5.206295013427734\n",
      "epoch 40 batch 111 loss: 5.210368633270264\n",
      "epoch 40 batch 112 loss: 5.221329212188721\n",
      "epoch 40 batch 113 loss: 5.185206890106201\n",
      "epoch 40 batch 114 loss: 5.240752220153809\n",
      "epoch 40 batch 115 loss: 5.210018157958984\n",
      "epoch 40 batch 116 loss: 5.163125991821289\n",
      "epoch 40 batch 117 loss: 5.223576545715332\n",
      "epoch 40 batch 118 loss: 5.242726802825928\n",
      "epoch 40 batch 119 loss: 5.210832595825195\n",
      "epoch 40 batch 120 loss: 5.226937770843506\n",
      "epoch 40 batch 121 loss: 5.197094440460205\n",
      "epoch 40 batch 122 loss: 5.173329830169678\n",
      "epoch 40 batch 123 loss: 5.19749116897583\n",
      "epoch 40 batch 124 loss: 5.192393779754639\n",
      "epoch loss: 5.209149749755859\n",
      "epoch 41 batch 0 loss: 5.174910545349121\n",
      "epoch 41 batch 1 loss: 5.221056938171387\n",
      "epoch 41 batch 2 loss: 5.200008869171143\n",
      "epoch 41 batch 3 loss: 5.230649471282959\n",
      "epoch 41 batch 4 loss: 5.20958137512207\n",
      "epoch 41 batch 5 loss: 5.191341400146484\n",
      "epoch 41 batch 6 loss: 5.150585651397705\n",
      "epoch 41 batch 7 loss: 5.190962791442871\n",
      "epoch 41 batch 8 loss: 5.2337565422058105\n",
      "epoch 41 batch 9 loss: 5.187750339508057\n",
      "epoch 41 batch 10 loss: 5.1901092529296875\n",
      "epoch 41 batch 11 loss: 5.188615798950195\n",
      "epoch 41 batch 12 loss: 5.214543342590332\n",
      "epoch 41 batch 13 loss: 5.207441806793213\n",
      "epoch 41 batch 14 loss: 5.194789409637451\n",
      "epoch 41 batch 15 loss: 5.195838451385498\n",
      "epoch 41 batch 16 loss: 5.177501201629639\n",
      "epoch 41 batch 17 loss: 5.179565906524658\n",
      "epoch 41 batch 18 loss: 5.248696327209473\n",
      "epoch 41 batch 19 loss: 5.176450252532959\n",
      "epoch 41 batch 20 loss: 5.195164680480957\n",
      "epoch 41 batch 21 loss: 5.202358722686768\n",
      "epoch 41 batch 22 loss: 5.196890830993652\n",
      "epoch 41 batch 23 loss: 5.190878868103027\n",
      "epoch 41 batch 24 loss: 5.224925518035889\n",
      "epoch 41 batch 25 loss: 5.196405410766602\n",
      "epoch 41 batch 26 loss: 5.208322048187256\n",
      "epoch 41 batch 27 loss: 5.234970569610596\n",
      "epoch 41 batch 28 loss: 5.2243571281433105\n",
      "epoch 41 batch 29 loss: 5.227598667144775\n",
      "epoch 41 batch 30 loss: 5.200443744659424\n",
      "epoch 41 batch 31 loss: 5.210033893585205\n",
      "epoch 41 batch 32 loss: 5.246055603027344\n",
      "epoch 41 batch 33 loss: 5.200422286987305\n",
      "epoch 41 batch 34 loss: 5.206226825714111\n",
      "epoch 41 batch 35 loss: 5.1615447998046875\n",
      "epoch 41 batch 36 loss: 5.200812816619873\n",
      "epoch 41 batch 37 loss: 5.178518772125244\n",
      "epoch 41 batch 38 loss: 5.173612117767334\n",
      "epoch 41 batch 39 loss: 5.224620342254639\n",
      "epoch 41 batch 40 loss: 5.190436363220215\n",
      "epoch 41 batch 41 loss: 5.197948932647705\n",
      "epoch 41 batch 42 loss: 5.161063194274902\n",
      "epoch 41 batch 43 loss: 5.177035331726074\n",
      "epoch 41 batch 44 loss: 5.202187538146973\n",
      "epoch 41 batch 45 loss: 5.1876678466796875\n",
      "epoch 41 batch 46 loss: 5.256276607513428\n",
      "epoch 41 batch 47 loss: 5.211875915527344\n",
      "epoch 41 batch 48 loss: 5.2201762199401855\n",
      "epoch 41 batch 49 loss: 5.18077278137207\n",
      "epoch 41 batch 50 loss: 5.1649394035339355\n",
      "epoch 41 batch 51 loss: 5.229493141174316\n",
      "epoch 41 batch 52 loss: 5.184884071350098\n",
      "epoch 41 batch 53 loss: 5.224335193634033\n",
      "epoch 41 batch 54 loss: 5.207463264465332\n",
      "epoch 41 batch 55 loss: 5.196035861968994\n",
      "epoch 41 batch 56 loss: 5.193602561950684\n",
      "epoch 41 batch 57 loss: 5.232684135437012\n",
      "epoch 41 batch 58 loss: 5.199347019195557\n",
      "epoch 41 batch 59 loss: 5.232494831085205\n",
      "epoch 41 batch 60 loss: 5.234675407409668\n",
      "epoch 41 batch 61 loss: 5.214327812194824\n",
      "epoch 41 batch 62 loss: 5.219544410705566\n",
      "epoch 41 batch 63 loss: 5.216034889221191\n",
      "epoch 41 batch 64 loss: 5.17927885055542\n",
      "epoch 41 batch 65 loss: 5.211116790771484\n",
      "epoch 41 batch 66 loss: 5.194859981536865\n",
      "epoch 41 batch 67 loss: 5.215644359588623\n",
      "epoch 41 batch 68 loss: 5.182032585144043\n",
      "epoch 41 batch 69 loss: 5.2177276611328125\n",
      "epoch 41 batch 70 loss: 5.211241245269775\n",
      "epoch 41 batch 71 loss: 5.176469802856445\n",
      "epoch 41 batch 72 loss: 5.248167514801025\n",
      "epoch 41 batch 73 loss: 5.198083400726318\n",
      "epoch 41 batch 74 loss: 5.220337867736816\n",
      "epoch 41 batch 75 loss: 5.215487957000732\n",
      "epoch 41 batch 76 loss: 5.203348159790039\n",
      "epoch 41 batch 77 loss: 5.173821926116943\n",
      "epoch 41 batch 78 loss: 5.206303596496582\n",
      "epoch 41 batch 79 loss: 5.226877212524414\n",
      "epoch 41 batch 80 loss: 5.187254428863525\n",
      "epoch 41 batch 81 loss: 5.172633171081543\n",
      "epoch 41 batch 82 loss: 5.169730186462402\n",
      "epoch 41 batch 83 loss: 5.198031902313232\n",
      "epoch 41 batch 84 loss: 5.17578125\n",
      "epoch 41 batch 85 loss: 5.212173938751221\n",
      "epoch 41 batch 86 loss: 5.197449207305908\n",
      "epoch 41 batch 87 loss: 5.20203971862793\n",
      "epoch 41 batch 88 loss: 5.234236240386963\n",
      "epoch 41 batch 89 loss: 5.238677978515625\n",
      "epoch 41 batch 90 loss: 5.193112850189209\n",
      "epoch 41 batch 91 loss: 5.2384490966796875\n",
      "epoch 41 batch 92 loss: 5.2386884689331055\n",
      "epoch 41 batch 93 loss: 5.210220813751221\n",
      "epoch 41 batch 94 loss: 5.232479572296143\n",
      "epoch 41 batch 95 loss: 5.198173522949219\n",
      "epoch 41 batch 96 loss: 5.1793212890625\n",
      "epoch 41 batch 97 loss: 5.214086055755615\n",
      "epoch 41 batch 98 loss: 5.230618953704834\n",
      "epoch 41 batch 99 loss: 5.197768211364746\n",
      "epoch 41 batch 100 loss: 5.217207431793213\n",
      "epoch 41 batch 101 loss: 5.171899318695068\n",
      "epoch 41 batch 102 loss: 5.199732780456543\n",
      "epoch 41 batch 103 loss: 5.205468654632568\n",
      "epoch 41 batch 104 loss: 5.2075982093811035\n",
      "epoch 41 batch 105 loss: 5.179910182952881\n",
      "epoch 41 batch 106 loss: 5.223615646362305\n",
      "epoch 41 batch 107 loss: 5.186102390289307\n",
      "epoch 41 batch 108 loss: 5.209064960479736\n",
      "epoch 41 batch 109 loss: 5.1932291984558105\n",
      "epoch 41 batch 110 loss: 5.211541175842285\n",
      "epoch 41 batch 111 loss: 5.184141635894775\n",
      "epoch 41 batch 112 loss: 5.233012676239014\n",
      "epoch 41 batch 113 loss: 5.210638523101807\n",
      "epoch 41 batch 114 loss: 5.217448711395264\n",
      "epoch 41 batch 115 loss: 5.180187702178955\n",
      "epoch 41 batch 116 loss: 5.195084095001221\n",
      "epoch 41 batch 117 loss: 5.19781494140625\n",
      "epoch 41 batch 118 loss: 5.2128825187683105\n",
      "epoch 41 batch 119 loss: 5.193411350250244\n",
      "epoch 41 batch 120 loss: 5.197383880615234\n",
      "epoch 41 batch 121 loss: 5.211040019989014\n",
      "epoch 41 batch 122 loss: 5.238315105438232\n",
      "epoch 41 batch 123 loss: 5.232029914855957\n",
      "epoch 41 batch 124 loss: 5.2291460037231445\n",
      "epoch loss: 5.204346038818359\n",
      "epoch 42 batch 0 loss: 5.21705436706543\n",
      "epoch 42 batch 1 loss: 5.219735145568848\n",
      "epoch 42 batch 2 loss: 5.217540264129639\n",
      "epoch 42 batch 3 loss: 5.245753288269043\n",
      "epoch 42 batch 4 loss: 5.202021598815918\n",
      "epoch 42 batch 5 loss: 5.213736534118652\n",
      "epoch 42 batch 6 loss: 5.224498748779297\n",
      "epoch 42 batch 7 loss: 5.171828269958496\n",
      "epoch 42 batch 8 loss: 5.198403835296631\n",
      "epoch 42 batch 9 loss: 5.181901931762695\n",
      "epoch 42 batch 10 loss: 5.183564186096191\n",
      "epoch 42 batch 11 loss: 5.204988956451416\n",
      "epoch 42 batch 12 loss: 5.204642295837402\n",
      "epoch 42 batch 13 loss: 5.207829475402832\n",
      "epoch 42 batch 14 loss: 5.22943115234375\n",
      "epoch 42 batch 15 loss: 5.205970287322998\n",
      "epoch 42 batch 16 loss: 5.18450927734375\n",
      "epoch 42 batch 17 loss: 5.231783390045166\n",
      "epoch 42 batch 18 loss: 5.2091593742370605\n",
      "epoch 42 batch 19 loss: 5.2035441398620605\n",
      "epoch 42 batch 20 loss: 5.208549499511719\n",
      "epoch 42 batch 21 loss: 5.223679065704346\n",
      "epoch 42 batch 22 loss: 5.186032772064209\n",
      "epoch 42 batch 23 loss: 5.188788890838623\n",
      "epoch 42 batch 24 loss: 5.235156536102295\n",
      "epoch 42 batch 25 loss: 5.170494079589844\n",
      "epoch 42 batch 26 loss: 5.228726863861084\n",
      "epoch 42 batch 27 loss: 5.220062732696533\n",
      "epoch 42 batch 28 loss: 5.208752155303955\n",
      "epoch 42 batch 29 loss: 5.178797721862793\n",
      "epoch 42 batch 30 loss: 5.179512977600098\n",
      "epoch 42 batch 31 loss: 5.20094108581543\n",
      "epoch 42 batch 32 loss: 5.2275896072387695\n",
      "epoch 42 batch 33 loss: 5.218108177185059\n",
      "epoch 42 batch 34 loss: 5.207421779632568\n",
      "epoch 42 batch 35 loss: 5.22156286239624\n",
      "epoch 42 batch 36 loss: 5.201918125152588\n",
      "epoch 42 batch 37 loss: 5.19290018081665\n",
      "epoch 42 batch 38 loss: 5.205711364746094\n",
      "epoch 42 batch 39 loss: 5.21205472946167\n",
      "epoch 42 batch 40 loss: 5.2140960693359375\n",
      "epoch 42 batch 41 loss: 5.207873344421387\n",
      "epoch 42 batch 42 loss: 5.214241981506348\n",
      "epoch 42 batch 43 loss: 5.201072692871094\n",
      "epoch 42 batch 44 loss: 5.205668926239014\n",
      "epoch 42 batch 45 loss: 5.184438228607178\n",
      "epoch 42 batch 46 loss: 5.172552585601807\n",
      "epoch 42 batch 47 loss: 5.214186668395996\n",
      "epoch 42 batch 48 loss: 5.182125568389893\n",
      "epoch 42 batch 49 loss: 5.194800853729248\n",
      "epoch 42 batch 50 loss: 5.172173976898193\n",
      "epoch 42 batch 51 loss: 5.180729866027832\n",
      "epoch 42 batch 52 loss: 5.198998928070068\n",
      "epoch 42 batch 53 loss: 5.193459510803223\n",
      "epoch 42 batch 54 loss: 5.2119140625\n",
      "epoch 42 batch 55 loss: 5.180488109588623\n",
      "epoch 42 batch 56 loss: 5.214861869812012\n",
      "epoch 42 batch 57 loss: 5.205002784729004\n",
      "epoch 42 batch 58 loss: 5.1869330406188965\n",
      "epoch 42 batch 59 loss: 5.22508430480957\n",
      "epoch 42 batch 60 loss: 5.237545013427734\n",
      "epoch 42 batch 61 loss: 5.230452537536621\n",
      "epoch 42 batch 62 loss: 5.220283031463623\n",
      "epoch 42 batch 63 loss: 5.191107749938965\n",
      "epoch 42 batch 64 loss: 5.211706638336182\n",
      "epoch 42 batch 65 loss: 5.157684326171875\n",
      "epoch 42 batch 66 loss: 5.183121204376221\n",
      "epoch 42 batch 67 loss: 5.19731330871582\n",
      "epoch 42 batch 68 loss: 5.227009296417236\n",
      "epoch 42 batch 69 loss: 5.196838855743408\n",
      "epoch 42 batch 70 loss: 5.212161540985107\n",
      "epoch 42 batch 71 loss: 5.204953670501709\n",
      "epoch 42 batch 72 loss: 5.23118257522583\n",
      "epoch 42 batch 73 loss: 5.160263538360596\n",
      "epoch 42 batch 74 loss: 5.170961856842041\n",
      "epoch 42 batch 75 loss: 5.236237525939941\n",
      "epoch 42 batch 76 loss: 5.2012200355529785\n",
      "epoch 42 batch 77 loss: 5.224348068237305\n",
      "epoch 42 batch 78 loss: 5.231568336486816\n",
      "epoch 42 batch 79 loss: 5.18351936340332\n",
      "epoch 42 batch 80 loss: 5.193458557128906\n",
      "epoch 42 batch 81 loss: 5.185069561004639\n",
      "epoch 42 batch 82 loss: 5.205259323120117\n",
      "epoch 42 batch 83 loss: 5.202770709991455\n",
      "epoch 42 batch 84 loss: 5.174983501434326\n",
      "epoch 42 batch 85 loss: 5.199440956115723\n",
      "epoch 42 batch 86 loss: 5.223299026489258\n",
      "epoch 42 batch 87 loss: 5.1887407302856445\n",
      "epoch 42 batch 88 loss: 5.175814151763916\n",
      "epoch 42 batch 89 loss: 5.186160564422607\n",
      "epoch 42 batch 90 loss: 5.1854071617126465\n",
      "epoch 42 batch 91 loss: 5.197165489196777\n",
      "epoch 42 batch 92 loss: 5.201043128967285\n",
      "epoch 42 batch 93 loss: 5.219140529632568\n",
      "epoch 42 batch 94 loss: 5.167960166931152\n",
      "epoch 42 batch 95 loss: 5.203591346740723\n",
      "epoch 42 batch 96 loss: 5.220900535583496\n",
      "epoch 42 batch 97 loss: 5.177510738372803\n",
      "epoch 42 batch 98 loss: 5.244754314422607\n",
      "epoch 42 batch 99 loss: 5.196601390838623\n",
      "epoch 42 batch 100 loss: 5.248527526855469\n",
      "epoch 42 batch 101 loss: 5.230342864990234\n",
      "epoch 42 batch 102 loss: 5.186328411102295\n",
      "epoch 42 batch 103 loss: 5.173361301422119\n",
      "epoch 42 batch 104 loss: 5.202270984649658\n",
      "epoch 42 batch 105 loss: 5.212564468383789\n",
      "epoch 42 batch 106 loss: 5.209043025970459\n",
      "epoch 42 batch 107 loss: 5.197739124298096\n",
      "epoch 42 batch 108 loss: 5.208018779754639\n",
      "epoch 42 batch 109 loss: 5.176711082458496\n",
      "epoch 42 batch 110 loss: 5.160621166229248\n",
      "epoch 42 batch 111 loss: 5.2040276527404785\n",
      "epoch 42 batch 112 loss: 5.2237443923950195\n",
      "epoch 42 batch 113 loss: 5.173645973205566\n",
      "epoch 42 batch 114 loss: 5.248156547546387\n",
      "epoch 42 batch 115 loss: 5.199883937835693\n",
      "epoch 42 batch 116 loss: 5.1670637130737305\n",
      "epoch 42 batch 117 loss: 5.223443984985352\n",
      "epoch 42 batch 118 loss: 5.198542594909668\n",
      "epoch 42 batch 119 loss: 5.194792747497559\n",
      "epoch 42 batch 120 loss: 5.221827983856201\n",
      "epoch 42 batch 121 loss: 5.184863090515137\n",
      "epoch 42 batch 122 loss: 5.217261791229248\n",
      "epoch 42 batch 123 loss: 5.189919948577881\n",
      "epoch 42 batch 124 loss: 5.225348472595215\n",
      "epoch loss: 5.202975704193115\n",
      "epoch 43 batch 0 loss: 5.199460506439209\n",
      "epoch 43 batch 1 loss: 5.187805652618408\n",
      "epoch 43 batch 2 loss: 5.209880828857422\n",
      "epoch 43 batch 3 loss: 5.220788478851318\n",
      "epoch 43 batch 4 loss: 5.2149577140808105\n",
      "epoch 43 batch 5 loss: 5.196378231048584\n",
      "epoch 43 batch 6 loss: 5.25486421585083\n",
      "epoch 43 batch 7 loss: 5.189727783203125\n",
      "epoch 43 batch 8 loss: 5.196084022521973\n",
      "epoch 43 batch 9 loss: 5.193044185638428\n",
      "epoch 43 batch 10 loss: 5.1924357414245605\n",
      "epoch 43 batch 11 loss: 5.1975579261779785\n",
      "epoch 43 batch 12 loss: 5.201899528503418\n",
      "epoch 43 batch 13 loss: 5.229106903076172\n",
      "epoch 43 batch 14 loss: 5.198199272155762\n",
      "epoch 43 batch 15 loss: 5.259490013122559\n",
      "epoch 43 batch 16 loss: 5.1664934158325195\n",
      "epoch 43 batch 17 loss: 5.214081764221191\n",
      "epoch 43 batch 18 loss: 5.162624359130859\n",
      "epoch 43 batch 19 loss: 5.193566799163818\n",
      "epoch 43 batch 20 loss: 5.228774070739746\n",
      "epoch 43 batch 21 loss: 5.214131832122803\n",
      "epoch 43 batch 22 loss: 5.208427429199219\n",
      "epoch 43 batch 23 loss: 5.177437782287598\n",
      "epoch 43 batch 24 loss: 5.210293292999268\n",
      "epoch 43 batch 25 loss: 5.218451976776123\n",
      "epoch 43 batch 26 loss: 5.201959133148193\n",
      "epoch 43 batch 27 loss: 5.177490711212158\n",
      "epoch 43 batch 28 loss: 5.2198944091796875\n",
      "epoch 43 batch 29 loss: 5.221524715423584\n",
      "epoch 43 batch 30 loss: 5.177559852600098\n",
      "epoch 43 batch 31 loss: 5.214434623718262\n",
      "epoch 43 batch 32 loss: 5.1898956298828125\n",
      "epoch 43 batch 33 loss: 5.221695423126221\n",
      "epoch 43 batch 34 loss: 5.210896968841553\n",
      "epoch 43 batch 35 loss: 5.237144470214844\n",
      "epoch 43 batch 36 loss: 5.170123100280762\n",
      "epoch 43 batch 37 loss: 5.198503494262695\n",
      "epoch 43 batch 38 loss: 5.2190775871276855\n",
      "epoch 43 batch 39 loss: 5.196274280548096\n",
      "epoch 43 batch 40 loss: 5.211110591888428\n",
      "epoch 43 batch 41 loss: 5.209022045135498\n",
      "epoch 43 batch 42 loss: 5.23018741607666\n",
      "epoch 43 batch 43 loss: 5.20461893081665\n",
      "epoch 43 batch 44 loss: 5.197059154510498\n",
      "epoch 43 batch 45 loss: 5.177238464355469\n",
      "epoch 43 batch 46 loss: 5.198111534118652\n",
      "epoch 43 batch 47 loss: 5.210216999053955\n",
      "epoch 43 batch 48 loss: 5.187993049621582\n",
      "epoch 43 batch 49 loss: 5.243843078613281\n",
      "epoch 43 batch 50 loss: 5.20472526550293\n",
      "epoch 43 batch 51 loss: 5.207626819610596\n",
      "epoch 43 batch 52 loss: 5.2083916664123535\n",
      "epoch 43 batch 53 loss: 5.1794352531433105\n",
      "epoch 43 batch 54 loss: 5.2078537940979\n",
      "epoch 43 batch 55 loss: 5.240801811218262\n",
      "epoch 43 batch 56 loss: 5.203298091888428\n",
      "epoch 43 batch 57 loss: 5.195512771606445\n",
      "epoch 43 batch 58 loss: 5.225869178771973\n",
      "epoch 43 batch 59 loss: 5.220600605010986\n",
      "epoch 43 batch 60 loss: 5.1961669921875\n",
      "epoch 43 batch 61 loss: 5.208897590637207\n",
      "epoch 43 batch 62 loss: 5.198921203613281\n",
      "epoch 43 batch 63 loss: 5.231551170349121\n",
      "epoch 43 batch 64 loss: 5.212347984313965\n",
      "epoch 43 batch 65 loss: 5.224091529846191\n",
      "epoch 43 batch 66 loss: 5.2264509201049805\n",
      "epoch 43 batch 67 loss: 5.190305233001709\n",
      "epoch 43 batch 68 loss: 5.205015659332275\n",
      "epoch 43 batch 69 loss: 5.209871768951416\n",
      "epoch 43 batch 70 loss: 5.175124168395996\n",
      "epoch 43 batch 71 loss: 5.2048516273498535\n",
      "epoch 43 batch 72 loss: 5.201872825622559\n",
      "epoch 43 batch 73 loss: 5.211277961730957\n",
      "epoch 43 batch 74 loss: 5.205226898193359\n",
      "epoch 43 batch 75 loss: 5.161528587341309\n",
      "epoch 43 batch 76 loss: 5.218916893005371\n",
      "epoch 43 batch 77 loss: 5.210392951965332\n",
      "epoch 43 batch 78 loss: 5.167045593261719\n",
      "epoch 43 batch 79 loss: 5.199282646179199\n",
      "epoch 43 batch 80 loss: 5.170066833496094\n",
      "epoch 43 batch 81 loss: 5.199203968048096\n",
      "epoch 43 batch 82 loss: 5.191647052764893\n",
      "epoch 43 batch 83 loss: 5.198878288269043\n",
      "epoch 43 batch 84 loss: 5.199372291564941\n",
      "epoch 43 batch 85 loss: 5.202024936676025\n",
      "epoch 43 batch 86 loss: 5.196540355682373\n",
      "epoch 43 batch 87 loss: 5.193730354309082\n",
      "epoch 43 batch 88 loss: 5.2230987548828125\n",
      "epoch 43 batch 89 loss: 5.199234485626221\n",
      "epoch 43 batch 90 loss: 5.231082439422607\n",
      "epoch 43 batch 91 loss: 5.243220329284668\n",
      "epoch 43 batch 92 loss: 5.212957859039307\n",
      "epoch 43 batch 93 loss: 5.221732139587402\n",
      "epoch 43 batch 94 loss: 5.197067737579346\n",
      "epoch 43 batch 95 loss: 5.230974197387695\n",
      "epoch 43 batch 96 loss: 5.200026512145996\n",
      "epoch 43 batch 97 loss: 5.21041202545166\n",
      "epoch 43 batch 98 loss: 5.226507663726807\n",
      "epoch 43 batch 99 loss: 5.18101692199707\n",
      "epoch 43 batch 100 loss: 5.241451263427734\n",
      "epoch 43 batch 101 loss: 5.191826820373535\n",
      "epoch 43 batch 102 loss: 5.247950553894043\n",
      "epoch 43 batch 103 loss: 5.204221248626709\n",
      "epoch 43 batch 104 loss: 5.214119911193848\n",
      "epoch 43 batch 105 loss: 5.195306301116943\n",
      "epoch 43 batch 106 loss: 5.18870210647583\n",
      "epoch 43 batch 107 loss: 5.225034236907959\n",
      "epoch 43 batch 108 loss: 5.22513484954834\n",
      "epoch 43 batch 109 loss: 5.224338054656982\n",
      "epoch 43 batch 110 loss: 5.17323112487793\n",
      "epoch 43 batch 111 loss: 5.201684474945068\n",
      "epoch 43 batch 112 loss: 5.220666885375977\n",
      "epoch 43 batch 113 loss: 5.204123497009277\n",
      "epoch 43 batch 114 loss: 5.211678504943848\n",
      "epoch 43 batch 115 loss: 5.197867393493652\n",
      "epoch 43 batch 116 loss: 5.212486267089844\n",
      "epoch 43 batch 117 loss: 5.208017349243164\n",
      "epoch 43 batch 118 loss: 5.173094272613525\n",
      "epoch 43 batch 119 loss: 5.249933242797852\n",
      "epoch 43 batch 120 loss: 5.224427223205566\n",
      "epoch 43 batch 121 loss: 5.222334384918213\n",
      "epoch 43 batch 122 loss: 5.1830010414123535\n",
      "epoch 43 batch 123 loss: 5.189213275909424\n",
      "epoch 43 batch 124 loss: 5.1791672706604\n",
      "epoch loss: 5.206023212432862\n",
      "epoch 44 batch 0 loss: 5.192065238952637\n",
      "epoch 44 batch 1 loss: 5.214835166931152\n",
      "epoch 44 batch 2 loss: 5.203555583953857\n",
      "epoch 44 batch 3 loss: 5.175973892211914\n",
      "epoch 44 batch 4 loss: 5.2109293937683105\n",
      "epoch 44 batch 5 loss: 5.207669734954834\n",
      "epoch 44 batch 6 loss: 5.228701114654541\n",
      "epoch 44 batch 7 loss: 5.206724643707275\n",
      "epoch 44 batch 8 loss: 5.195609092712402\n",
      "epoch 44 batch 9 loss: 5.203240394592285\n",
      "epoch 44 batch 10 loss: 5.229251384735107\n",
      "epoch 44 batch 11 loss: 5.205967903137207\n",
      "epoch 44 batch 12 loss: 5.171487331390381\n",
      "epoch 44 batch 13 loss: 5.168705940246582\n",
      "epoch 44 batch 14 loss: 5.228718280792236\n",
      "epoch 44 batch 15 loss: 5.196908473968506\n",
      "epoch 44 batch 16 loss: 5.2142462730407715\n",
      "epoch 44 batch 17 loss: 5.2193121910095215\n",
      "epoch 44 batch 18 loss: 5.171905517578125\n",
      "epoch 44 batch 19 loss: 5.19102668762207\n",
      "epoch 44 batch 20 loss: 5.196776390075684\n",
      "epoch 44 batch 21 loss: 5.248010635375977\n",
      "epoch 44 batch 22 loss: 5.218800067901611\n",
      "epoch 44 batch 23 loss: 5.241261959075928\n",
      "epoch 44 batch 24 loss: 5.168980598449707\n",
      "epoch 44 batch 25 loss: 5.161250591278076\n",
      "epoch 44 batch 26 loss: 5.215368747711182\n",
      "epoch 44 batch 27 loss: 5.203790187835693\n",
      "epoch 44 batch 28 loss: 5.160712242126465\n",
      "epoch 44 batch 29 loss: 5.212481498718262\n",
      "epoch 44 batch 30 loss: 5.20283317565918\n",
      "epoch 44 batch 31 loss: 5.229898452758789\n",
      "epoch 44 batch 32 loss: 5.205857276916504\n",
      "epoch 44 batch 33 loss: 5.212483882904053\n",
      "epoch 44 batch 34 loss: 5.15031623840332\n",
      "epoch 44 batch 35 loss: 5.192352294921875\n",
      "epoch 44 batch 36 loss: 5.184226036071777\n",
      "epoch 44 batch 37 loss: 5.206303119659424\n",
      "epoch 44 batch 38 loss: 5.157582759857178\n",
      "epoch 44 batch 39 loss: 5.168989181518555\n",
      "epoch 44 batch 40 loss: 5.203757286071777\n",
      "epoch 44 batch 41 loss: 5.177016258239746\n",
      "epoch 44 batch 42 loss: 5.218869686126709\n",
      "epoch 44 batch 43 loss: 5.2188520431518555\n",
      "epoch 44 batch 44 loss: 5.223603248596191\n",
      "epoch 44 batch 45 loss: 5.222232341766357\n",
      "epoch 44 batch 46 loss: 5.240840435028076\n",
      "epoch 44 batch 47 loss: 5.218763828277588\n",
      "epoch 44 batch 48 loss: 5.204231262207031\n",
      "epoch 44 batch 49 loss: 5.170782566070557\n",
      "epoch 44 batch 50 loss: 5.217534065246582\n",
      "epoch 44 batch 51 loss: 5.214417457580566\n",
      "epoch 44 batch 52 loss: 5.189265251159668\n",
      "epoch 44 batch 53 loss: 5.220363616943359\n",
      "epoch 44 batch 54 loss: 5.2068705558776855\n",
      "epoch 44 batch 55 loss: 5.199997425079346\n",
      "epoch 44 batch 56 loss: 5.204413414001465\n",
      "epoch 44 batch 57 loss: 5.199105262756348\n",
      "epoch 44 batch 58 loss: 5.176671028137207\n",
      "epoch 44 batch 59 loss: 5.204915523529053\n",
      "epoch 44 batch 60 loss: 5.199317455291748\n",
      "epoch 44 batch 61 loss: 5.214935302734375\n",
      "epoch 44 batch 62 loss: 5.227728366851807\n",
      "epoch 44 batch 63 loss: 5.212551116943359\n",
      "epoch 44 batch 64 loss: 5.192980766296387\n",
      "epoch 44 batch 65 loss: 5.205795764923096\n",
      "epoch 44 batch 66 loss: 5.199565887451172\n",
      "epoch 44 batch 67 loss: 5.214388847351074\n",
      "epoch 44 batch 68 loss: 5.217858791351318\n",
      "epoch 44 batch 69 loss: 5.183948993682861\n",
      "epoch 44 batch 70 loss: 5.160394191741943\n",
      "epoch 44 batch 71 loss: 5.227944374084473\n",
      "epoch 44 batch 72 loss: 5.192394733428955\n",
      "epoch 44 batch 73 loss: 5.20364236831665\n",
      "epoch 44 batch 74 loss: 5.216582298278809\n",
      "epoch 44 batch 75 loss: 5.196684837341309\n",
      "epoch 44 batch 76 loss: 5.199898719787598\n",
      "epoch 44 batch 77 loss: 5.190052509307861\n",
      "epoch 44 batch 78 loss: 5.212060928344727\n",
      "epoch 44 batch 79 loss: 5.197654724121094\n",
      "epoch 44 batch 80 loss: 5.2296342849731445\n",
      "epoch 44 batch 81 loss: 5.194070339202881\n",
      "epoch 44 batch 82 loss: 5.179222106933594\n",
      "epoch 44 batch 83 loss: 5.197444438934326\n",
      "epoch 44 batch 84 loss: 5.17624568939209\n",
      "epoch 44 batch 85 loss: 5.214591979980469\n",
      "epoch 44 batch 86 loss: 5.175704002380371\n",
      "epoch 44 batch 87 loss: 5.192741394042969\n",
      "epoch 44 batch 88 loss: 5.207675457000732\n",
      "epoch 44 batch 89 loss: 5.236605167388916\n",
      "epoch 44 batch 90 loss: 5.187099456787109\n",
      "epoch 44 batch 91 loss: 5.220822811126709\n",
      "epoch 44 batch 92 loss: 5.204190731048584\n",
      "epoch 44 batch 93 loss: 5.186679363250732\n",
      "epoch 44 batch 94 loss: 5.201714992523193\n",
      "epoch 44 batch 95 loss: 5.201230525970459\n",
      "epoch 44 batch 96 loss: 5.224710464477539\n",
      "epoch 44 batch 97 loss: 5.183480262756348\n",
      "epoch 44 batch 98 loss: 5.202536582946777\n",
      "epoch 44 batch 99 loss: 5.167722225189209\n",
      "epoch 44 batch 100 loss: 5.158610820770264\n",
      "epoch 44 batch 101 loss: 5.201605796813965\n",
      "epoch 44 batch 102 loss: 5.2387847900390625\n",
      "epoch 44 batch 103 loss: 5.220561504364014\n",
      "epoch 44 batch 104 loss: 5.200069904327393\n",
      "epoch 44 batch 105 loss: 5.180712699890137\n",
      "epoch 44 batch 106 loss: 5.196553707122803\n",
      "epoch 44 batch 107 loss: 5.225739002227783\n",
      "epoch 44 batch 108 loss: 5.180787086486816\n",
      "epoch 44 batch 109 loss: 5.218745231628418\n",
      "epoch 44 batch 110 loss: 5.185052394866943\n",
      "epoch 44 batch 111 loss: 5.202308654785156\n",
      "epoch 44 batch 112 loss: 5.2025146484375\n",
      "epoch 44 batch 113 loss: 5.17659330368042\n",
      "epoch 44 batch 114 loss: 5.209059715270996\n",
      "epoch 44 batch 115 loss: 5.214101314544678\n",
      "epoch 44 batch 116 loss: 5.2122721672058105\n",
      "epoch 44 batch 117 loss: 5.195956707000732\n",
      "epoch 44 batch 118 loss: 5.223762035369873\n",
      "epoch 44 batch 119 loss: 5.200599670410156\n",
      "epoch 44 batch 120 loss: 5.186967849731445\n",
      "epoch 44 batch 121 loss: 5.202615737915039\n",
      "epoch 44 batch 122 loss: 5.251957416534424\n",
      "epoch 44 batch 123 loss: 5.216850280761719\n",
      "epoch 44 batch 124 loss: 5.167367935180664\n",
      "epoch loss: 5.2018261260986325\n",
      "epoch 45 batch 0 loss: 5.181491374969482\n",
      "epoch 45 batch 1 loss: 5.198565483093262\n",
      "epoch 45 batch 2 loss: 5.206659317016602\n",
      "epoch 45 batch 3 loss: 5.205619812011719\n",
      "epoch 45 batch 4 loss: 5.209732532501221\n",
      "epoch 45 batch 5 loss: 5.18681001663208\n",
      "epoch 45 batch 6 loss: 5.2156476974487305\n",
      "epoch 45 batch 7 loss: 5.166680335998535\n",
      "epoch 45 batch 8 loss: 5.208221435546875\n",
      "epoch 45 batch 9 loss: 5.20701265335083\n",
      "epoch 45 batch 10 loss: 5.2306389808654785\n",
      "epoch 45 batch 11 loss: 5.204031944274902\n",
      "epoch 45 batch 12 loss: 5.192327976226807\n",
      "epoch 45 batch 13 loss: 5.193259239196777\n",
      "epoch 45 batch 14 loss: 5.167254447937012\n",
      "epoch 45 batch 15 loss: 5.163733005523682\n",
      "epoch 45 batch 16 loss: 5.201793670654297\n",
      "epoch 45 batch 17 loss: 5.169455051422119\n",
      "epoch 45 batch 18 loss: 5.190916061401367\n",
      "epoch 45 batch 19 loss: 5.184333324432373\n",
      "epoch 45 batch 20 loss: 5.218233108520508\n",
      "epoch 45 batch 21 loss: 5.187700271606445\n",
      "epoch 45 batch 22 loss: 5.226733684539795\n",
      "epoch 45 batch 23 loss: 5.167189121246338\n",
      "epoch 45 batch 24 loss: 5.1922607421875\n",
      "epoch 45 batch 25 loss: 5.162057399749756\n",
      "epoch 45 batch 26 loss: 5.220931053161621\n",
      "epoch 45 batch 27 loss: 5.175601005554199\n",
      "epoch 45 batch 28 loss: 5.196712493896484\n",
      "epoch 45 batch 29 loss: 5.206419467926025\n",
      "epoch 45 batch 30 loss: 5.1913580894470215\n",
      "epoch 45 batch 31 loss: 5.144697189331055\n",
      "epoch 45 batch 32 loss: 5.140401363372803\n",
      "epoch 45 batch 33 loss: 5.203668594360352\n",
      "epoch 45 batch 34 loss: 5.190685272216797\n",
      "epoch 45 batch 35 loss: 5.210626125335693\n",
      "epoch 45 batch 36 loss: 5.1749491691589355\n",
      "epoch 45 batch 37 loss: 5.1772613525390625\n",
      "epoch 45 batch 38 loss: 5.227731227874756\n",
      "epoch 45 batch 39 loss: 5.173559665679932\n",
      "epoch 45 batch 40 loss: 5.190500259399414\n",
      "epoch 45 batch 41 loss: 5.181335926055908\n",
      "epoch 45 batch 42 loss: 5.175196647644043\n",
      "epoch 45 batch 43 loss: 5.180609226226807\n",
      "epoch 45 batch 44 loss: 5.1924729347229\n",
      "epoch 45 batch 45 loss: 5.18555212020874\n",
      "epoch 45 batch 46 loss: 5.209719657897949\n",
      "epoch 45 batch 47 loss: 5.183743000030518\n",
      "epoch 45 batch 48 loss: 5.193217754364014\n",
      "epoch 45 batch 49 loss: 5.161019325256348\n",
      "epoch 45 batch 50 loss: 5.195214748382568\n",
      "epoch 45 batch 51 loss: 5.202363967895508\n",
      "epoch 45 batch 52 loss: 5.200125694274902\n",
      "epoch 45 batch 53 loss: 5.208932876586914\n",
      "epoch 45 batch 54 loss: 5.219050407409668\n",
      "epoch 45 batch 55 loss: 5.195058345794678\n",
      "epoch 45 batch 56 loss: 5.191153526306152\n",
      "epoch 45 batch 57 loss: 5.230301380157471\n",
      "epoch 45 batch 58 loss: 5.190975189208984\n",
      "epoch 45 batch 59 loss: 5.168349266052246\n",
      "epoch 45 batch 60 loss: 5.1607279777526855\n",
      "epoch 45 batch 61 loss: 5.19587516784668\n",
      "epoch 45 batch 62 loss: 5.232483863830566\n",
      "epoch 45 batch 63 loss: 5.1563720703125\n",
      "epoch 45 batch 64 loss: 5.217106819152832\n",
      "epoch 45 batch 65 loss: 5.237069606781006\n",
      "epoch 45 batch 66 loss: 5.184659481048584\n",
      "epoch 45 batch 67 loss: 5.2362213134765625\n",
      "epoch 45 batch 68 loss: 5.1819024085998535\n",
      "epoch 45 batch 69 loss: 5.200753688812256\n",
      "epoch 45 batch 70 loss: 5.190155982971191\n",
      "epoch 45 batch 71 loss: 5.181147575378418\n",
      "epoch 45 batch 72 loss: 5.203237056732178\n",
      "epoch 45 batch 73 loss: 5.204683303833008\n",
      "epoch 45 batch 74 loss: 5.2126078605651855\n",
      "epoch 45 batch 75 loss: 5.188281536102295\n",
      "epoch 45 batch 76 loss: 5.21672248840332\n",
      "epoch 45 batch 77 loss: 5.223504543304443\n",
      "epoch 45 batch 78 loss: 5.1890869140625\n",
      "epoch 45 batch 79 loss: 5.1989006996154785\n",
      "epoch 45 batch 80 loss: 5.199823379516602\n",
      "epoch 45 batch 81 loss: 5.190985679626465\n",
      "epoch 45 batch 82 loss: 5.199410915374756\n",
      "epoch 45 batch 83 loss: 5.2064056396484375\n",
      "epoch 45 batch 84 loss: 5.153987884521484\n",
      "epoch 45 batch 85 loss: 5.2496185302734375\n",
      "epoch 45 batch 86 loss: 5.2361674308776855\n",
      "epoch 45 batch 87 loss: 5.185085773468018\n",
      "epoch 45 batch 88 loss: 5.195214748382568\n",
      "epoch 45 batch 89 loss: 5.227243423461914\n",
      "epoch 45 batch 90 loss: 5.208174705505371\n",
      "epoch 45 batch 91 loss: 5.17601203918457\n",
      "epoch 45 batch 92 loss: 5.194138050079346\n",
      "epoch 45 batch 93 loss: 5.139164924621582\n",
      "epoch 45 batch 94 loss: 5.194262504577637\n",
      "epoch 45 batch 95 loss: 5.171023845672607\n",
      "epoch 45 batch 96 loss: 5.202858924865723\n",
      "epoch 45 batch 97 loss: 5.220226287841797\n",
      "epoch 45 batch 98 loss: 5.190591812133789\n",
      "epoch 45 batch 99 loss: 5.180653095245361\n",
      "epoch 45 batch 100 loss: 5.179538249969482\n",
      "epoch 45 batch 101 loss: 5.184327125549316\n",
      "epoch 45 batch 102 loss: 5.229516506195068\n",
      "epoch 45 batch 103 loss: 5.183178424835205\n",
      "epoch 45 batch 104 loss: 5.1799750328063965\n",
      "epoch 45 batch 105 loss: 5.17365837097168\n",
      "epoch 45 batch 106 loss: 5.178250789642334\n",
      "epoch 45 batch 107 loss: 5.243668079376221\n",
      "epoch 45 batch 108 loss: 5.23793363571167\n",
      "epoch 45 batch 109 loss: 5.189377307891846\n",
      "epoch 45 batch 110 loss: 5.206353187561035\n",
      "epoch 45 batch 111 loss: 5.247684955596924\n",
      "epoch 45 batch 112 loss: 5.208324909210205\n",
      "epoch 45 batch 113 loss: 5.186152935028076\n",
      "epoch 45 batch 114 loss: 5.181926727294922\n",
      "epoch 45 batch 115 loss: 5.20149040222168\n",
      "epoch 45 batch 116 loss: 5.210947036743164\n",
      "epoch 45 batch 117 loss: 5.216952800750732\n",
      "epoch 45 batch 118 loss: 5.212585926055908\n",
      "epoch 45 batch 119 loss: 5.19826078414917\n",
      "epoch 45 batch 120 loss: 5.173649787902832\n",
      "epoch 45 batch 121 loss: 5.164767742156982\n",
      "epoch 45 batch 122 loss: 5.187106609344482\n",
      "epoch 45 batch 123 loss: 5.176172256469727\n",
      "epoch 45 batch 124 loss: 5.15590763092041\n",
      "epoch loss: 5.194944881439209\n",
      "epoch 46 batch 0 loss: 5.167201995849609\n",
      "epoch 46 batch 1 loss: 5.188363552093506\n",
      "epoch 46 batch 2 loss: 5.1582255363464355\n",
      "epoch 46 batch 3 loss: 5.169892311096191\n",
      "epoch 46 batch 4 loss: 5.187662124633789\n",
      "epoch 46 batch 5 loss: 5.17510986328125\n",
      "epoch 46 batch 6 loss: 5.1960320472717285\n",
      "epoch 46 batch 7 loss: 5.231123924255371\n",
      "epoch 46 batch 8 loss: 5.217761516571045\n",
      "epoch 46 batch 9 loss: 5.214048385620117\n",
      "epoch 46 batch 10 loss: 5.164629936218262\n",
      "epoch 46 batch 11 loss: 5.234089374542236\n",
      "epoch 46 batch 12 loss: 5.193285942077637\n",
      "epoch 46 batch 13 loss: 5.173037052154541\n",
      "epoch 46 batch 14 loss: 5.20906925201416\n",
      "epoch 46 batch 15 loss: 5.2378830909729\n",
      "epoch 46 batch 16 loss: 5.219939231872559\n",
      "epoch 46 batch 17 loss: 5.204335689544678\n",
      "epoch 46 batch 18 loss: 5.179328918457031\n",
      "epoch 46 batch 19 loss: 5.198579788208008\n",
      "epoch 46 batch 20 loss: 5.233325481414795\n",
      "epoch 46 batch 21 loss: 5.179755687713623\n",
      "epoch 46 batch 22 loss: 5.181144714355469\n",
      "epoch 46 batch 23 loss: 5.200163841247559\n",
      "epoch 46 batch 24 loss: 5.211772918701172\n",
      "epoch 46 batch 25 loss: 5.20596170425415\n",
      "epoch 46 batch 26 loss: 5.228276252746582\n",
      "epoch 46 batch 27 loss: 5.185085773468018\n",
      "epoch 46 batch 28 loss: 5.19514274597168\n",
      "epoch 46 batch 29 loss: 5.173338413238525\n",
      "epoch 46 batch 30 loss: 5.177554607391357\n",
      "epoch 46 batch 31 loss: 5.187655925750732\n",
      "epoch 46 batch 32 loss: 5.212496757507324\n",
      "epoch 46 batch 33 loss: 5.173851013183594\n",
      "epoch 46 batch 34 loss: 5.186443328857422\n",
      "epoch 46 batch 35 loss: 5.199066162109375\n",
      "epoch 46 batch 36 loss: 5.245789051055908\n",
      "epoch 46 batch 37 loss: 5.216598987579346\n",
      "epoch 46 batch 38 loss: 5.165817737579346\n",
      "epoch 46 batch 39 loss: 5.17446756362915\n",
      "epoch 46 batch 40 loss: 5.2034592628479\n",
      "epoch 46 batch 41 loss: 5.229492664337158\n",
      "epoch 46 batch 42 loss: 5.171424388885498\n",
      "epoch 46 batch 43 loss: 5.253939151763916\n",
      "epoch 46 batch 44 loss: 5.219738483428955\n",
      "epoch 46 batch 45 loss: 5.182724475860596\n",
      "epoch 46 batch 46 loss: 5.175905227661133\n",
      "epoch 46 batch 47 loss: 5.20483922958374\n",
      "epoch 46 batch 48 loss: 5.171975612640381\n",
      "epoch 46 batch 49 loss: 5.1638946533203125\n",
      "epoch 46 batch 50 loss: 5.183879375457764\n",
      "epoch 46 batch 51 loss: 5.208934307098389\n",
      "epoch 46 batch 52 loss: 5.201638698577881\n",
      "epoch 46 batch 53 loss: 5.199391841888428\n",
      "epoch 46 batch 54 loss: 5.2028069496154785\n",
      "epoch 46 batch 55 loss: 5.221472263336182\n",
      "epoch 46 batch 56 loss: 5.1866068840026855\n",
      "epoch 46 batch 57 loss: 5.180440425872803\n",
      "epoch 46 batch 58 loss: 5.233206748962402\n",
      "epoch 46 batch 59 loss: 5.184795379638672\n",
      "epoch 46 batch 60 loss: 5.2116851806640625\n",
      "epoch 46 batch 61 loss: 5.168354511260986\n",
      "epoch 46 batch 62 loss: 5.205593109130859\n",
      "epoch 46 batch 63 loss: 5.21006965637207\n",
      "epoch 46 batch 64 loss: 5.1736345291137695\n",
      "epoch 46 batch 65 loss: 5.209220886230469\n",
      "epoch 46 batch 66 loss: 5.212189674377441\n",
      "epoch 46 batch 67 loss: 5.17935037612915\n",
      "epoch 46 batch 68 loss: 5.229424953460693\n",
      "epoch 46 batch 69 loss: 5.188097953796387\n",
      "epoch 46 batch 70 loss: 5.178396701812744\n",
      "epoch 46 batch 71 loss: 5.247676849365234\n",
      "epoch 46 batch 72 loss: 5.211029052734375\n",
      "epoch 46 batch 73 loss: 5.17759370803833\n",
      "epoch 46 batch 74 loss: 5.21693229675293\n",
      "epoch 46 batch 75 loss: 5.209827899932861\n",
      "epoch 46 batch 76 loss: 5.1717023849487305\n",
      "epoch 46 batch 77 loss: 5.2114481925964355\n",
      "epoch 46 batch 78 loss: 5.202680587768555\n",
      "epoch 46 batch 79 loss: 5.218845844268799\n",
      "epoch 46 batch 80 loss: 5.229190349578857\n",
      "epoch 46 batch 81 loss: 5.205824851989746\n",
      "epoch 46 batch 82 loss: 5.206223011016846\n",
      "epoch 46 batch 83 loss: 5.236330986022949\n",
      "epoch 46 batch 84 loss: 5.1865363121032715\n",
      "epoch 46 batch 85 loss: 5.206966400146484\n",
      "epoch 46 batch 86 loss: 5.2144927978515625\n",
      "epoch 46 batch 87 loss: 5.172610282897949\n",
      "epoch 46 batch 88 loss: 5.158107757568359\n",
      "epoch 46 batch 89 loss: 5.180675029754639\n",
      "epoch 46 batch 90 loss: 5.197682857513428\n",
      "epoch 46 batch 91 loss: 5.204038143157959\n",
      "epoch 46 batch 92 loss: 5.232620716094971\n",
      "epoch 46 batch 93 loss: 5.2039079666137695\n",
      "epoch 46 batch 94 loss: 5.144549369812012\n",
      "epoch 46 batch 95 loss: 5.181638240814209\n",
      "epoch 46 batch 96 loss: 5.199861526489258\n",
      "epoch 46 batch 97 loss: 5.205461502075195\n",
      "epoch 46 batch 98 loss: 5.211167335510254\n",
      "epoch 46 batch 99 loss: 5.243186950683594\n",
      "epoch 46 batch 100 loss: 5.1915812492370605\n",
      "epoch 46 batch 101 loss: 5.239871501922607\n",
      "epoch 46 batch 102 loss: 5.2053704261779785\n",
      "epoch 46 batch 103 loss: 5.196879863739014\n",
      "epoch 46 batch 104 loss: 5.179652214050293\n",
      "epoch 46 batch 105 loss: 5.2151312828063965\n",
      "epoch 46 batch 106 loss: 5.1841044425964355\n",
      "epoch 46 batch 107 loss: 5.170032978057861\n",
      "epoch 46 batch 108 loss: 5.226704120635986\n",
      "epoch 46 batch 109 loss: 5.17692232131958\n",
      "epoch 46 batch 110 loss: 5.202853202819824\n",
      "epoch 46 batch 111 loss: 5.208334922790527\n",
      "epoch 46 batch 112 loss: 5.237059593200684\n",
      "epoch 46 batch 113 loss: 5.162642002105713\n",
      "epoch 46 batch 114 loss: 5.194957733154297\n",
      "epoch 46 batch 115 loss: 5.197532653808594\n",
      "epoch 46 batch 116 loss: 5.208666801452637\n",
      "epoch 46 batch 117 loss: 5.19849157333374\n",
      "epoch 46 batch 118 loss: 5.172854900360107\n",
      "epoch 46 batch 119 loss: 5.2084879875183105\n",
      "epoch 46 batch 120 loss: 5.212021827697754\n",
      "epoch 46 batch 121 loss: 5.162580490112305\n",
      "epoch 46 batch 122 loss: 5.139042854309082\n",
      "epoch 46 batch 123 loss: 5.202785015106201\n",
      "epoch 46 batch 124 loss: 5.203100681304932\n",
      "epoch loss: 5.198290908813476\n",
      "epoch 47 batch 0 loss: 5.188414096832275\n",
      "epoch 47 batch 1 loss: 5.155402660369873\n",
      "epoch 47 batch 2 loss: 5.200372695922852\n",
      "epoch 47 batch 3 loss: 5.167409420013428\n",
      "epoch 47 batch 4 loss: 5.165470600128174\n",
      "epoch 47 batch 5 loss: 5.186622619628906\n",
      "epoch 47 batch 6 loss: 5.184196472167969\n",
      "epoch 47 batch 7 loss: 5.180522441864014\n",
      "epoch 47 batch 8 loss: 5.212956428527832\n",
      "epoch 47 batch 9 loss: 5.188703536987305\n",
      "epoch 47 batch 10 loss: 5.233065128326416\n",
      "epoch 47 batch 11 loss: 5.2348504066467285\n",
      "epoch 47 batch 12 loss: 5.204828262329102\n",
      "epoch 47 batch 13 loss: 5.166705131530762\n",
      "epoch 47 batch 14 loss: 5.207543849945068\n",
      "epoch 47 batch 15 loss: 5.204777240753174\n",
      "epoch 47 batch 16 loss: 5.1935014724731445\n",
      "epoch 47 batch 17 loss: 5.208897113800049\n",
      "epoch 47 batch 18 loss: 5.203602313995361\n",
      "epoch 47 batch 19 loss: 5.214299201965332\n",
      "epoch 47 batch 20 loss: 5.205963134765625\n",
      "epoch 47 batch 21 loss: 5.19624662399292\n",
      "epoch 47 batch 22 loss: 5.17164945602417\n",
      "epoch 47 batch 23 loss: 5.250800609588623\n",
      "epoch 47 batch 24 loss: 5.202670574188232\n",
      "epoch 47 batch 25 loss: 5.218019485473633\n",
      "epoch 47 batch 26 loss: 5.178399562835693\n",
      "epoch 47 batch 27 loss: 5.185011386871338\n",
      "epoch 47 batch 28 loss: 5.169711589813232\n",
      "epoch 47 batch 29 loss: 5.227053165435791\n",
      "epoch 47 batch 30 loss: 5.142100811004639\n",
      "epoch 47 batch 31 loss: 5.180577278137207\n",
      "epoch 47 batch 32 loss: 5.182413101196289\n",
      "epoch 47 batch 33 loss: 5.201638221740723\n",
      "epoch 47 batch 34 loss: 5.207701206207275\n",
      "epoch 47 batch 35 loss: 5.215594291687012\n",
      "epoch 47 batch 36 loss: 5.189655780792236\n",
      "epoch 47 batch 37 loss: 5.180453777313232\n",
      "epoch 47 batch 38 loss: 5.177614688873291\n",
      "epoch 47 batch 39 loss: 5.202469825744629\n",
      "epoch 47 batch 40 loss: 5.152655124664307\n",
      "epoch 47 batch 41 loss: 5.167332649230957\n",
      "epoch 47 batch 42 loss: 5.213468074798584\n",
      "epoch 47 batch 43 loss: 5.206490993499756\n",
      "epoch 47 batch 44 loss: 5.248754024505615\n",
      "epoch 47 batch 45 loss: 5.174463272094727\n",
      "epoch 47 batch 46 loss: 5.16591215133667\n",
      "epoch 47 batch 47 loss: 5.193307399749756\n",
      "epoch 47 batch 48 loss: 5.199158668518066\n",
      "epoch 47 batch 49 loss: 5.204317569732666\n",
      "epoch 47 batch 50 loss: 5.218598365783691\n",
      "epoch 47 batch 51 loss: 5.182154655456543\n",
      "epoch 47 batch 52 loss: 5.162835597991943\n",
      "epoch 47 batch 53 loss: 5.188235282897949\n",
      "epoch 47 batch 54 loss: 5.185571670532227\n",
      "epoch 47 batch 55 loss: 5.220441818237305\n",
      "epoch 47 batch 56 loss: 5.144941329956055\n",
      "epoch 47 batch 57 loss: 5.189873695373535\n",
      "epoch 47 batch 58 loss: 5.20111083984375\n",
      "epoch 47 batch 59 loss: 5.192494869232178\n",
      "epoch 47 batch 60 loss: 5.231736183166504\n",
      "epoch 47 batch 61 loss: 5.198765754699707\n",
      "epoch 47 batch 62 loss: 5.173255443572998\n",
      "epoch 47 batch 63 loss: 5.1616411209106445\n",
      "epoch 47 batch 64 loss: 5.195706844329834\n",
      "epoch 47 batch 65 loss: 5.167555809020996\n",
      "epoch 47 batch 66 loss: 5.170369625091553\n",
      "epoch 47 batch 67 loss: 5.204421043395996\n",
      "epoch 47 batch 68 loss: 5.229311466217041\n",
      "epoch 47 batch 69 loss: 5.18142557144165\n",
      "epoch 47 batch 70 loss: 5.221912860870361\n",
      "epoch 47 batch 71 loss: 5.20726203918457\n",
      "epoch 47 batch 72 loss: 5.173030853271484\n",
      "epoch 47 batch 73 loss: 5.217620849609375\n",
      "epoch 47 batch 74 loss: 5.26959753036499\n",
      "epoch 47 batch 75 loss: 5.221474647521973\n",
      "epoch 47 batch 76 loss: 5.158750534057617\n",
      "epoch 47 batch 77 loss: 5.19233512878418\n",
      "epoch 47 batch 78 loss: 5.190751075744629\n",
      "epoch 47 batch 79 loss: 5.210273265838623\n",
      "epoch 47 batch 80 loss: 5.196115493774414\n",
      "epoch 47 batch 81 loss: 5.231011867523193\n",
      "epoch 47 batch 82 loss: 5.208523273468018\n",
      "epoch 47 batch 83 loss: 5.218768119812012\n",
      "epoch 47 batch 84 loss: 5.2060770988464355\n",
      "epoch 47 batch 85 loss: 5.194850921630859\n",
      "epoch 47 batch 86 loss: 5.170950889587402\n",
      "epoch 47 batch 87 loss: 5.220782279968262\n",
      "epoch 47 batch 88 loss: 5.1874308586120605\n",
      "epoch 47 batch 89 loss: 5.170161247253418\n",
      "epoch 47 batch 90 loss: 5.221883296966553\n",
      "epoch 47 batch 91 loss: 5.220099925994873\n",
      "epoch 47 batch 92 loss: 5.19639253616333\n",
      "epoch 47 batch 93 loss: 5.192960262298584\n",
      "epoch 47 batch 94 loss: 5.195640563964844\n",
      "epoch 47 batch 95 loss: 5.218421936035156\n",
      "epoch 47 batch 96 loss: 5.216651916503906\n",
      "epoch 47 batch 97 loss: 5.194693565368652\n",
      "epoch 47 batch 98 loss: 5.169011116027832\n",
      "epoch 47 batch 99 loss: 5.14323091506958\n",
      "epoch 47 batch 100 loss: 5.193099021911621\n",
      "epoch 47 batch 101 loss: 5.196495056152344\n",
      "epoch 47 batch 102 loss: 5.197697639465332\n",
      "epoch 47 batch 103 loss: 5.19529390335083\n",
      "epoch 47 batch 104 loss: 5.180182933807373\n",
      "epoch 47 batch 105 loss: 5.180423259735107\n",
      "epoch 47 batch 106 loss: 5.149470329284668\n",
      "epoch 47 batch 107 loss: 5.192623615264893\n",
      "epoch 47 batch 108 loss: 5.21685791015625\n",
      "epoch 47 batch 109 loss: 5.216179847717285\n",
      "epoch 47 batch 110 loss: 5.200684547424316\n",
      "epoch 47 batch 111 loss: 5.186587333679199\n",
      "epoch 47 batch 112 loss: 5.157382488250732\n",
      "epoch 47 batch 113 loss: 5.172127723693848\n",
      "epoch 47 batch 114 loss: 5.194468975067139\n",
      "epoch 47 batch 115 loss: 5.163604736328125\n",
      "epoch 47 batch 116 loss: 5.195443630218506\n",
      "epoch 47 batch 117 loss: 5.162959575653076\n",
      "epoch 47 batch 118 loss: 5.237460613250732\n",
      "epoch 47 batch 119 loss: 5.185708999633789\n",
      "epoch 47 batch 120 loss: 5.208559989929199\n",
      "epoch 47 batch 121 loss: 5.236677169799805\n",
      "epoch 47 batch 122 loss: 5.2065629959106445\n",
      "epoch 47 batch 123 loss: 5.207243919372559\n",
      "epoch 47 batch 124 loss: 5.207456111907959\n",
      "epoch loss: 5.195136894226074\n",
      "epoch 48 batch 0 loss: 5.166351318359375\n",
      "epoch 48 batch 1 loss: 5.1808695793151855\n",
      "epoch 48 batch 2 loss: 5.202017784118652\n",
      "epoch 48 batch 3 loss: 5.165788650512695\n",
      "epoch 48 batch 4 loss: 5.216653347015381\n",
      "epoch 48 batch 5 loss: 5.204215049743652\n",
      "epoch 48 batch 6 loss: 5.16511344909668\n",
      "epoch 48 batch 7 loss: 5.1980695724487305\n",
      "epoch 48 batch 8 loss: 5.223439693450928\n",
      "epoch 48 batch 9 loss: 5.178181171417236\n",
      "epoch 48 batch 10 loss: 5.18582820892334\n",
      "epoch 48 batch 11 loss: 5.174840450286865\n",
      "epoch 48 batch 12 loss: 5.215571403503418\n",
      "epoch 48 batch 13 loss: 5.16341495513916\n",
      "epoch 48 batch 14 loss: 5.21270751953125\n",
      "epoch 48 batch 15 loss: 5.173097610473633\n",
      "epoch 48 batch 16 loss: 5.1634087562561035\n",
      "epoch 48 batch 17 loss: 5.194146156311035\n",
      "epoch 48 batch 18 loss: 5.185835838317871\n",
      "epoch 48 batch 19 loss: 5.1422624588012695\n",
      "epoch 48 batch 20 loss: 5.205821514129639\n",
      "epoch 48 batch 21 loss: 5.181213855743408\n",
      "epoch 48 batch 22 loss: 5.16886568069458\n",
      "epoch 48 batch 23 loss: 5.180224418640137\n",
      "epoch 48 batch 24 loss: 5.201915740966797\n",
      "epoch 48 batch 25 loss: 5.226082801818848\n",
      "epoch 48 batch 26 loss: 5.1565680503845215\n",
      "epoch 48 batch 27 loss: 5.166727066040039\n",
      "epoch 48 batch 28 loss: 5.173512935638428\n",
      "epoch 48 batch 29 loss: 5.195775508880615\n",
      "epoch 48 batch 30 loss: 5.244773864746094\n",
      "epoch 48 batch 31 loss: 5.23447847366333\n",
      "epoch 48 batch 32 loss: 5.206014156341553\n",
      "epoch 48 batch 33 loss: 5.187184810638428\n",
      "epoch 48 batch 34 loss: 5.166322708129883\n",
      "epoch 48 batch 35 loss: 5.198175430297852\n",
      "epoch 48 batch 36 loss: 5.150876998901367\n",
      "epoch 48 batch 37 loss: 5.209917068481445\n",
      "epoch 48 batch 38 loss: 5.189533710479736\n",
      "epoch 48 batch 39 loss: 5.195103645324707\n",
      "epoch 48 batch 40 loss: 5.197175025939941\n",
      "epoch 48 batch 41 loss: 5.209558010101318\n",
      "epoch 48 batch 42 loss: 5.209245681762695\n",
      "epoch 48 batch 43 loss: 5.182261943817139\n",
      "epoch 48 batch 44 loss: 5.224609375\n",
      "epoch 48 batch 45 loss: 5.219746112823486\n",
      "epoch 48 batch 46 loss: 5.17963981628418\n",
      "epoch 48 batch 47 loss: 5.184282302856445\n",
      "epoch 48 batch 48 loss: 5.207695007324219\n",
      "epoch 48 batch 49 loss: 5.209139347076416\n",
      "epoch 48 batch 50 loss: 5.184538841247559\n",
      "epoch 48 batch 51 loss: 5.205948829650879\n",
      "epoch 48 batch 52 loss: 5.197706699371338\n",
      "epoch 48 batch 53 loss: 5.203877925872803\n",
      "epoch 48 batch 54 loss: 5.210445404052734\n",
      "epoch 48 batch 55 loss: 5.199502944946289\n",
      "epoch 48 batch 56 loss: 5.169346809387207\n",
      "epoch 48 batch 57 loss: 5.2041916847229\n",
      "epoch 48 batch 58 loss: 5.165333271026611\n",
      "epoch 48 batch 59 loss: 5.197177410125732\n",
      "epoch 48 batch 60 loss: 5.169478416442871\n",
      "epoch 48 batch 61 loss: 5.2097272872924805\n",
      "epoch 48 batch 62 loss: 5.212031841278076\n",
      "epoch 48 batch 63 loss: 5.189480304718018\n",
      "epoch 48 batch 64 loss: 5.185835361480713\n",
      "epoch 48 batch 65 loss: 5.164666652679443\n",
      "epoch 48 batch 66 loss: 5.165915012359619\n",
      "epoch 48 batch 67 loss: 5.242931842803955\n",
      "epoch 48 batch 68 loss: 5.190339088439941\n",
      "epoch 48 batch 69 loss: 5.162261962890625\n",
      "epoch 48 batch 70 loss: 5.202193260192871\n",
      "epoch 48 batch 71 loss: 5.175975322723389\n",
      "epoch 48 batch 72 loss: 5.17147159576416\n",
      "epoch 48 batch 73 loss: 5.1702423095703125\n",
      "epoch 48 batch 74 loss: 5.212609767913818\n",
      "epoch 48 batch 75 loss: 5.161942005157471\n",
      "epoch 48 batch 76 loss: 5.156552791595459\n",
      "epoch 48 batch 77 loss: 5.188948154449463\n",
      "epoch 48 batch 78 loss: 5.218517780303955\n",
      "epoch 48 batch 79 loss: 5.189995765686035\n",
      "epoch 48 batch 80 loss: 5.161093711853027\n",
      "epoch 48 batch 81 loss: 5.2039642333984375\n",
      "epoch 48 batch 82 loss: 5.177292823791504\n",
      "epoch 48 batch 83 loss: 5.223160266876221\n",
      "epoch 48 batch 84 loss: 5.207807540893555\n",
      "epoch 48 batch 85 loss: 5.171260833740234\n",
      "epoch 48 batch 86 loss: 5.184572696685791\n",
      "epoch 48 batch 87 loss: 5.193985939025879\n",
      "epoch 48 batch 88 loss: 5.212525367736816\n",
      "epoch 48 batch 89 loss: 5.22791862487793\n",
      "epoch 48 batch 90 loss: 5.229111194610596\n",
      "epoch 48 batch 91 loss: 5.202062129974365\n",
      "epoch 48 batch 92 loss: 5.206977844238281\n",
      "epoch 48 batch 93 loss: 5.165040016174316\n",
      "epoch 48 batch 94 loss: 5.202678203582764\n",
      "epoch 48 batch 95 loss: 5.185030460357666\n",
      "epoch 48 batch 96 loss: 5.243239402770996\n",
      "epoch 48 batch 97 loss: 5.196871280670166\n",
      "epoch 48 batch 98 loss: 5.152365207672119\n",
      "epoch 48 batch 99 loss: 5.162563323974609\n",
      "epoch 48 batch 100 loss: 5.161290168762207\n",
      "epoch 48 batch 101 loss: 5.171431064605713\n",
      "epoch 48 batch 102 loss: 5.212216854095459\n",
      "epoch 48 batch 103 loss: 5.191418170928955\n",
      "epoch 48 batch 104 loss: 5.201931476593018\n",
      "epoch 48 batch 105 loss: 5.203903675079346\n",
      "epoch 48 batch 106 loss: 5.176239013671875\n",
      "epoch 48 batch 107 loss: 5.211511135101318\n",
      "epoch 48 batch 108 loss: 5.194403648376465\n",
      "epoch 48 batch 109 loss: 5.164395809173584\n",
      "epoch 48 batch 110 loss: 5.179996967315674\n",
      "epoch 48 batch 111 loss: 5.1989030838012695\n",
      "epoch 48 batch 112 loss: 5.188601016998291\n",
      "epoch 48 batch 113 loss: 5.206247329711914\n",
      "epoch 48 batch 114 loss: 5.203820705413818\n",
      "epoch 48 batch 115 loss: 5.197030544281006\n",
      "epoch 48 batch 116 loss: 5.1853132247924805\n",
      "epoch 48 batch 117 loss: 5.195162296295166\n",
      "epoch 48 batch 118 loss: 5.198794841766357\n",
      "epoch 48 batch 119 loss: 5.199238300323486\n",
      "epoch 48 batch 120 loss: 5.197028160095215\n",
      "epoch 48 batch 121 loss: 5.208607196807861\n",
      "epoch 48 batch 122 loss: 5.183750629425049\n",
      "epoch 48 batch 123 loss: 5.230488300323486\n",
      "epoch 48 batch 124 loss: 5.180232048034668\n",
      "epoch loss: 5.192039337158203\n",
      "epoch 49 batch 0 loss: 5.163339614868164\n",
      "epoch 49 batch 1 loss: 5.207435607910156\n",
      "epoch 49 batch 2 loss: 5.186985969543457\n",
      "epoch 49 batch 3 loss: 5.169755935668945\n",
      "epoch 49 batch 4 loss: 5.179953575134277\n",
      "epoch 49 batch 5 loss: 5.19493293762207\n",
      "epoch 49 batch 6 loss: 5.216681003570557\n",
      "epoch 49 batch 7 loss: 5.148848056793213\n",
      "epoch 49 batch 8 loss: 5.152807712554932\n",
      "epoch 49 batch 9 loss: 5.1822075843811035\n",
      "epoch 49 batch 10 loss: 5.176939964294434\n",
      "epoch 49 batch 11 loss: 5.197556018829346\n",
      "epoch 49 batch 12 loss: 5.177307605743408\n",
      "epoch 49 batch 13 loss: 5.208766937255859\n",
      "epoch 49 batch 14 loss: 5.176466941833496\n",
      "epoch 49 batch 15 loss: 5.138552188873291\n",
      "epoch 49 batch 16 loss: 5.187345027923584\n",
      "epoch 49 batch 17 loss: 5.152602195739746\n",
      "epoch 49 batch 18 loss: 5.19830322265625\n",
      "epoch 49 batch 19 loss: 5.189206600189209\n",
      "epoch 49 batch 20 loss: 5.250294208526611\n",
      "epoch 49 batch 21 loss: 5.210072040557861\n",
      "epoch 49 batch 22 loss: 5.199610710144043\n",
      "epoch 49 batch 23 loss: 5.203372478485107\n",
      "epoch 49 batch 24 loss: 5.203444004058838\n",
      "epoch 49 batch 25 loss: 5.167264461517334\n",
      "epoch 49 batch 26 loss: 5.167018890380859\n",
      "epoch 49 batch 27 loss: 5.188439846038818\n",
      "epoch 49 batch 28 loss: 5.181684970855713\n",
      "epoch 49 batch 29 loss: 5.218803405761719\n",
      "epoch 49 batch 30 loss: 5.190526008605957\n",
      "epoch 49 batch 31 loss: 5.153418064117432\n",
      "epoch 49 batch 32 loss: 5.247365474700928\n",
      "epoch 49 batch 33 loss: 5.2034149169921875\n",
      "epoch 49 batch 34 loss: 5.18263053894043\n",
      "epoch 49 batch 35 loss: 5.212853908538818\n",
      "epoch 49 batch 36 loss: 5.174794673919678\n",
      "epoch 49 batch 37 loss: 5.246967792510986\n",
      "epoch 49 batch 38 loss: 5.185634613037109\n",
      "epoch 49 batch 39 loss: 5.18861198425293\n",
      "epoch 49 batch 40 loss: 5.208763122558594\n",
      "epoch 49 batch 41 loss: 5.175695896148682\n",
      "epoch 49 batch 42 loss: 5.16935920715332\n",
      "epoch 49 batch 43 loss: 5.189230918884277\n",
      "epoch 49 batch 44 loss: 5.1784586906433105\n",
      "epoch 49 batch 45 loss: 5.207478046417236\n",
      "epoch 49 batch 46 loss: 5.206871032714844\n",
      "epoch 49 batch 47 loss: 5.214848041534424\n",
      "epoch 49 batch 48 loss: 5.217952728271484\n",
      "epoch 49 batch 49 loss: 5.209113597869873\n",
      "epoch 49 batch 50 loss: 5.1942667961120605\n",
      "epoch 49 batch 51 loss: 5.189364433288574\n",
      "epoch 49 batch 52 loss: 5.24445915222168\n",
      "epoch 49 batch 53 loss: 5.18314790725708\n",
      "epoch 49 batch 54 loss: 5.210391521453857\n",
      "epoch 49 batch 55 loss: 5.223555564880371\n",
      "epoch 49 batch 56 loss: 5.220740795135498\n",
      "epoch 49 batch 57 loss: 5.181560516357422\n",
      "epoch 49 batch 58 loss: 5.228584289550781\n",
      "epoch 49 batch 59 loss: 5.2328691482543945\n",
      "epoch 49 batch 60 loss: 5.190883159637451\n",
      "epoch 49 batch 61 loss: 5.1392316818237305\n",
      "epoch 49 batch 62 loss: 5.160455226898193\n",
      "epoch 49 batch 63 loss: 5.186161994934082\n",
      "epoch 49 batch 64 loss: 5.180660247802734\n",
      "epoch 49 batch 65 loss: 5.169737339019775\n",
      "epoch 49 batch 66 loss: 5.174594402313232\n",
      "epoch 49 batch 67 loss: 5.1836771965026855\n",
      "epoch 49 batch 68 loss: 5.188470363616943\n",
      "epoch 49 batch 69 loss: 5.17930793762207\n",
      "epoch 49 batch 70 loss: 5.208864688873291\n",
      "epoch 49 batch 71 loss: 5.193314075469971\n",
      "epoch 49 batch 72 loss: 5.183253765106201\n",
      "epoch 49 batch 73 loss: 5.204726219177246\n",
      "epoch 49 batch 74 loss: 5.187376499176025\n",
      "epoch 49 batch 75 loss: 5.17741060256958\n",
      "epoch 49 batch 76 loss: 5.192834854125977\n",
      "epoch 49 batch 77 loss: 5.177394866943359\n",
      "epoch 49 batch 78 loss: 5.202028274536133\n",
      "epoch 49 batch 79 loss: 5.214287757873535\n",
      "epoch 49 batch 80 loss: 5.198287010192871\n",
      "epoch 49 batch 81 loss: 5.219061374664307\n",
      "epoch 49 batch 82 loss: 5.19480037689209\n",
      "epoch 49 batch 83 loss: 5.1755290031433105\n",
      "epoch 49 batch 84 loss: 5.191424369812012\n",
      "epoch 49 batch 85 loss: 5.204044818878174\n",
      "epoch 49 batch 86 loss: 5.20712947845459\n",
      "epoch 49 batch 87 loss: 5.187955379486084\n",
      "epoch 49 batch 88 loss: 5.171159744262695\n",
      "epoch 49 batch 89 loss: 5.178150653839111\n",
      "epoch 49 batch 90 loss: 5.202030658721924\n",
      "epoch 49 batch 91 loss: 5.2121806144714355\n",
      "epoch 49 batch 92 loss: 5.166421890258789\n",
      "epoch 49 batch 93 loss: 5.178005218505859\n",
      "epoch 49 batch 94 loss: 5.188511371612549\n",
      "epoch 49 batch 95 loss: 5.201333045959473\n",
      "epoch 49 batch 96 loss: 5.15130615234375\n",
      "epoch 49 batch 97 loss: 5.179780960083008\n",
      "epoch 49 batch 98 loss: 5.211709976196289\n",
      "epoch 49 batch 99 loss: 5.167853832244873\n",
      "epoch 49 batch 100 loss: 5.184187412261963\n",
      "epoch 49 batch 101 loss: 5.210105895996094\n",
      "epoch 49 batch 102 loss: 5.195868968963623\n",
      "epoch 49 batch 103 loss: 5.192944049835205\n",
      "epoch 49 batch 104 loss: 5.216867446899414\n",
      "epoch 49 batch 105 loss: 5.18733549118042\n",
      "epoch 49 batch 106 loss: 5.185959815979004\n",
      "epoch 49 batch 107 loss: 5.201135158538818\n",
      "epoch 49 batch 108 loss: 5.20223331451416\n",
      "epoch 49 batch 109 loss: 5.243492126464844\n",
      "epoch 49 batch 110 loss: 5.183463096618652\n",
      "epoch 49 batch 111 loss: 5.189436912536621\n",
      "epoch 49 batch 112 loss: 5.198918342590332\n",
      "epoch 49 batch 113 loss: 5.199535846710205\n",
      "epoch 49 batch 114 loss: 5.224118709564209\n",
      "epoch 49 batch 115 loss: 5.213423728942871\n",
      "epoch 49 batch 116 loss: 5.185738563537598\n",
      "epoch 49 batch 117 loss: 5.173579216003418\n",
      "epoch 49 batch 118 loss: 5.185861110687256\n",
      "epoch 49 batch 119 loss: 5.169592380523682\n",
      "epoch 49 batch 120 loss: 5.20974063873291\n",
      "epoch 49 batch 121 loss: 5.2047905921936035\n",
      "epoch 49 batch 122 loss: 5.193612098693848\n",
      "epoch 49 batch 123 loss: 5.216906547546387\n",
      "epoch 49 batch 124 loss: 5.187037944793701\n",
      "epoch loss: 5.1928492927551275\n",
      "epoch 50 batch 0 loss: 5.176243782043457\n",
      "epoch 50 batch 1 loss: 5.200887203216553\n",
      "epoch 50 batch 2 loss: 5.220545768737793\n",
      "epoch 50 batch 3 loss: 5.179572582244873\n",
      "epoch 50 batch 4 loss: 5.173349380493164\n",
      "epoch 50 batch 5 loss: 5.168503284454346\n",
      "epoch 50 batch 6 loss: 5.203913688659668\n",
      "epoch 50 batch 7 loss: 5.237799167633057\n",
      "epoch 50 batch 8 loss: 5.157073974609375\n",
      "epoch 50 batch 9 loss: 5.172476291656494\n",
      "epoch 50 batch 10 loss: 5.148006439208984\n",
      "epoch 50 batch 11 loss: 5.188704490661621\n",
      "epoch 50 batch 12 loss: 5.203672409057617\n",
      "epoch 50 batch 13 loss: 5.180456638336182\n",
      "epoch 50 batch 14 loss: 5.225849628448486\n",
      "epoch 50 batch 15 loss: 5.207859516143799\n",
      "epoch 50 batch 16 loss: 5.234961986541748\n",
      "epoch 50 batch 17 loss: 5.152278900146484\n",
      "epoch 50 batch 18 loss: 5.205873966217041\n",
      "epoch 50 batch 19 loss: 5.165184020996094\n",
      "epoch 50 batch 20 loss: 5.167759895324707\n",
      "epoch 50 batch 21 loss: 5.195790767669678\n",
      "epoch 50 batch 22 loss: 5.208892822265625\n",
      "epoch 50 batch 23 loss: 5.203731536865234\n",
      "epoch 50 batch 24 loss: 5.178761959075928\n",
      "epoch 50 batch 25 loss: 5.184970378875732\n",
      "epoch 50 batch 26 loss: 5.209183216094971\n",
      "epoch 50 batch 27 loss: 5.174201965332031\n",
      "epoch 50 batch 28 loss: 5.185696125030518\n",
      "epoch 50 batch 29 loss: 5.198898792266846\n",
      "epoch 50 batch 30 loss: 5.171786785125732\n",
      "epoch 50 batch 31 loss: 5.1923065185546875\n",
      "epoch 50 batch 32 loss: 5.17896842956543\n",
      "epoch 50 batch 33 loss: 5.167723655700684\n",
      "epoch 50 batch 34 loss: 5.19621467590332\n",
      "epoch 50 batch 35 loss: 5.182356834411621\n",
      "epoch 50 batch 36 loss: 5.200782299041748\n",
      "epoch 50 batch 37 loss: 5.207713603973389\n",
      "epoch 50 batch 38 loss: 5.216681003570557\n",
      "epoch 50 batch 39 loss: 5.195461273193359\n",
      "epoch 50 batch 40 loss: 5.176139831542969\n",
      "epoch 50 batch 41 loss: 5.210141658782959\n",
      "epoch 50 batch 42 loss: 5.226953029632568\n",
      "epoch 50 batch 43 loss: 5.175728797912598\n",
      "epoch 50 batch 44 loss: 5.155242919921875\n",
      "epoch 50 batch 45 loss: 5.194277763366699\n",
      "epoch 50 batch 46 loss: 5.232925891876221\n",
      "epoch 50 batch 47 loss: 5.207344055175781\n",
      "epoch 50 batch 48 loss: 5.184409141540527\n",
      "epoch 50 batch 49 loss: 5.185213565826416\n",
      "epoch 50 batch 50 loss: 5.20585823059082\n",
      "epoch 50 batch 51 loss: 5.2142205238342285\n",
      "epoch 50 batch 52 loss: 5.211423873901367\n",
      "epoch 50 batch 53 loss: 5.189549446105957\n",
      "epoch 50 batch 54 loss: 5.190516948699951\n",
      "epoch 50 batch 55 loss: 5.205831050872803\n",
      "epoch 50 batch 56 loss: 5.174825191497803\n",
      "epoch 50 batch 57 loss: 5.176987648010254\n",
      "epoch 50 batch 58 loss: 5.143040180206299\n",
      "epoch 50 batch 59 loss: 5.167965412139893\n",
      "epoch 50 batch 60 loss: 5.229068279266357\n",
      "epoch 50 batch 61 loss: 5.131398677825928\n",
      "epoch 50 batch 62 loss: 5.169638633728027\n",
      "epoch 50 batch 63 loss: 5.199413299560547\n",
      "epoch 50 batch 64 loss: 5.208232402801514\n",
      "epoch 50 batch 65 loss: 5.146665096282959\n",
      "epoch 50 batch 66 loss: 5.213804721832275\n",
      "epoch 50 batch 67 loss: 5.190535545349121\n",
      "epoch 50 batch 68 loss: 5.154109477996826\n",
      "epoch 50 batch 69 loss: 5.217890739440918\n",
      "epoch 50 batch 70 loss: 5.19301176071167\n",
      "epoch 50 batch 71 loss: 5.16579532623291\n",
      "epoch 50 batch 72 loss: 5.173856735229492\n",
      "epoch 50 batch 73 loss: 5.18895149230957\n",
      "epoch 50 batch 74 loss: 5.164744853973389\n",
      "epoch 50 batch 75 loss: 5.232097148895264\n",
      "epoch 50 batch 76 loss: 5.219020843505859\n",
      "epoch 50 batch 77 loss: 5.157529354095459\n",
      "epoch 50 batch 78 loss: 5.155903339385986\n",
      "epoch 50 batch 79 loss: 5.212954521179199\n",
      "epoch 50 batch 80 loss: 5.190433979034424\n",
      "epoch 50 batch 81 loss: 5.161341667175293\n",
      "epoch 50 batch 82 loss: 5.223434925079346\n",
      "epoch 50 batch 83 loss: 5.222118854522705\n",
      "epoch 50 batch 84 loss: 5.173099994659424\n",
      "epoch 50 batch 85 loss: 5.204102039337158\n",
      "epoch 50 batch 86 loss: 5.177050590515137\n",
      "epoch 50 batch 87 loss: 5.177002429962158\n",
      "epoch 50 batch 88 loss: 5.192557334899902\n",
      "epoch 50 batch 89 loss: 5.201855182647705\n",
      "epoch 50 batch 90 loss: 5.146236419677734\n",
      "epoch 50 batch 91 loss: 5.179751873016357\n",
      "epoch 50 batch 92 loss: 5.200697898864746\n",
      "epoch 50 batch 93 loss: 5.18392276763916\n",
      "epoch 50 batch 94 loss: 5.181893825531006\n",
      "epoch 50 batch 95 loss: 5.180965900421143\n",
      "epoch 50 batch 96 loss: 5.1997551918029785\n",
      "epoch 50 batch 97 loss: 5.200747013092041\n",
      "epoch 50 batch 98 loss: 5.189027786254883\n",
      "epoch 50 batch 99 loss: 5.175814628601074\n",
      "epoch 50 batch 100 loss: 5.174552917480469\n",
      "epoch 50 batch 101 loss: 5.158367156982422\n",
      "epoch 50 batch 102 loss: 5.163339138031006\n",
      "epoch 50 batch 103 loss: 5.2201032638549805\n",
      "epoch 50 batch 104 loss: 5.21314811706543\n",
      "epoch 50 batch 105 loss: 5.163814544677734\n",
      "epoch 50 batch 106 loss: 5.16030740737915\n",
      "epoch 50 batch 107 loss: 5.175863742828369\n",
      "epoch 50 batch 108 loss: 5.1845855712890625\n",
      "epoch 50 batch 109 loss: 5.233942985534668\n",
      "epoch 50 batch 110 loss: 5.162467956542969\n",
      "epoch 50 batch 111 loss: 5.205093860626221\n",
      "epoch 50 batch 112 loss: 5.171440124511719\n",
      "epoch 50 batch 113 loss: 5.207780838012695\n",
      "epoch 50 batch 114 loss: 5.210262298583984\n",
      "epoch 50 batch 115 loss: 5.208049297332764\n",
      "epoch 50 batch 116 loss: 5.211121559143066\n",
      "epoch 50 batch 117 loss: 5.173497676849365\n",
      "epoch 50 batch 118 loss: 5.224937438964844\n",
      "epoch 50 batch 119 loss: 5.190850257873535\n",
      "epoch 50 batch 120 loss: 5.197498798370361\n",
      "epoch 50 batch 121 loss: 5.187996864318848\n",
      "epoch 50 batch 122 loss: 5.164483547210693\n",
      "epoch 50 batch 123 loss: 5.216995716094971\n",
      "epoch 50 batch 124 loss: 5.190842151641846\n",
      "epoch loss: 5.189659732818604\n",
      "epoch 51 batch 0 loss: 5.187165260314941\n",
      "epoch 51 batch 1 loss: 5.188694477081299\n",
      "epoch 51 batch 2 loss: 5.187326431274414\n",
      "epoch 51 batch 3 loss: 5.1943039894104\n",
      "epoch 51 batch 4 loss: 5.195942401885986\n",
      "epoch 51 batch 5 loss: 5.195506572723389\n",
      "epoch 51 batch 6 loss: 5.193728923797607\n",
      "epoch 51 batch 7 loss: 5.204078197479248\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     34\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m, epoch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     37\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m,total_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataset)\u001b[38;5;241m*\u001b[39mbatch_size)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d479dbf-7797-4d3b-a272-ea0c835123f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainstage2.py\n",
    "import torch,argparse,os\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "pre_model_path = \"/remote-home/songtianwei/research/unlearn_multimodal/output/unlearn_test_self_supervised/model_stage1_epoch50.pth\"\n",
    "\n",
    "\n",
    "# train stage two\n",
    "def train_stage2():\n",
    "    if torch.cuda.is_available():\n",
    "        DEVICE = torch.device(\"cuda:\" + str(2))   #config.gpu_name\n",
    "        # 每次训练计算图改动较小使用，在开始前选取较优的基础算法（比如选择一种当前高效的卷积算法）\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "    print(\"current deveice:\", DEVICE)\n",
    "\n",
    "    # load dataset for train and eval\n",
    "    train_dataset = CIFAR10(root='dataset', train=True, transform=train_transform, download=True)\n",
    "    train_data = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "    eval_dataset = CIFAR10(root='dataset', train=False, transform=test_transform, download=True)\n",
    "    eval_data = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    model =net.SimCLRStage2(num_class=len(train_dataset.classes)).to(DEVICE)\n",
    "    model.load_state_dict(torch.load(args.pre_model, map_location='cpu'),strict=False)\n",
    "    loss_criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "    for epoch in range(1,max_epoch+1):\n",
    "        model.train()\n",
    "        total_loss=0\n",
    "        for batch, (data, target) in enumerate(train_data):\n",
    "            data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "            pred = model(data)\n",
    "\n",
    "            loss = loss_criterion(pred, target)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(\"epoch\",epoch,\"loss:\", total_loss / len(train_dataset)*args.batch_size)\n",
    "        with open(os.path.join(save_path, \"stage2_loss.txt\"), \"a\") as f:\n",
    "            f.write(str(total_loss / len(train_dataset)*args.batch_size) + \" \")\n",
    "\n",
    "        if epoch % 5==0:\n",
    "            torch.save(model.state_dict(), os.path.join(save_path, 'model_stage2_epoch' + str(epoch) + '.pth'))\n",
    "\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                print(\"batch\", \" \" * 1, \"top1 acc\", \" \" * 1, \"top5 acc\")\n",
    "                total_loss, total_correct_1, total_correct_5, total_num = 0.0, 0.0, 0.0, 0\n",
    "                for batch, (data, target) in enumerate(train_data):\n",
    "                    data, target = data.to(DEVICE), target.to(DEVICE)\n",
    "                    pred = model(data)\n",
    "\n",
    "                    total_num += data.size(0)\n",
    "                    prediction = torch.argsort(pred, dim=-1, descending=True)\n",
    "                    top1_acc = torch.sum((prediction[:, 0:1] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "                    top5_acc = torch.sum((prediction[:, 0:5] == target.unsqueeze(dim=-1)).any(dim=-1).float()).item()\n",
    "                    total_correct_1 += top1_acc\n",
    "                    total_correct_5 += top5_acc\n",
    "\n",
    "                    print(\"  {:02}  \".format(batch + 1), \" {:02.3f}%  \".format(top1_acc / data.size(0) * 100),\n",
    "                          \"{:02.3f}%  \".format(top5_acc / data.size(0) * 100))\n",
    "\n",
    "                print(\"all eval dataset:\", \"top1 acc: {:02.3f}%\".format(total_correct_1 / total_num * 100),\n",
    "                          \"top5 acc:{:02.3f}%\".format(total_correct_5 / total_num * 100))\n",
    "                with open(os.path.join(save_path, \"stage2_top1_acc.txt\"), \"a\") as f:\n",
    "                    f.write(str(total_correct_1 / total_num * 100) + \" \")\n",
    "                with open(os.path.join(save_path, \"stage2_top5_acc.txt\"), \"a\") as f:\n",
    "                    f.write(str(total_correct_5 / total_num * 100) + \" \")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(description='Train SimCLR')\n",
    "    parser.add_argument('--batch_size', default=200, type=int, help='')\n",
    "    parser.add_argument('--max_epoch', default=200, type=int, help='')\n",
    "    parser.add_argument('--pre_model', default=config.pre_model, type=str, help='')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    train(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b3bc61-3d70-42ae-8c2a-10617b5cca69",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
