common:
  # model attacked
  model: 'clip'
  # clip_model: 'ViT-B/32'
  clip_model: 'RN50'

  # dataset protected
  dataset: 'flickr'

  # dataset json file
  train_file:  ['/remote-home/songtianwei/research/unlearn_multimodal/data/cifar10.json']
  val_file: '/remote-home/songtianwei/research/unlearn_multimodal/data/flickr30k_val.json'                
  test_file: '/remote-home/songtianwei/research/unlearn_multimodal/data/flickr30k_test.json'

  # dataset path
  image_root: '/remote-home/songtianwei/research/albef/ALBEF/datasets/' #flickr30k-images/

  # noise path
  noise_root: '/remote-home/songtianwei/research/unlearn_multimodal/output/clip'

  bert_config: 'configs/config_bert.json'

  alpha: 0.4
  distill: True
  warm_up: True

  model_init: xavier

step1:  # train generator
  batch_size_train: 24  
  batch_size_test: 64
  image_res: 224
  max_words: 77
  # generator training setting
  optimizer: {opt: adamW, lr: 0.0001, betas: (0.0, 0.9)}
  schedular: {sched: cosine, lr: 1e-4, epochs: 15, min_lr: 1e-6, decay_rate: 1, warmup_lr: 1e-5, save_freq: 1, warmup_epochs: 1, cooldown_epochs: 0}
  
  output_dir: 'output/train_generator_linf_max_loss/'

step2:  # generate poison dataset
  batch_size_train: 64  # ori : 64
  batch_size_test: 64
  image_res: 224
  max_words: 77
  poison_delta_root: "/remote-home/songtianwei/research/unlearn_multimodal/datasets/poison_linf_max_loss/poison_data"

step3:  # finetune in downstream dataset
  batch_size_train: 64  # ori : 256
  batch_size_test: 64
  image_res: 224
  max_words: 77

  # clip finetune setting
  optimizer: {opt: adamW, lr: 1e-5, betas: (0.9, 0.98), eps: 1.0e-6, weight_decay: 0.2}
  schedular: {sched: cosine, lr: 1e-5, epochs: 15, min_lr: 1e-6, decay_rate: 1, warmup_lr: 1e-5, save_freq: 1, warmup_epochs: 1, cooldown_epochs: 0}
  
  poison_delta_root: "/remote-home/songtianwei/research/unlearn_multimodal/datasets/poison_max_loss/flickr/poison_data"
  output_dir: 'output/flickr/clip_retrieval'

Pretrain:
  batch_size_train: 128  
  batch_size_test: 64
  image_res: 224
  max_words: 77
  # generator training setting
  optimizer: {opt: adamW, lr:  0.0005, betas: (0.0, 0.9),eps: 1.0e-6, weight_decay: 1e-3}
  schedular: {sched: cosine, lr: 0.0005, epochs: 65, min_lr: 1e-6, decay_rate: 1, warmup_lr: 1e-5, save_freq: 1, warmup_epochs: 15, cooldown_epochs: 0}
  
  output_dir: 'output/cifar10-Pretrain-2/'





